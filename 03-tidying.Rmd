# Tidying Data

In the functionality provided by `itraxR`, the need for data cleaning is much reduced. However, you may still encounter poor quality data that needs removing from subsequent analysis. This can be broadly defined as:

- Data at the start and end of the the core, where a volume of core material is "missing".
- Measurements where the optical configuration is out of position (marked as `validity == 0`), often due to holes or stones in the core.
- Areas of the core with low total counts.
- Individual measurements that are statistical outliers.

The easiest way to do this is using a `tidyverse` style sequence of pipes that set the observations of faulty data as `NA`. 

## Low Count Rates

The count rate ("cps", or counts-per-second) is the rate of energy event detection at the detector, and is a function of both the excitation beam condition (tube type, voltage and current) and the matrix. Because the tube type and voltage are often chosen based on other considerations, the operator usually only adjusts the tube current to optimise the count rate. The higher the total counts for each measurement, the better the measurement will be in terms of detection limits and uncertainties. Higher count rates allow for shorter dwell times, but this must be balanced against the need to minimise harmonics in the spectra. Harmonics can occur where the photon flux is so high that the detector cannot differentiate between two photons, and so registers the sum energy instead. For example, a particularly common phenomenon is the detection of two Fe Kα photons (energy 6.4 keV) as a single photon with the energy 12.8 keV. The graph below shows that for these cores there is the expected positive correlation between the count rate and the Fe Kα * 2 sum peak. To some extent the Q-Spec software will accommodate these harmonics, but operators commonly aim for a count rate of between 30-50 kcps. 

```{r cps, warning=FALSE}
ggplot(data = CD166_19_xrf, mapping = aes(x = cps, y = `Fe a*2`)) + 
  geom_point(alpha = 0.1) 
```

Deletion criteria can be on the basis of count rates (excluding very low and high values), selected harmonics (e.g. Fe a*2), or both. An example is shown below. 

```{r cps_deletion}
CD166_19_xrf %>%
  mutate(in_cps_tolerance = ifelse(cps <=30000 | cps >=60000 | is.na(cps) == TRUE, FALSE, TRUE)) %>%
  
  ggplot(mapping = aes(x = depth, y = cps, col = in_cps_tolerance)) + 
  geom_line(aes(group = 1)) +
  scale_x_reverse() +
  geom_hline(yintercept = c(30000, 60000)) +
  geom_rug(sides = "b", data = . %>% filter(in_cps_tolerance == FALSE)) 
```

## Model Errors (MSE)

The mean-squared-error (MSE) of the model provides an indication of the goodness-of-fit for the peak fitting model implemented in the Q-Spec software. Operators aim to produce models with the lowest possible MSE, but compromises are sometimes required because normally the same model should be applied to observations that are being compared (for example, for the same core sequence). 

```{r mse}
CD166_19_xrf %>%
  mutate(in_mse_tolerance = ifelse(MSE >=2, FALSE, TRUE)) %>%
  
  ggplot(mapping = aes(x = depth,  y = MSE, col = in_mse_tolerance)) +
  geom_line(aes(group = 1)) +
  scale_x_reverse() +
  geom_hline(yintercept = 2) +
  geom_rug(sides = "b", data = . %>% filter(in_mse_tolerance == FALSE)) 
```

## Surface Slope

There is a relationship between the slope of the surface of the core material and the intensity for most elements. Hence areas with a large slope may produce an increase or decrease in a particular element intensity regardless of any actual change in the abundance of an element. This can be corrected for where the effect can be quantified experimentally, but @Jarvis2015 report results of experiments where the slope varies from -0.3 to +0.3 causing variation of around 120 to 80% of the true value. As such, we might initially seek to exclude areas of the core with a high slope. By default, `itraxR::itrax_import()` doesn't import the `sample surface` variable, so the parameter `parameters = "all"` should be passed to access it. The computation is simple to perform using `dplyr::lag()`, and could be used as part of a conditional expression that would mark all measurements with a slope beyond a certain tolerance as having `validity == FALSE`. For example, the example below marks all values with a slope (in either direction) greater than 0.1 mm/200 μm (1:5) as being invalid. As shown for the core below, the core slope is well within the defined tolerances.

```{r slope, warning=FALSE}
CD166_19_xrf %>%
  mutate(slope = `sample surface` - dplyr::lag(`sample surface`)) %>%
  mutate(in_tolerance = ifelse(slope <=-0.1 | slope >=0.1 | is.na(slope) == TRUE, FALSE, TRUE)) %>%
  
  ggplot(mapping = aes(x = depth, y = slope, col = in_tolerance)) +
  scale_y_continuous(limits = c(-0.15, 0.15), oob = scales::squish) +
  geom_line(aes(group = 1)) +
  geom_hline(yintercept = c(-0.1, 0.1)) +
  geom_rug(data = . %>% filter(validity == FALSE)) +
  scale_x_reverse()
```

## Combining "Validity" Flags

It is often desirable to combine all deletion criteria into a single binary variable. This means combining multiple binary variables, returning `FALSE` if any of the values are `FALSE`, but only returning `TRUE` if all of the values are `TRUE`. 

```{r combined_validity_checks, warning=FALSE}
CD166_19_xrf <- CD166_19_xrf %>%
  mutate(slope = `sample surface` - dplyr::lag(`sample surface`)) %>%
  mutate(in_slope_tolerance = ifelse(slope <=-0.1 | slope >=0.1 | is.na(slope) == TRUE, FALSE, TRUE)) %>%
  select(-slope) %>%
  mutate(in_cps_tolerance = ifelse(cps <=30000 | cps >=60000 | is.na(cps) == TRUE, FALSE, TRUE)) %>%
  mutate(in_mse_tolerance = ifelse(MSE <=2, TRUE, FALSE)) %>%
  rowwise() %>%
  mutate(qc = !any(c(validity, in_slope_tolerance, in_cps_tolerance, in_mse_tolerance) == FALSE)) %>%
  ungroup() %>%
  select(-c(in_slope_tolerance, in_cps_tolerance, in_mse_tolerance)) %>%
  
  ggplot(data = CD166_19_xrf, aes(y = depth, x = `Mo inc`/`Mo coh`, col = qc)) + 
  geom_lineh(aes(group = 1)) +
  geom_rug(sides = "l", data = . %>% filter(qc == FALSE)) +
  scale_color_discrete(name = "pass QC") +
  scale_y_reverse(name = "depth [mm]")
```

Bear in mind that this doesn't remove observations considered defective, only marks them has `qc == FALSE`. If they are to be excluded from subsequent analysis, they should be removed using `filter(qc == TRUE)`.

## Tube current

Although element peak areas are not exactly linearly related to tube current, they can be approximated to a linear relationship (see @Jarvis2015). The `Q-Spec` software can report either peak areas (n) or intensities (n/mA), and it is easy to tell which you are using: peak areas are always integers, but intensities are always fractions. It is easy to adjust from one to the other:

```{r current correction}
current <- as.numeric(CD166_19_S1$metadata[18, 2])
  CD166_19_S1$xrf %>%
  mutate(intensity_Fe = round(Fe/current, 3)) %>%        # convert to intensity
  mutate(peakarea_Fe  = round(intensity_Fe*current)) %>% # convert to peak area
  select(Fe, intensity_Fe, peakarea_Fe) %>%
  head()
```

```{r include=FALSE}
rm(current)
```

## Dead Time

In silicone drift ED-XRF detectors the electronics are only so fast. This limits the flux of photons into the detector that can be measured. When the count rate is high, the detector will not be making observations whilst the electronics "catch-up"; this is the dead-time, and here it is expressed as a fraction of the overall dwell time. When the system is optimised variations in dead-time are usually small enough to be ignored, but where the matrix or configuration leads to large variations in dead-time they should be corrected for. 

```{r deadtime, warning=FALSE}
ggplot(data = CD166_19_xrf, aes(x = depth, y = Dt)) +
  scale_x_reverse() + 
  scale_y_continuous(sec.axis = sec_axis( trans=~(.+(1-mean(CD166_19_xrf$Dt, na.rm = TRUE))), name="Correction Factor")) +
  geom_line() +
  geom_hline(yintercept = mean(CD166_19_xrf$Dt, na.rm = TRUE), linetype = "dotted")
```

```{r dt_correction}
CD166_19_xrf %>%
  mutate(newDt = Dt+(1-mean(CD166_19_xrf$Dt, na.rm = TRUE))) %>% 
  mutate(across(any_of(elementsList)) * newDt) %>%
  select(-newDt) %>%
  head()
```

## Correcting for Water Content

Water content can be corrected for as a simple dilution effect, for example, if a sample contains 50% water by weight, the peak intensity can be doubled. The water content must be determined using some other lab method (e.g. loss-on-ignition), and it is likely that this can only be performed on samples larger than the original scan interval (e.g. 10 mm subsamples). A conservative approach is to use the `itrax_averaging()` function to re-sample the XRF data to the resolution of the water content data, or the reverse (upsampling the water content data) might be applied with caution. For some scans the Mo inc./Mo coh. varies with water content and can be used as a proxy to correct for water content. 

## Correcting Grain Size

Grain size variations are known the affect the detection of some elements. Grain size determinations made using analytical equipment (e.g. a laser granulometer) can be used to correct for grain size. 

## Uncertainties

The best way to quantify the uncertainties in the XRF data is in repeat measurements. The example dataset contains short sections of repeat measurements. An example might begin by reading in the repeat scan data, as below for `CD166_19_S1`. 

```{r uncertainties}
CD166_19_S1_REP1 <- itrax_import("CD166_19_S1/CD166_19_S1_REP1/Results.txt") %>%
  select(any_of(c(elementsList, "position", "Mo inc", "Mo coh"))) %>%
  mutate(scan = "scan1")
CD166_19_S1_REP2 <- itrax_import("CD166_19_S1/CD166_19_S1_REP2/Results.txt") %>%
  select(any_of(c(elementsList, "position", "Mo inc", "Mo coh"))) %>%
  mutate(scan = "scan2")
CD166_19_S1_REP3 <- CD166_19_S1$xrf %>%
  filter(position >= min(c(CD166_19_S1_REP1$position, CD166_19_S1_REP2$position)) & 
           position <= max(c(CD166_19_S1_REP1$position, CD166_19_S1_REP2$position))) %>%
  select(any_of(c(elementsList, "position", "Mo inc", "Mo coh"))) %>%
  mutate(scan = "scan3")
```

It should then be combined into a single dataset, and finally, a function to calculate the uncertainties is called (here we use `sd()`). The `errors` package is used to create an output that can combine both the value and the associated uncertainty. It is a really neat way of working with quantities and ensures the uncertainties are correctly propagated throughout the work. 

```{r message=FALSE}
S1_reps <- list(CD166_19_S1_REP1, CD166_19_S1_REP2, CD166_19_S1_REP3) %>%
  reduce(full_join) %>%
  select(scan, position, everything()) %>%
  group_by(position) %>%
  summarise(across(any_of(c(elementsList, "Mo inc", "Mo coh")), 
                   function(x){set_errors(x = mean(x, na.rm = TRUE), 
                                          value = sd(x, na.rm = TRUE))}))
```

```{r include=FALSE}
rm(CD166_19_S1_REP1, CD166_19_S1_REP2, CD166_19_S1_REP3)
```
 
These can be plotted using the mean and standard deviation instead of a single value using `errors::errors_min()` and `errors::errors_max()` really easily. Note that the errors are propagated through any arithmetic - in the example below, where the ratio of coherent Mo and incoherent Mo peak area is calculated.

```{r message=FALSE, warning=FALSE}
S1_reps %>%
  mutate(`coh/inc` = `Mo coh`/`Mo inc`) %>%
  select(Al, Si, Ti, Fe, Pb, Ca, `coh/inc`, position) %>%
  pivot_longer(!c("position"), names_to = "elements", values_to = "peakarea") %>% 
  drop_na() %>%
  mutate(elements = factor(elements, levels = c(elementsList, "coh/inc"))) %>%

  ggplot(aes(x = peakarea, y = position)) +
  scale_y_reverse() + 
  geom_ribbonh(aes(xmin = errors_min(peakarea), xmax = errors_max(peakarea)), fill = "grey80") +
  geom_lineh() + 
  scale_x_continuous(n.breaks = 3) +
  facet_wrap(vars(elements), scales = "free_x", nrow = 1) + 
  theme_paleo()
```

This information can be used to inform decisions about the inclusion and treatment of the remainder of the data - where there are unacceptably high uncertainties that element might be considered for exclusion from further analysis. 

```{r include=FALSE}
rm(S1_reps)
```

## Noisy Data

It is not always possible to do repeat scans for entire scanning projects, and as such it may be necessary to look at others ways of identifying problematic noisy data. Much of this can be done by good judgement on the part of the analyst, and will depend on the requirements of the projects. However, there are some analytical tools that can help. One method is to look at the autocorrelation of a time-series. We can use the auto-correlation factor (ACF) to explore this for our data. 

We might begin with examples for just two elements, Ca and Pb. It is clear from these plots that whereas Ca has the expected pattern of autocorrelation, Pb has generally much lower and a more disordered pattern of autocorrelation. 

```{r}
library(forecast)
egg::ggarrange(
  ggAcf(CD166_19_xrf$Ca) + 
    ylim(c(NA,1)),
  ggAcf(CD166_19_xrf$Pb) + 
    ylim(c(NA,1)),
  nrow = 1)
```
Now we might move on to making calculations for the entirety of the data. In the example below we apply the `Acf()` function to each of the elements. We might then explore the data by sorting for an arbitrary lag, or by plotting the results together for inspection. In this case, the elements have been sorted by the autocorrelation value at a lag of 5. Although the visualisation is a bit messy with all the elements plotting over one another, it is clear that some elements exhibit very low autocorrelation (e.g. I, Pb, Sc, Ge, Cs, P) and could be possible candidates for exclusion. 

```{r warning=FALSE}
apply(CD166_19_xrf %>% select(any_of(elementsList)), 2, FUN = function(x){round(Acf(x, plot = F)$acf, 3)}) %>%
  as_tibble(rownames = "lag") %>%
  pivot_longer(!c("lag"), names_to = "elements", values_to = "value") %>%
  mutate(lag = as.numeric(lag),
         elements = factor(elements, levels = filter(., lag == 5) %>% arrange(desc(value)) %>% pull(elements))) %>%
  group_by(elements) %>%
  ggplot(aes(x = lag, y = value, col = elements)) +
  geom_line()
```

It is worth noting that this analysis has been performed on the entirety of the data, but there is no reason why the data could be problematic for some facies and acceptable for others. It may be necessary to perform some facies analysis and perform these analyses per facies. 

Putting all the selection criteria together could look something like the example below. 

```{r warning=FALSE}
# identify acceptable variables
apply(CD166_19_xrf %>% select(any_of(elementsList)), 2, FUN = function(x){round(Acf(x, plot = F)$acf, 3)}) %>%
  as_tibble(rownames = "lag") %>%
  pivot_longer(!c("lag"), names_to = "elements", values_to = "value") %>%
  mutate(lag = as.numeric(lag),
         elements = factor(elements, levels = filter(., lag == 5) %>% arrange(desc(value)) %>% pull(elements))) %>%
  group_by(elements) %>%
  filter(lag == 5) %>%
  filter(value >= 0.8) %>%
  pull(elements) %>% 
  ordered() -> myElements

# get acceptable observations
CD166_19_xrf %>% 
  filter(qc == TRUE) %>%
  
  # pivot long
  select(any_of(myElements), depth, label) %>% 
  tidyr::pivot_longer(!c("depth", "label"), names_to = "elements", values_to = "peakarea") %>% 
  mutate(elements = factor(elements, levels = c(elementsList, "coh/inc"))) %>%
  
  # plot
  ggplot(aes(x = peakarea, y = depth)) +
    tidypaleo::geom_lineh(aes(color = label)) +
    scale_y_reverse() +
    scale_x_continuous(n.breaks = 2) +
    facet_geochem_gridh(vars(elements)) +
    labs(x = "peak area", y = "Depth [mm]") +
    tidypaleo::theme_paleo() +
    theme(legend.position = "none")
```

## Reversing Data

Sometimes a core section goes in the scanner backwards! If this has happened, the functions below simply re-map the `position` and `depth` data for the radiograph, the optical image and the xrf data. These functions can either be called at the same time as the data itself (for example in a plot), or used to over-write the original data itself.

```{r reverse_functions}
# for xrf data
reverse_xrf <- function(xrf = itrax_import()){
  xrf %>% 
    mutate(depth = rev(depth),
           position = rev(position)) %>%
    arrange(position) %>%
    return()
}

# for images
reverse_image <- function(image = itrax_image()){
  image <- image[dim(image)[1]:1, , ]
  row.names(image) <- rev(row.names(image))
  return(image)
}

# for radiographs
reverse_radio <- function(image = itrax_radiograph()){
  image <- image[dim(image)[1]:1, ]
  row.names(image) <- rev(row.names(image))
  return(image)
}
```

## Visualising Raw Data

A useful tool for investigating areas of high errors or other problems in the scan data is to visualise the raw spectral data. This shows a heat map of the spectra, and can be used to identify areas where particular spectral lines appear and disappear along the core. At its simplest, it can be used to produce a plot of a single scan section. 

```{r restspectra}
itrax_restspectra(foldername = "CD166_19_S1/CD166_19_S1/XRF data/",
                  parameters = "CD166_19_S1/CD166_19_S1/Results_ settings.dfl"
                  ) %>% 
  invisible()
```

By combining all of the sections, and joining the raw spectral data with the processed data from the `Results.txt` `Q-Spec` output file, it can be used as a very powerful diagnostic tool.

```{r spectra2}
# import spectral data
read_spectra <- function(foldername, labeltext){itrax_restspectra(foldername = foldername,
                                                       plot = FALSE) %>%
    mutate(label = labeltext) %>%
    select(label, everything()) %>%
    mutate(filename = str_split(filename, pattern = "//") %>% sapply(., `[`, 2))}

# import the channel energy information
settings <- itrax_qspecsettings("CD166_19_S1/CD166_19_S1/Results_ settings.dfl")

# import and join them
left_join(CD166_19_xrf %>%
  select(depth, validity, filename, label) %>%
  mutate(filename = filename %>%
           str_split(pattern = "\\\\") %>%
           sapply(., `[`, 7)), 
  bind_rows(read_spectra(foldername = "CD166_19_S1/CD166_19_S1/XRF data/", labeltext = "S1"), 
            read_spectra(foldername = "CD166_19_S2/CD166_19_S2/XRF data/", labeltext = "S2"),
            read_spectra(foldername = "CD166_19_S3/CD166_19_S3/XRF data/", labeltext = "S3")) %>%
    mutate(label = as.factor(label)), 
  by = c("filename", "label")) %>%

  pivot_longer(cols = -c(depth, validity, filename, label, position),
               names_to = "channel",
               values_to = "counts") %>%
  mutate(channel = as.numeric(channel)) %>%
  select(-c("filename", "validity", "position")) %>%

ggplot(aes(x = channel, y = depth, fill = counts)) +
  geom_tile() +
  scale_fill_gradient(name = "value",
                      trans = "pseudo_log",
                      low = "#132B43",
                      high = "#56B1F7",
                      labels = round) +
  scale_y_reverse(breaks = seq(from = 0, to = max(CD166_19_xrf$depth), by = 500),
                  name = "depth [mm]") +
  scale_x_continuous(name = "channel [n]",
                     sec.axis = sec_axis(trans = ~ ((. * as.numeric(settings[1,2])) + as.numeric(settings[2,2])),
                                         name = "energy [k eV]")) +
  guides(fill = FALSE) +
  facet_grid(rows = vars(label),
             scales = "free_y",
             space = "free_y")

rm(settings, read_spectra)
```

To inspect individual spectra, you can plot them from the object returned from `itrax_restspectra()`. 

```{r}
itrax_spectra(filename = "CD166_19_S1/CD166_19_S1/XRF data/L000642.spe",
              parameters = "CD166_19_S1/CD166_19_S1/Results_ settings.dfl") %>% 
  invisible()
```
