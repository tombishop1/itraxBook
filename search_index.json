[["index.html", "Using Itrax Data in R Preface 0.1 Why did I write this book? 0.2 Prerequisites 0.3 Example Data", " Using Itrax Data in R Thomas Bishop 2020-11-10 Preface 0.1 Why did I write this book? During the COVID-19 pandemic in 2020 it became necessary to formalise (or at least, organise) the support offered to students using the Itrax core scanner at The University of Manchester Geography Laboratories so they could be offered online. This led to an unexpectedly popular series of seminars. When these came to an end, we decided it would be useful to summarise our seminars into a reference guide that could be updated over time. This is that guide. 0.2 Prerequisites This guide assumes a basic knowledge of R and the tidyverse, including data types, assignments, and pipes. It also assumes a background knowledge of the core scanner and the nature of the data it produces; see Croudace et al. (2019) and references therein. Some of the sections on data analysis assume some knowledge of compositional data analysis. 0.3 Example Data All of the examples in this book use scans from a sequence of marine sediment cores obtained during a cruise of the RRS Charles Darwin CD166 on the 6th of November 2004. The 4.3 meter sequence was obtained at station CD16619-01 (31º30.794’N, 17º11.777’W) from a depth of 4,467 meters. It is notable for containing a series of volcanic “bypass” turbidites. The scans were performed at the British Ocean Sediment Core Research Facility (BOSCORF) at the National Oceanographic Centre, Southampton in 2020. The drive is a sequence of three consecutive cores, and have been scanned for photographic and radiographic line-scan imagery, as well as three repeats of x-ray flourescence measurements. The data can be downloaded from the Github pages that form the source for this document, available at github.com/tombishop1/itraxBook. References "],["intro.html", "Chapter 1 Data Structure 1.1 Metadata 1.2 XRF Data 1.3 Optical Images 1.4 Radiographic Images 1.5 Magnetic Susceptability", " Chapter 1 Data Structure The Itrax core scanner is a multi-sensor device, with seperate data outputs for different measurements and uses. They are described in the following sections. Note that your data may not contain all of these objects, depending on the exact scanner, configuration, or data repository you use. The folder structure may vary between operators, but typically there will be a folder for each scan section, and each will contain the data described in the following sections. Note that where radiographs and XRF data have been aquired using different step-sizes (measurement intervals), the operator will create seperate scan sections (folders) for the x-radiograph and XRF measurement. This is because a scan section can only have a single fixed step-size. For example, it is not uncommon for users to require an step-size of 200 micrometers for the x-radiograph, but only 1 millimeter for the XRF measurement. 1.1 Metadata Every scan section has a document.txt file that contains information about the parameters of the scan. For example, it contains the current and voltage settings used for the x-ray tube, the step size, and the start and stop positions of the scan. The file can be opened using any text editor, or can be easily parsed in R using itraxR::itrax_meta(). Sometimes this information is required to process other parts of the data, and as such it is an important part of the overall data package. 1.2 XRF Data The XRF data can be split into two groups — “raw” and “processed” data. The raw data is contained in a seperate folder called XRF data, and consists of a single file, beginning with L000000.spe and incrementing sequentially. This file can be read using a text editor and is tab-delimited. The first part is a header, containing metadata information. The second part is a table of all the channels of the detector and the corresponding count for each channel. Increasing channel numbers represent increasing energy, but some thought needs to be given to calibrating channels into an energy — this step is usually performed using specialist software. In addition, a file called sumspectra.spe is often included in the root directory; this is simply the sum of all the *.spe files in the XRF data directory, and is sometimes useful in processing the data. Processed data comes from the Q-Spec software (Cox Analytical Systems, Sweden) provided with the machine. Its function is to process the spectral data files (*.spe) into peak-areas for each element of interest by fitting a model to the data. The model needs some user input and intervention to optimise it, and the quality of the model can be assessed using a number of diagnostic parameters, the most important being the RMSE. The Q-Spec software can also perform some calibration of the data, although this is a less typical use-case. Often the operator will include a file that contains all of the settings used by Q-Spec to translate the raw data files into the peak area outfile file — this file will have the extension *.dfl and will often simply be called settings.dfl. It comes in the form of a table, with a single row for each measurement step, and a column for parameters including individual element peak areas in counts (n) or intensities (n/mA). The data files commonly have names like result.txt or Results.txt, but may have been subsequently renamed. 1.3 Optical Images The scanner collects good quality optical images that have consistent lighting and because they are line-scan images, they do not suffer from optical distortions. Medium resolution images are usually included in all scan section folders (typically optical.tif), and optional high-resolution images are often included by operators elsewhere. High resolution images are typically supplied as both 8-bit and 16-bit images, and are usually hundreds of megabytes in size. Although the brightness of the lighting in the scanner is adjustable, images sometimes need to be brightened and/or have the contrast adjusted. This can be performed in desktop publishing software (e.g. Adobe Photoshop, Corel Photo-Draw), but it is easiest to use the open-source scientific image analysis software imageJ (NIH, USA). By including an appropriate colour reference card in the scan the image can be calibrated, although it is often desirable to increase the contrast to elucidate features of the core. The photograph will of the entire length of the bed scanned, rather than cropped to the limits of an individual scan section. If multiple scan sections are placed on the bed and are scanned together, it will include all of the scanned sections. The image needs to be cropped using the metadata for the relevant scan section; this process is covered in later chapters. 1.4 Radiographic Images The scanner has an x-radiographic line array capable of producing good-quality x-radiographs of the cores. The scan data can be split into two parts — “raw” and “processed” data. The raw data (usually radiograph.raw) is a tab-delimited text file containing a matrix of greyscale values. Each column represents a single step (measurement interval), and each row represents a single pixel on the line array. The pixel spacing is around 20 microns. The processed image (radiograph.tif) has a lower resolution than the raw data. This is because the pixels must be square, and so the pixels are down-sampled to fit with the step-size of the scan. Thus, if the step-size was 200 micrometers, each pixel will be 200 x 200 micrometers, whereas the raw data will have rectangular pixels with dimensions of 20 x 200 micrometers. Like the optical image, the radiograph often requires contrast and brightness adjustments, and these are easiest to perform in imageJ. With the inclusion of a suitable density standard, some relative or, where “u-channels” are used, absolute density calibration can be performed using these data. 1.5 Magnetic Susceptability Some scanners include a Bartington MS2E surface sensor. "],["importing.html", "Chapter 2 Importing Data 2.1 Metadata 2.2 XRF Data 2.3 Optical Images 2.4 Radiographic Images 2.5 Magnetic Susceptability 2.6 Importing Everything", " Chapter 2 Importing Data All of the Itrax data is in either text-format or “tagged image format” (*.tif). Although this means it is easily read by the various import functions available in R, it still needs considerable cleaning and wrangling to get it to a point where it is useable for most analyses. There are three possible approaches to this task: Use existing functions published in the itraxR package available from github.com/tombishop1/itraxR. These are at an early stage and functionality might be broken, but are largely convenience functions for wrangling and analysing Itrax data. It is easy to install the package directly from Github using remotes::install_github(\"tombishop1/itraxR\"). Work in base R to wrangle the data. This is perfectly achievable, and much of the current itraxR functionality was originally written this way. Work in the tidyverse family of packages and style. For data wrangling tasks, this approach can result in simpler and more resilient code. In this guide examples will be given mostly using the tidyverse, and reference will be made to the convenience functions in itraxR. 2.1 Metadata The scan data file document.txt can be quickly parsed using itraxR::itrax_meta(). The output is a dataframe from which the individual components can be easily accessed through subsetting functions, for example as.numeric(itrax_meta()[6:7, 2]) would return a numeric vector of the start and end position of a scan. The file is tab-delimited, but the formatting is not consistent and will need considerable re-organising and tidying to get it all usable. itrax_meta(&quot;CD166_19_S1/CD166_19_S1/document.txt&quot;) ## Parameter Value Unit ## 1 Sample name CD166_19_S1 str ## 2 Section name CD166_19_S1 str ## 3 Aquisition date 22/9/2020 dd/mm/yyyy ## 4 Operator name MC str ## 5 Tube Mo element ## 6 Start coordinate 31.5 mm ## 7 Stop coordinate 1314.1 mm ## 8 Step size 1000 microns ## 9 Optical Start 0.5 mm ## 10 Optical End 1401.3 mm ## 11 Optical step size 0.188 mm ## 12 Rad. voltage 55 kV ## 13 Rad. current 50 mA ## 14 Rad. exposure 0 ms ## 15 line camera signal level 323154 at 25 ms ## 16 XRF ON ON/OFF ## 17 XRF voltage 30 kV ## 18 XRF current 30 mA ## 19 XRF exp. time 15 seconds ## 20 Start temperature \\xb0C ## 21 Stop temperature \\xb0C ## 22 Start humidity % ## 23 Stop humidity % ## 24 Start vacuum -95.0 kPa ## 25 Stop vacuum -94.8 kPa 2.2 XRF Data 2.2.1 Processed Data This is the data most commonly used in analysis and it can be quickly imported using itraxR::itrax_import(). Note that, like for the example data, it is possible to have more than one processed data file. Typically cores have at least two, one created at the time of the scan based on settings for a single point near the top of the sequence, and another from a holistic re-analysis of the sequence. CD166_19_S1 &lt;- itrax_import(&quot;CD166_19_S1/CD166_19_S1/Results.txt&quot;, depth = 0) head(CD166_19_S1) depth MSE cps validity Al Si P S Cl K Ca Sc Ti V Cr Mn Fe Ni Cu Zn Ga Ge Br Rb Sr Y Zr Pd Cd I Cs Ba Nd Sm Yb Ta W Pb Bi Mo inc Mo coh position 1 1.41 34525 TRUE 76 275 0 10 1318 2565 112734 0 1661 51 258 508 35168 123 277 205 4 0 491 329 8306 15 255 31 13 38 0 63 17 43 144 789 1992 77 72 26323 8799 32.54 2 1.57 38370 TRUE 57 306 0 0 1513 2628 144287 14 1806 88 326 559 36494 104 230 268 48 40 605 76 9181 140 322 78 34 48 0 48 67 46 282 776 2019 0 136 26778 9117 33.54 3 1.55 39796 TRUE 74 330 0 32 1470 2378 162938 0 2121 22 301 483 35952 134 150 123 0 43 550 295 9644 119 280 75 55 80 0 25 44 39 158 703 2016 13 199 26550 9303 34.54 4 1.41 40022 TRUE 26 206 0 21 1312 2265 153194 0 2031 75 306 485 35512 144 206 239 0 0 609 313 9940 107 475 66 24 67 0 60 75 64 218 794 2160 152 134 28310 9886 35.54 5 1.41 41973 TRUE 53 233 0 37 1740 2947 135879 106 1826 89 440 738 44303 166 276 240 0 0 659 304 10287 180 160 31 34 23 0 132 42 36 163 919 2344 54 157 31356 10140 36.54 6 1.36 41268 TRUE 27 347 0 28 1576 3179 132183 14 1923 107 401 792 45283 151 250 294 40 48 709 318 9917 106 312 72 28 58 0 93 97 12 184 852 2336 54 182 30631 9968 37.54 If you’d like more control over the import process, a typical Tidyverse workflow would consist of reading the file (they are tab-delimited), removing variables that are not of interest, and correctly labelling data that should be NA. You may also wish to assign a coring depth information at this stage. Firstly, we need to load the packages we will use, and define a list of elements and other names of variables we might expect to find in the files. library(readr) library(dplyr) library(janitor) elements &lt;- c( &quot;H&quot;, &quot;He&quot;, &quot;Li&quot;, &quot;Be&quot;, &quot;B&quot; , &quot;C&quot; , &quot;N&quot;, &quot;O&quot;, &quot;F&quot; , &quot;Ne&quot;, &quot;Na&quot;, &quot;Mg&quot;, &quot;Al&quot;, &quot;Si&quot;, &quot;P&quot;, &quot;S&quot;, &quot;Cl&quot;, # &quot;Ar&quot;, &quot;K&quot;, &quot;Ca&quot;, &quot;Sc&quot;, &quot;Ti&quot;, &quot;V&quot;, &quot;Cr&quot;, &quot;Mn&quot;, &quot;Fe&quot;, &quot;Co&quot;, &quot;Ni&quot;, &quot;Cu&quot;, &quot;Zn&quot;, &quot;Ga&quot;, &quot;Ge&quot;, &quot;As&quot;, &quot;Se&quot;, &quot;Br&quot;, &quot;Kr&quot;, &quot;Rb&quot;, &quot;Sr&quot;, &quot;Y&quot;, &quot;Zr&quot;, &quot;Nb&quot;, &quot;Mo&quot;, &quot;Tc&quot;, &quot;Ru&quot;, &quot;Rh&quot;, &quot;Pd&quot;, &quot;Ag&quot;, &quot;Cd&quot;, &quot;In&quot;, &quot;Sn&quot;, &quot;Sb&quot;, &quot;Te&quot;, &quot;I&quot;, &quot;Xe&quot;, &quot;Cs&quot;, &quot;Ba&quot;, &quot;La&quot;, &quot;Ce&quot;, &quot;Pr&quot;, &quot;Nd&quot;, &quot;Pm&quot;, &quot;Sm&quot;, &quot;Eu&quot;, &quot;Gd&quot;, &quot;Tb&quot;, &quot;Dy&quot;, &quot;Ho&quot;, &quot;Er&quot;, &quot;Tm&quot;, &quot;Yb&quot;, &quot;Lu&quot;, &quot;Hf&quot;, &quot;Ta&quot;, &quot;W&quot;, &quot;Re&quot;, &quot;Os&quot;, &quot;Ir&quot;, &quot;Pt&quot;, &quot;Au&quot;, &quot;Hg&quot;, &quot;Tl&quot;, &quot;Pb&quot;, &quot;Bi&quot;, &quot;Po&quot;, &quot;At&quot;, &quot;Rn&quot;, &quot;Fr&quot;, &quot;Ra&quot;, &quot;Ac&quot;, &quot;Th&quot;, &quot;Pa&quot;, &quot;U&quot;, &quot;Np&quot;, &quot;Pu&quot;, &quot;Am&quot;, &quot;Cm&quot;, &quot;Bk&quot;, &quot;Cf&quot;, &quot;Es&quot;, &quot;Fm&quot;, &quot;Md&quot;, &quot;No&quot;, &quot;Lr&quot;, &quot;Unq&quot;, &quot;Unp&quot;, &quot;Unh&quot;, &quot;Uns&quot;, &quot;Uno&quot;, &quot;Une&quot;, &quot;Unn&quot; ) others &lt;- c( &quot;position (mm)&quot;, &quot;sample.surface&quot;, &quot;MSE&quot;, &quot;cps&quot;, &quot;validity&quot;, &quot;Mo inc&quot;, &quot;Mo coh&quot;, &quot;Cr inc&quot;, &quot;Cr coh&quot; ) Secondly, we import the file, remove empty rows and columns, and change the data type of the validity flag. df &lt;- read_tsv(&quot;results.txt&quot;, skip = 2) %&gt;% remove_empty() %&gt;% mutate(validity = as.logical(validity)) Next we can select only the variables we need. The code below leaves anything that is a chemical element or in our list of other useful parameters, but you can change these depending on your needs. df &lt;- df %&gt;% select(any_of(c(elements, others))) If you have coring depth information, you could add it now. A simple calculation can be made using the position variable (demonstrated below, here assuming the top of the core is at a depth of 400 mm), or a more advanced calculation as you require. It is also possible to add ages derived from an age/depth model now. The final line re-orders the variables so the depth variable is the first. df &lt;- df %&gt;% rename(position = `position (mm)`) %&gt;% mutate(depth = position - min(position) + 400) %&gt;% select(depth, everything()) Finally, a quirk of the formatting of data where validity == FALSE needs to be addressed. These cells have a value of 0, but they could be confused with truely 0 value cells that have validity == TRUE. The solution is to convert them to NA as follows. Note that for metadata variables (depth, sample.surface etc. the measurements are still valid, so should not be set to NA). df &lt;- full_join(df %&gt;% filter(validity == TRUE), df %&gt;% filter(validity == FALSE) %&gt;% select(any_of(c(elements, &quot;position&quot;))) %&gt;% na_if(0)) %&gt;% arrange(position) %&gt;% mutate(depth = df$depth, validity = df$validity, cps = df$cps, MSE = df$MSE) Finally, you may wish to do some quality control. The deletion criteria will be more fully discussed in the next chapter, but it is often the case that the first and last few measurements are of poor quality. The code below removes the first and last 10 mm of measurements, however depending on your workflow you may wish to simply flag it in some way. df &lt;- df %&gt;% filter(depth &gt; min(depth) + 10 &amp; depth &lt; max(depth) - 10) 2.2.2 Joining XRF Data Often a core (sometimes referred to as a drive) is comprised of a sequence of individual sections, which may or may not be overlapping. Often we will want to integrate them into a continuous dataset for analytical purposes. When joining cores that do not overlap, this process is trivial — the data might simply appended in order of depth, and a new column is added with the identity of the original core section. Where overlapping cores are present, there can be multiple measurements at a single depth (on different cores). In these cases not only will the individual measurements need to be re-ordered by depth, but an additional variable should be created that can be used in combination or alone to uniquely identify each measurement. The code below does this by creating an additional variable called label, with the name of the original core given in the named list. mylist &lt;- list(core1 = core1, core2 = core2) df &lt;- lapply(names(mylist), function(i) within(mylist[[i]], {label &lt;- i})) %&gt;% bind_rows() %&gt;% arrange(depth) This process can be simplified using itraxR::itrax_join(), for example: # import the core sections CD166_19_S1 &lt;- itrax_import(&quot;CD166_19_S1/CD166_19_S1/Results.txt&quot;, depth_top = 0) CD166_19_S2 &lt;- itrax_import(&quot;CD166_19_S2/CD166_19_S2/Results.txt&quot;, depth_top = max(CD166_19_S1$depth)) CD166_19_S3 &lt;- itrax_import(&quot;CD166_19_S3/CD166_19_S3/Results.txt&quot;, depth_top = max(CD166_19_S2$depth)) #join them together CD166_19 &lt;- itrax_join(list(S1 = CD166_19_S1, S2 = CD166_19_S2, S3 = CD166_19_S3)) str(CD166_19) ## tibble [4,206 × 43] (S3: tbl_df/tbl/data.frame) ## $ depth : num [1:4206] 1 2 3 4 5 6 7 8 9 10 ... ## $ MSE : num [1:4206] 1.41 1.57 1.55 1.41 1.41 1.36 1.52 1.38 1.58 1.52 ... ## $ cps : num [1:4206] 34525 38370 39796 40022 41973 ... ## $ validity: logi [1:4206] TRUE TRUE TRUE TRUE TRUE TRUE ... ## $ Al : num [1:4206] 76 57 74 26 53 27 72 78 69 70 ... ## $ Si : num [1:4206] 275 306 330 206 233 347 337 346 403 381 ... ## $ P : num [1:4206] 0 0 0 0 0 0 0 0 0 0 ... ## $ S : num [1:4206] 10 0 32 21 37 28 59 19 44 30 ... ## $ Cl : num [1:4206] 1318 1513 1470 1312 1740 ... ## $ K : num [1:4206] 2565 2628 2378 2265 2947 ... ## $ Ca : num [1:4206] 112734 144287 162938 153194 135879 ... ## $ Sc : num [1:4206] 0 14 0 0 106 14 0 46 0 0 ... ## $ Ti : num [1:4206] 1661 1806 2121 2031 1826 ... ## $ V : num [1:4206] 51 88 22 75 89 107 0 71 23 91 ... ## $ Cr : num [1:4206] 258 326 301 306 440 401 440 407 449 336 ... ## $ Mn : num [1:4206] 508 559 483 485 738 792 799 868 794 647 ... ## $ Fe : num [1:4206] 35168 36494 35952 35512 44303 ... ## $ Ni : num [1:4206] 123 104 134 144 166 151 125 157 180 147 ... ## $ Cu : num [1:4206] 277 230 150 206 276 250 217 178 191 197 ... ## $ Zn : num [1:4206] 205 268 123 239 240 294 284 238 230 211 ... ## $ Ga : num [1:4206] 4 48 0 0 0 40 8 22 26 0 ... ## $ Ge : num [1:4206] 0 40 43 0 0 48 115 100 80 164 ... ## $ Br : num [1:4206] 491 605 550 609 659 709 583 582 635 545 ... ## $ Rb : num [1:4206] 329 76 295 313 304 318 337 323 223 306 ... ## $ Sr : num [1:4206] 8306 9181 9644 9940 10287 ... ## $ Y : num [1:4206] 15 140 119 107 180 106 52 77 112 144 ... ## $ Zr : num [1:4206] 255 322 280 475 160 312 319 319 552 484 ... ## $ Pd : num [1:4206] 31 78 75 66 31 72 31 54 58 70 ... ## $ Cd : num [1:4206] 13 34 55 24 34 28 56 62 96 79 ... ## $ I : num [1:4206] 38 48 80 67 23 58 30 53 93 43 ... ## $ Cs : num [1:4206] 0 0 0 0 0 0 0 0 0 0 ... ## $ Ba : num [1:4206] 63 48 25 60 132 93 172 108 108 22 ... ## $ Nd : num [1:4206] 17 67 44 75 42 97 92 68 56 59 ... ## $ Sm : num [1:4206] 43 46 39 64 36 12 24 69 68 17 ... ## $ Yb : num [1:4206] 144 282 158 218 163 184 248 166 188 169 ... ## $ Ta : num [1:4206] 789 776 703 794 919 852 926 840 847 790 ... ## $ W : num [1:4206] 1992 2019 2016 2160 2344 ... ## $ Pb : num [1:4206] 77 0 13 152 54 54 75 111 78 0 ... ## $ Bi : num [1:4206] 72 136 199 134 157 182 151 185 116 164 ... ## $ Mo inc : num [1:4206] 26323 26778 26550 28310 31356 ... ## $ Mo coh : num [1:4206] 8799 9117 9303 9886 10140 ... ## $ position: num [1:4206] 32.5 33.5 34.5 35.5 36.5 ... ## $ label : chr [1:4206] &quot;S1&quot; &quot;S1&quot; &quot;S1&quot; &quot;S1&quot; ... 2.2.3 Raw Data Sometimes it is useful to work with raw data rather than the calculated intensity data from the Q-Spec software. In this case, the raw data can be read directly from the individual files in the relevant directory. For individual measurements this is fairly trivial, although it must be considered that the data output is not calibrated to an energy and the data are in counts, not intensities. If the entire scan is read, some mechanism to iterate through the individual data files, adding them to a structured data object with relevant metadata (positions, for example) is required. 2.3 Optical Images In order to make the images usable for plotting beside other data, they need to be cropped to the scanned area and referenced to the position or a depth variable. Images in R are read in as a matrix with 3 dimensions (length, width, and the three colours). To begin with the start and end positions of the scan need to be established; this usually involves parsing the metadata file (e.g. itraxR::itrax_meta()[]), although in some circumstances they can be obtained from another data source (e.g. range(myData[,\"Position\"])). These then need to be mapped onto individual pixel values using the known size of a single pixel. Once this has been calculated, it is trivial to then subset the part of the image required. library(itraxR) library(tiff) image &lt;- readTIFF(&quot;optical.tif&quot;) meta &lt;- itrax_meta(&quot;document.txt&quot;) image_depth &lt;- rev(seq(from = as.numeric(meta[9, 2]), to = as.numeric(meta[10, 2]), length.out = dim(image)[2])) image &lt;- image[ , which(image_depth &gt; as.numeric(meta[6, 2]) &amp; image_depth &lt; as.numeric(meta[7, 2])) , ] 2.4 Radiographic Images The workflow for manipulating the processed radiographic images (*.tif) is very similar to that for the optical images, the main difference being the matrix only has two dimensions (length and width) as the image is greyscale. However, if there is a desire to manipulate the raw data from the radiographic image, some further work is required because the “pixel” is not square, but rectangular. That is to say the length of the pixel differs from its width. On the core scanner a single pixel has a width across the core of 20 micrometers, but has a variable coverage along the core (usually between 50 and 200 micrometers). The processed image downscales the pixel width to match the pixel length in order to force square pixels, losing some resolution along the way. In addition, it should be noted that unlike the optical images that always begin from position == 0, the radiographic images have defined start and end points just like an XRF scan, the parameters of which can be accessed from the document.txt metadata file (or by using itrax_meta()). 2.5 Magnetic Susceptability 2.6 Importing Everything Given the structured nature of individual core section scans, it might be helpful to import everything into a single data object. "],["tidying-data.html", "Chapter 3 Tidying Data", " Chapter 3 Tidying Data In the functionality provided by itraxR, the need for data cleaning is much reduced. However, you may still encounter poor quality data that needs removing from subsequent analysis. This can be broadly defined as: Data at the start and end of the the core, where a volume of core material is “missing”. Measurements where the optical configuration is out of position (marked as validity == 0), often due to holes or stones in the core. Areas of the core with low total counts. Individual measurements that are statistical outliers. The easiest way to do this is using a tidyverse style sequence of pipes that set the observations of faulty data as NA. "],["plotting.html", "Chapter 4 Plotting 4.1 Plotting XRF Data 4.2 Plotting Images", " Chapter 4 Plotting There are a number of plotting options included in base R and some paleoenvironmental packages like analouge, but here we will use ggplot2 and compatible packages. 4.1 Plotting XRF Data For simple biplots, ggplot2 provides the following solution. ggplot(data = na.omit(df), mapping = aes(x = depth, y = Mo.coh/Mo.inc)) + geom_line(aes(color = label)) + coord_flip() + scale_x_reverse() + labs(x = &quot;Depth [mm]&quot;,color = &quot;Core&quot;) + theme_classic() + theme(legend.position = &quot;none&quot;) Where a traditional stratigraphic plot is desired, the tidypaleo package provides useful functionality for producting these. However, the data does need to converted to long-form from the existing table. In this way tidypaleo::facet_geochem_gridh works in a way similar to ggplot::facet_wrap but has been creates a stratigraphic plot. library(tidypaleo) df %&gt;% mutate(`coh/inc` = Mo.coh/Mo.inc) %&gt;% select(Fe, Ti, `coh/inc`, Mn, depth, label) %&gt;% pivot_longer(!c(&quot;depth&quot;, &quot;label&quot;), names_to = &quot;elements&quot;, values_to = &quot;peakarea&quot;) %&gt;% drop_na() %&gt;% mutate(elements = factor(elements, levels = c(&quot;Fe&quot;, &quot;Mn&quot;, &quot;Ti&quot;, &quot;coh/inc&quot;))) %&gt;% ggplot(aes(x = peakarea, y = depth)) + geom_lineh(aes(color = label)) + scale_y_reverse() + facet_geochem_gridh(vars(elements)) + labs(x = &quot;peak area&quot;, y = &quot;Depth [mm]&quot;) + theme_paleo() + theme(legend.position = &quot;none&quot;) 4.2 Plotting Images Once an image file has been imported, the image can be integrated into ggplot code using the workflow below. library(tidyverse) library(grid) ggplot() + annotation_custom(rasterGrob(image2, width = unit(1, &quot;npc&quot;), height = unit(1, &quot;npc&quot;))) + xlab(&quot;&quot;) + ylab(&quot;Position [mm]&quot;) + ylim(as.numeric(meta[6:7, 2])) If the image needs to be rotated, the aperm() function needs to be used, followed by reversing the image. image2 &lt;- aperm(image, c(2,1,3)) image2 &lt;- image2[c(dim(image2)[1]: 1), , ] "],["transforming-data.html", "Chapter 5 Transforming Data 5.1 Ratios and Normalisation 5.2 Preparing Data for Multivariate Data Analysis 5.3 Data Reduction", " Chapter 5 Transforming Data The XRF data typically reported from the Itrax core scanner come from the spectral processing software Q-Spec. The output is usually in the form of an intensity, which is a dimensionless metric derived from the size of the spectral peak for a particular element, above the background Bremsstralung radiation, sometimes normalised for the tube current and/or counting time. 5.1 Ratios and Normalisation These data are compositional, and represent the changes in the relative proportions of all components of the matrix, measured and un-measured. As such it is likely the data will need transforming for certain types of multivariate analysis. As previously mentioned, these data are dimensionless, and as such do not represent a quantity, but are directly related to the absolute amount of a particular element in the matrix. It is often the case that ratios of elements are used to represent changes in composition — this is sometimes referred to as normalisation, or normalising one element against another. It is trivial to calculate element ratios, to the extent that these can often simply be calculated where they are required rather than saving them to memory. For example, if a plot of the Compton divided by the Rayleigh scatter was desired, there is no need to save the computed value to a new variable (e.g. coh_inc &lt;- df$Mo.coh/df$Mo.inc ) — simply define the calculation during plotting. 5.2 Preparing Data for Multivariate Data Analysis Where multivariate methods (cluster analysis, principle components analysis, correlation matricies) are required, it is usually necessary to transform data. This is because the statistical assumptions that underlie these methods are often not met when dealing with compositional data like that from an XRF core-scanner. There is no “right” or ideal way to deal with these issues, but a common method is to use a form of log transformation. Here we use a centred log transformation, which cannot be performed on zero values. df %&gt;% chemometrics::clr(df) There are a number of possible solutions to this problem of zero values, none ideal, but the most common are to add an arbitary number to the entirity of the data, or to replace zero values with a very small number, perhaps the limit of precision or the limit of detection. In the example shown, the limit of precision for the peak area intensity is used (0.001). input[input == 0] &lt;- 0.001 A final solution may be to exclude zero values from any subsequent analysis, although not all methods tolerate NA in the data. Where there are many zero values for a particular variable, the variable may not provide good data and could be excluded. df %&gt;% na_if(0) In most multivariate analyses there are a number of variables which have high signal-to-noise ratios or otheriwse only add noise to multivariate methods. In these cases, they might be excluded from multivariate methods. The selection of variables for inclusion is a matter for the analyst. It goes without saying that variables that are not part of the composition (that is, anything that is not an element) must be removed from the data used for multivariate analysis. df %&gt;% select(-any_of(c(&quot;Mg&quot;, &quot;Co&quot;, &quot;Mo&quot;)) Finally, in order to correctly identify the measurements to their original data source, it is necessary to use row names that uniquely identify observations. For single scans this is not usually an issue — the depth or position variable can be used. However, where a dataset is a composition of multiple cores, you may find that there are multiple observations for a particular depth where cores overlap. In the example shown we create a unique name for each observation by concatating the name of the core as recorded in the label column of a core sequence joined using itraxR::itrax_join() with the corresponding depth variable, but the code could be modified to suit a different workflow. rowlabels &lt;- str_c(df$label, df$depth, sep = &quot;_&quot;) input &lt;- df %&gt;% select(any_of(elements)) input &lt;- input %&gt;% select(-any_of(c(&quot;Mg&quot;, &quot;Co&quot;, &quot;Mo&quot;))) input[input == 0] &lt;- 0.001 library(chemometrics) input &lt;- clr(input) row.names(input) &lt;- rowlabels 5.3 Data Reduction Sometimes it is necessary to reduce the resolution of Itrax XRF data, usually to facilitate direct comparison with some other lower resolution data. For example, if calibrating using ICP-MS sub-samples taken at 10 mm intervals, but the Itrax XRF scan is at 0.2 mm, it will be necessary to average 50 Itrax measurements for each ICP-MS measurement. This is facilitated using the itrax_averaging() function. Despite the name, the function can use any appropriate summary function (e.g. standard deviation sd() or median median().) # get the Ti mean and standard deviation in 10 mm intervals CD166_19_S1 &lt;- itrax_import(&quot;CD166_19_S1/CD166_19_S1/Results.txt&quot;, depth_top = 0) tibble( depthmin = itrax_averaging(CD166_19_S1, 10)$depthmin, depthmax = itrax_averaging(CD166_19_S1, 10)$depthmax, Fe_mean = itrax_averaging(CD166_19_S1, 10)$Ti, Fe_sd = itrax_averaging(CD166_19_S1, 10, fun = sd)$Ti ) ## [90m# A tibble: 129 x 4[39m ## depthmin depthmax Fe_mean Fe_sd ## [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m ## [90m 1[39m 1 10 [4m2[24m101. 331. ## [90m 2[39m 11 20 [4m2[24m461. 307. ## [90m 3[39m 21 30 [4m2[24m222. 50.3 ## [90m 4[39m 31 40. [4m2[24m265. 119. ## [90m 5[39m 41. 50. [4m2[24m312. 164. ## [90m 6[39m 51. 60. [4m2[24m401. 90.6 ## [90m 7[39m 61. 70 [4m2[24m322 61.7 ## [90m 8[39m 71 80 [4m2[24m760. 165. ## [90m 9[39m 81 90 [4m2[24m997. 144. ## [90m10[39m 91 100 [4m3[24m099. 147. ## [90m# … with 119 more rows[39m "],["multivariate-methods.html", "Chapter 6 Multivariate Methods 6.1 Correlation Coefficients 6.2 Unconstrained Cluster Analysis 6.3 Constrained Cluster Analaysis 6.4 Principle Components Analysis", " Chapter 6 Multivariate Methods Input data often needs extensive preparation if useful results are to be obtained — transformation, dealing with zero values, and NA values. It is worth mentioning some general issues around the robustness of multivariate analysis. 6.1 Correlation Coefficients The Pearson’s R correlation coefficient is occasionally used to study the relationships between individual elements. 6.2 Unconstrained Cluster Analysis 6.3 Constrained Cluster Analaysis 6.4 Principle Components Analysis "],["calibrating-data.html", "Chapter 7 Calibrating Data 7.1 Suitable Methods 7.2 Linear Methods 7.3 Log-Ratio Methods", " Chapter 7 Calibrating Data 7.1 Suitable Methods 7.2 Linear Methods 7.3 Log-Ratio Methods "],["exporting-data.html", "Chapter 8 Exporting Data", " Chapter 8 Exporting Data "],["references.html", "References", " References "]]
