[
["index.html", "Using Itrax Data in R Preface 0.1 Why did I write this book? 0.2 Prerequisites", " Using Itrax Data in R Thomas Bishop 2020-07-14 Preface 0.1 Why did I write this book? During the COVID-19 pandemic in 2020 it became necessary to formalise (or at least, organise) the support offered to students using the Itrax core scanner at The University of Manchester Geography Laboratories so they could be offered online. This led to an unexpectedly popular series of seminars. When these came to an end, we decided it would be useful to summarise our seminars into a reference guide that could be updated over time. This is that guide. 0.2 Prerequisites This guide assumes a basic knowledge of R and the tidyverse, including data types, assignments, and pipes. It also assumes a background knowledge of the core scanner and the nature of the data it produces. Some of the sections on data analysis assume some knowledge of compositional data analysis. "],
["intro.html", "Chapter 1 Data Structure 1.1 Metadata 1.2 XRF Data 1.3 Optical Images 1.4 Radiographic Images 1.5 Magnetic Susceptability", " Chapter 1 Data Structure The Itrax core scanner is a multi-sensor device, with seperate data outputs for different measurements and uses. They are described in the following sections. Note that your data may not contain all of these objects, depending on the exact scanner, configuration, or data repository you use. The folder structure may vary between operators, but typically there will be a folder for each scan section, and each will contain the data described in the following sections. Note that where radiographs and XRF data have been aquired using different step-sizes (measurement intervals), the operator will create seperate scan sections (folders) for the x-radiograph and XRF measurement. This is because a scan section can only have a single fixed step-size. For example, it is not uncommon for users to require an step-size of 200 micrometers for the x-radiograph, but only 1 millimeter for the XRF measurement. 1.1 Metadata Every scan section has a document.txt file that contains information about the parameters of the scan. For example, it contains the current and voltage settings used for the x-ray tube, the step size, and the start and stop positions of the scan. The file can be opened using any text editor, or can be easily parsed in R using itraxR::itrax_meta(). Sometimes this information is required to process other parts of the data, and as such it is an important part of the overall data package. 1.2 XRF Data The XRF data can be split into two groups — “raw” and “processed” data. The raw data is contained in a seperate folder called XRF data, and consists of a single file, beginning with L000000.spe and incrementing sequentially. This file can be read using a text editor and is tab-delimited. The first part is a header, containing metadata information. The second part is a table of all the channels of the detector and the corresponding count for each channel. Increasing channel numbers represent increasing energy, but some thought needs to be given to calibrating channels into an energy — this step is usually performed using specialist software. In addition, a file called sumspectra.spe is often included in the root directory; this is simply the sum of all the *.spe files in the XRF data directory, and is sometimes useful in processing the data. Processed data comes from the Q-Spec software (Cox Analytical Systems, Sweden) provided with the machine. Its function is to process the spectral data files (*.spe) into peak-areas for each element of interest by fitting a model to the data. The model needs some user input and intervention to optimise it, and the quality of the model can be assessed using a number of diagnostic parameters, the most important being the RMSE. The Q-Spec software can also perform some calibration of the data, although this is a less typical use-case. Often the operator will include a file that contains all of the settings used by Q-Spec to translate the raw data files into the peak area outfile file — this file will have the extension *.dfl and will often simply be called settings.dfl. It comes in the form of a table, with a single row for each measurement step, and a column for parameters including individual element peak areas in counts (n) or intensities (n/mA). The data files commonly have names like result.txt or Results.txt, but may have been subsequently renamed. 1.3 Optical Images The scanner collects good quality optical images that have consistent lighting and because they are line-scan images, they do not suffer from optical distortions. Medium resolution images are usually included in all scan section folders (typically optical.tif), and optional high-resolution images are often included by operators elsewhere. High resolution images are typically supplied as both 8-bit and 16-bit images, and are usually hundreds of megabytes in size. Although the brightness of the lighting in the scanner is adjustable, images sometimes need to be brightened and/or have the contrast adjusted. This can be performed in desktop publishing software (e.g. Adobe Photoshop, Corel Photo-Draw), but it is easiest to use the open-source scientific image analysis software imageJ (NIH, USA). By including an appropriate colour reference card in the scan the image can be calibrated, although it is often desirable to increase the contrast to elucidate features of the core. The photograph will of the entire length of the bed scanned, rather than cropped to the limits of an individual scan section. If multiple scan sections are placed on the bed and are scanned together, it will include all of the scanned sections. The image needs to be cropped using the metadata for the relevant scan section; this process is covered in later chapters. 1.4 Radiographic Images The scanner has an x-radiographic line array capable of producing good-quality x-radiographs of the cores. The scan data can be split into two parts — “raw” and “processed” data. The raw data (usually radiograph.raw) is a tab-delimited text file containing a matrix of greyscale values. Each column represents a single step (measurement interval), and each row represents a single pixel on the line array. The pixel spacing is around 20 microns. The processed image (radiograph.tif) has a lower resolution than the raw data. This is because the pixels must be square, and so the pixels are down-sampled to fit with the step-size of the scan. Thus, if the step-size was 200 micrometers, each pixel will be 200 x 200 micrometers, whereas the raw data will have rectangular pixels with dimensions of 20 x 200 micrometers. Like the optical image, the radiograph often requires contrast and brightness adjustments, and these are easiest to perform in imageJ. With the inclusion of a suitable density standard, some relative or, where “u-channels” are used, absolute density calibration can be performed using these data. 1.5 Magnetic Susceptability Some scanners include a Bartington MS2E surface sensor. "],
["importing.html", "Chapter 2 Importing Data 2.1 Metadata 2.2 XRF Data 2.3 Optical Images 2.4 Radiographic Images 2.5 Magnetic Susceptability", " Chapter 2 Importing Data All of the Itrax data is in either text-format or “tagged image format” (*.tif). Although this means it is easily read by the various import functions available in R, it still needs considerable cleaning and wrangling to get it to a point where it is useable for most analyses. There are three possible approaches to this task: Use existing functions published in the itraxR package available from github.com/tombishop1. These are at an early stage and functionality might be broken, but are largely convenience functions for wrangling and analysing Itrax data. Work in base R to wrangle the data. This is perfectly achievable, and much of the current itraxR functionality was written this way. Work in the tidyverse family of packages and style. For data wrangling tasks, this approach can result in simpler and more resiliant code. In this guide examples will be given mostly using the tidyverse, and reference will be made to the convenience functions in itraxR. All of the Itrax data is in either text-format or “tagged image format” (*.tiff). Although this means it is easily read by the various import functions available in R, it still needs considerable cleaning and wrangling to get it to a point where it is useable. A number of convienience functions are provided in the package itraxR. 2.1 Metadata The scan data file document.txt can be parsed using itraxR::itrax_meta(). The output is a dataframe from which the individual components can be easily accessed through subsetting functions, for example as.numeric(itrax_meta()[6:7, 2]) ould return a numeric vector of the start and end position of a scan. #library(itraxR) #example(itrax_meta) 2.2 XRF Data 2.2.1 Processed Data This is the data most commonly used in analysis and it can be quickly imported using itraxR::itrax_import(). If you’d like more control over the import process, a typical Tidyverse workflow would consist of reading the file (they are tab-delimited), removing variables that are not of interest, and correctly labelling data that should be NA. You may also wish to assign a coring depth information at this stage. Firstly, we need to load the packages we will use, and define a list of elements and other names of variables we might expect to find in the files. library(tidyverse) library(janitor) elements &lt;- c( &quot;H&quot;, &quot;He&quot;, &quot;Li&quot;, &quot;Be&quot;, &quot;B&quot; , &quot;C&quot; , &quot;N&quot;, &quot;O&quot;, &quot;F&quot; , &quot;Ne&quot;, &quot;Na&quot;, &quot;Mg&quot;, &quot;Al&quot;, &quot;Si&quot;, &quot;P&quot;, &quot;S&quot;, &quot;Cl&quot;, # &quot;Ar&quot;, &quot;K&quot;, &quot;Ca&quot;, &quot;Sc&quot;, &quot;Ti&quot;, &quot;V&quot;, &quot;Cr&quot;, &quot;Mn&quot;, &quot;Fe&quot;, &quot;Co&quot;, &quot;Ni&quot;, &quot;Cu&quot;, &quot;Zn&quot;, &quot;Ga&quot;, &quot;Ge&quot;, &quot;As&quot;, &quot;Se&quot;, &quot;Br&quot;, &quot;Kr&quot;, &quot;Rb&quot;, &quot;Sr&quot;, &quot;Y&quot;, &quot;Zr&quot;, &quot;Nb&quot;, &quot;Mo&quot;, &quot;Tc&quot;, &quot;Ru&quot;, &quot;Rh&quot;, &quot;Pd&quot;, &quot;Ag&quot;, &quot;Cd&quot;, &quot;In&quot;, &quot;Sn&quot;, &quot;Sb&quot;, &quot;Te&quot;, &quot;I&quot;, &quot;Xe&quot;, &quot;Cs&quot;, &quot;Ba&quot;, &quot;La&quot;, &quot;Ce&quot;, &quot;Pr&quot;, &quot;Nd&quot;, &quot;Pm&quot;, &quot;Sm&quot;, &quot;Eu&quot;, &quot;Gd&quot;, &quot;Tb&quot;, &quot;Dy&quot;, &quot;Ho&quot;, &quot;Er&quot;, &quot;Tm&quot;, &quot;Yb&quot;, &quot;Lu&quot;, &quot;Hf&quot;, &quot;Ta&quot;, &quot;W&quot;, &quot;Re&quot;, &quot;Os&quot;, &quot;Ir&quot;, &quot;Pt&quot;, &quot;Au&quot;, &quot;Hg&quot;, &quot;Tl&quot;, &quot;Pb&quot;, &quot;Bi&quot;, &quot;Po&quot;, &quot;At&quot;, &quot;Rn&quot;, &quot;Fr&quot;, &quot;Ra&quot;, &quot;Ac&quot;, &quot;Th&quot;, &quot;Pa&quot;, &quot;U&quot;, &quot;Np&quot;, &quot;Pu&quot;, &quot;Am&quot;, &quot;Cm&quot;, &quot;Bk&quot;, &quot;Cf&quot;, &quot;Es&quot;, &quot;Fm&quot;, &quot;Md&quot;, &quot;No&quot;, &quot;Lr&quot;, &quot;Unq&quot;, &quot;Unp&quot;, &quot;Unh&quot;, &quot;Uns&quot;, &quot;Uno&quot;, &quot;Une&quot;, &quot;Unn&quot; ) others &lt;- c( &quot;position (mm)&quot;, &quot;sample.surface&quot;, &quot;MSE&quot;, &quot;cps&quot;, &quot;validity&quot;, &quot;Mo inc&quot;, &quot;Mo coh&quot;, &quot;Cr inc&quot;, &quot;Cr coh&quot; ) Secondly, we import the file, remove empty rows and columns, and change the data type of the validity flag. df &lt;- read_tsv(&quot;results.txt&quot;, skip = 2) %&gt;% remove_empty() %&gt;% mutate(validity = as.logical(validity)) Next we can select only the variables we need. The code below leaves anything that is a chemical element or in our list of other useful parameters, but you can change these depending on your needs. df &lt;- df %&gt;% select(any_of(c(elements, others))) If you have coring depth information, you could add it now. A simple calculation can be made using the position variable (demonstrated below, here assuming the top of the core is at a depth of 400 mm), or a more advanced calculation as you require. It is also possible to add ages derived from an age/depth model now. The final line re-orders the variables so the depth variable is the first. df &lt;- df %&gt;% rename(position = `position (mm)`) %&gt;% mutate(depth = position - min(position) + 400) %&gt;% select(depth, everything()) Finally, a quirk of the formatting of data where validity == FALSE needs to be addressed. These cells have a value of 0, but they could be confused with truely 0 value cells that have validity == TRUE. The solution is to convert them to NA as follows. Note that for metadata variables (depth, sample.surface etc. the measurements are still valid, so should not be set to NA). df &lt;- full_join(df %&gt;% filter(validity == TRUE), df %&gt;% filter(validity == FALSE) %&gt;% select(any_of(c(elements, &quot;position&quot;))) %&gt;% na_if(0)) %&gt;% arrange(position) %&gt;% mutate(depth = df$depth, validity = df$validity, cps = df$cps, MSE = df$MSE) Finally, you may wish to do some quality control. The deletion criteria will be more fully discussed in the next chapter, but it is often the case that the first and last few measurements are of poor quality. The code below removes the first and last 10 mm of measurements, however depending on your workflow you may wish to simply flag it in some way. df &lt;- df %&gt;% filter(depth &gt; min(depth) + 10 &amp; depth &lt; max(depth) - 10) 2.2.2 Joining XRF Data Ofen a core (sometimes referred to as a drive) is comprised of a sequence of individual sections, which may or may not be overlapping. Often we will want to integrate them into a continuous dataset for analytical purposes. When joining cores that do not overlap, this process is trivial — the data is simply appended in order of depth, and a new column is added with the identity of the original core section. Where overlapping cores are present, there can be multiple measurements at a single depth (on different cores). In these cases not only will the individual measurements need to be re-ordered by depth, but an additional variable should be created that can be used in combination or alone tp uniquely identify each measurement. The code below does this by creating an additional variable called label, with the name of the original core given in the named list. mylist &lt;- list(core1 = core1, core2 = core2) df &lt;- lapply(names(mylist), function(i) within(mylist[[i]], {label &lt;- i})) %&gt;% bind_rows() %&gt;% arrange(depth) This process can be simplified using itraxR::itrax_join(). 2.2.3 Raw Data Sometimes it is useful to work with raw data rather than the calculated intensity data from the Q-Spec software. In this case, the raw data can be read directly from the individual files in the relevant directory. For individual measurements this is fairly trivial, although it must be considered that the data output is not calibrated to an energy and the data are in counts, not intensities. If the entire scan is read, some mechanism to iterate through the individual data files, adding them to a structured data object with relevant metadata (positions, for example) is required. 2.3 Optical Images In order to make the images useable for plotting beside other data, they need to be cropped to the scanned area and referenced to the position or a depth variable. Images in R are read in as a matrix with 3 dimensions (length, width, and the three colours). To begin with the start and end positions of the scan need to be established; this usually involves parsing the metadata file (e.g. itraxR::itrax_meta()[]), although in some circumstances they can be obtained from another data source (e.g. range(myData[,\"Position\"])). These then need to be mapped onto individual pixel values using the known size of a single pixel. Once this has been calculated, it is trvial to then subset the part of the image required. library(itraxR) library(tiff) image &lt;- readTIFF(&quot;optical.tif&quot;) meta &lt;- itrax_meta(&quot;document.txt&quot;) image_depth &lt;- rev(seq(from = as.numeric(meta[9, 2]), to = as.numeric(meta[10, 2]), length.out = dim(image)[2])) image &lt;- image[ , which(image_depth &gt; as.numeric(meta[6, 2]) &amp; image_depth &lt; as.numeric(meta[7, 2])) , ] 2.4 Radiographic Images The workflow for manipulating the processed radiographic images (*.tif) is very similar to that for the optical images, the main difference being the matrix only has two dimensions (length and width) as the image is greyscale. However, if there is a desire to manipulate the raw data from the radiographic image, some further work is required because the “pixel” is not square, but rectangular. That is to say the length of the pixel differs from its width. On the core scanner a single pixel has a width across the core of 20 micrometers, but has a variable coverage along the core (usually between 50 and 200 micrometers). The processed image downscales the pixel width to match the pixel length in order to force square pixels, losing some resolution along the way. In addition, it should be noted that unlike the optical images that always begin from position == 0, the radiographic images have defined start and end points just like an XRF scan, the parameters of which can be accessed from the document.txt metadata file (or by using itrax_meta()). 2.5 Magnetic Susceptability "],
["tidying-data.html", "Chapter 3 Tidying Data", " Chapter 3 Tidying Data In the functionality provided by itraxR, the need for data cleaning is much reduced. However, you may still encounter poor quality data that needs removing from subsequent analysis. This can be broadly defined as: Data at the start and end of the the core, where a volume of core material is “missing”. Measurements where the optical configuration is out of position (marked as validity == 0), often due to holes or stones in the core. Areas of the core with low total counts. Individual measurements that are statistical outliers. The easiest way to do this is using a tidyverse style sequence of pipes that set the observations of faulty data as NA. "],
["plotting.html", "Chapter 4 Plotting 4.1 Plotting XRF Data 4.2 Plotting Images", " Chapter 4 Plotting There are a number of plotting options included in base R and some paleoenvironmental packages like analouge, but here we will use ggplot2 and compatible packages. 4.1 Plotting XRF Data For simple biplots, ggplot2 provides the following solution. ggplot(data = na.omit(df), mapping = aes(x = depth, y = Mo.coh/Mo.inc)) + geom_line(aes(color = label)) + coord_flip() + scale_x_reverse() + labs(x = &quot;Depth [mm]&quot;,color = &quot;Core&quot;) + theme_classic() + theme(legend.position = &quot;none&quot;) Where a traditional stratigraphic plot is desired, the tidypaleo package provides useful functionality for producting these. However, the data does need to converted to long-form from the existing table. In this way tidypaleo::facet_geochem_gridh works in a way similar to ggplot::facet_wrap but has been creates a stratigraphic plot. library(tidypaleo) df %&gt;% mutate(`coh/inc` = Mo.coh/Mo.inc) %&gt;% select(Fe, Ti, `coh/inc`, Mn, depth, label) %&gt;% pivot_longer(!c(&quot;depth&quot;, &quot;label&quot;), names_to = &quot;elements&quot;, values_to = &quot;peakarea&quot;) %&gt;% drop_na() %&gt;% mutate(elements = factor(elements, levels = c(&quot;Fe&quot;, &quot;Mn&quot;, &quot;Ti&quot;, &quot;coh/inc&quot;))) %&gt;% ggplot(aes(x = peakarea, y = depth)) + geom_lineh(aes(color = label)) + scale_y_reverse() + facet_geochem_gridh(vars(elements)) + labs(x = &quot;peak area&quot;, y = &quot;Depth [mm]&quot;) + theme_paleo() + theme(legend.position = &quot;none&quot;) 4.2 Plotting Images Once an image file has been imported, the image can be integrated into ggplot code using the workflow below. library(tidyverse) library(grid) ggplot() + annotation_custom(rasterGrob(image2, width = unit(1, &quot;npc&quot;), height = unit(1, &quot;npc&quot;))) + xlab(&quot;&quot;) + ylab(&quot;Position [mm]&quot;) + ylim(as.numeric(meta[6:7, 2])) If the image needs to be rotated, the aperm() function needs to be used, followed by reversing the image. image2 &lt;- aperm(image, c(2,1,3)) image2 &lt;- image2[c(dim(image2)[1]: 1), , ] "],
["transform-data.html", "Chapter 5 Transform Data 5.1 Ratios and Normalisation 5.2 Preparing Data for Multivariate Data Analysis", " Chapter 5 Transform Data The XRF data typically reported from the Itrax core scanner come from the spectral processing software Q-Spec. The output is usually in the form of an intensity, which is a dimensionless metric derived from the size of the spectral peak for a particular element, above the background Bremsstralung radiation, sometimes normalised for the tube current and/or counting time. 5.1 Ratios and Normalisation These data are compositional, and represent the changes in the relative proportions of all components of the matrix, measured and un-measured. As such it is likely the data will need transforming for certain types of multivariate analysis. As previously mentioned, these data are dimensionless, and as such do not represent a quantity, but are directly related to the absolute amount of a particular element in the matrix. It is often the case that ratios of elements are used to represent changes in composition — this is sometimes referred to as normalisation, or normalising one element against another. It is trivial to calculate element ratios, to the extent that these can often simply be calculated where they are required rather than saving them to memory. For example, if a plot of the Compton divided by the Rayleigh scatter was desired, there is no need to save the computed value to a new variable (e.g. coh_inc &lt;- df$Mo.coh/df$Mo.inc ) — simply define the calculation during plotting. 5.2 Preparing Data for Multivariate Data Analysis Where multivariate methods (cluster analysis, principle components analysis, correlation matricies) are required, it is usually necessary to transform data. This is because the statistical assumptions that underlie these methods are often not met when dealing with compositional data like that from an XRF core-scanner. There is no “right” or ideal way to deal with these issues, but a common method is to use a form of log transformation. Here we use a centered log transformation, which cannot be performed on zero values. df %&gt;% chemometrics::clr(df) There are a number of possible solutions to this problem of zero values, none ideal, but the most common are to add an arbitary number to the entirity of the data, or to replace zero values with a very small number, perhaps the limit of precision or the limit of detection. In the example shown, the limit of precision for the peak area intensity is used (0.001). input[input == 0] &lt;- 0.001 A final solution may be to exclude zero values from any subsequent analysis, although not all methods tolerate NA in the data. Where there are many zero values for a particular variable, the variable may not provide good data and could be excluded. df %&gt;% na_if(0) In most multivariate analyses there are a number of variables which have high signal-to-noise ratios or otheriwse only add noise to multivariate methods. In these cases, they might be excluded from multivariate methods. The selection of variables for inclusion is a matter for the analyst. It goes without saying that variables that are not part of the composition (that is, anything that is not an element) must be removed from the data used for multivariate analysis. df %&gt;% select(-any_of(c(&quot;Mg&quot;, &quot;Co&quot;, &quot;Mo&quot;)) Finally, in order to correctly identify the measurements to their original data source, it is necessary to use row names that uniquely identify observations. For single scans this is not usually an issue — the depth or position variable can be used. However, where a dataset is a composition of multiple cores, you may find that there are multiple observations for a particular depth where cores overlap. In the example shown we create a unique name for each observation by concatating the name of the core as recorded in the label column of a core sequence joined using itraxR::itrax_join() with the corresponding depth variable, but the code could be modified to suit a different workflow. rowlabels &lt;- str_c(df$label, df$depth, sep = &quot;_&quot;) input &lt;- df %&gt;% select(any_of(elements)) input &lt;- input %&gt;% select(-any_of(c(&quot;Mg&quot;, &quot;Co&quot;, &quot;Mo&quot;))) input[input == 0] &lt;- 0.001 library(chemometrics) input &lt;- clr(input) row.names(input) &lt;- rowlabels "],
["multivariate-methods.html", "Chapter 6 Multivariate Methods 6.1 Correlation Coefficients 6.2 Unconstrained Cluster Analysis 6.3 Constrained Cluster Analaysis 6.4 Principle Components Analysis", " Chapter 6 Multivariate Methods Input data often needs extensive preparation if useful results are to be obtained — transformation, dealing with zero values, and NA values. It is worth mentioning some general issues around the robustness of multivariate analysis. 6.1 Correlation Coefficients The Pearson’s R correlation coefficient is occasionally used to study the relationships between individual elements. 6.2 Unconstrained Cluster Analysis 6.3 Constrained Cluster Analaysis 6.4 Principle Components Analysis "],
["calibrating-data.html", "Chapter 7 Calibrating Data 7.1 Suitable Methods 7.2 Linear Methods 7.3 Log-Ratio Methods", " Chapter 7 Calibrating Data 7.1 Suitable Methods 7.2 Linear Methods 7.3 Log-Ratio Methods "],
["transforming-data.html", "Chapter 8 Transforming Data 8.1 Ratios and Normalisation 8.2 Preparing Data for Multivariate Data Analysis", " Chapter 8 Transforming Data 8.1 Ratios and Normalisation 8.2 Preparing Data for Multivariate Data Analysis "],
["references.html", "References", " References "]
]
