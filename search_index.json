[["index.html", "Using Itrax Data in R Preface", " Using Itrax Data in R Thomas Bishop 2025-01-25 Preface Itrax core scanners (manufactured by Cox Analytical Systems, Sweden) are used in palaeoceanography, palaeolimnological, geological and other “down-core” studies of sedimentary core material. The multi-sensory data can include radiography, optical images, magnetic susceptibility, but most importantly energy-dispersive x-ray florescence (ED-XRF) measurements of elemental abundance. The data can be harder to work with compared to some other palaeoenvironmental techniques because: Very large quantities of data are produced. ED-XRF measurements can be made every 100-200 μm, so for long core sections, these datasets are large. Simple line-graphs can become problematic, and multivariate analysis can become unworkable on some software. Images can be very large and need correct alignment. Combining them with line-graphs or other images can be troublesome. ED-XRF elemental data is compositional, but dimensionless (they do not have units attached e.g. [ppm]). This can make the use of traditional statistical tools and tests problematic. This guide comes from a series of seminars offered to users of the Itrax core scanner at The University of Manchester Geography Laboratories in 2020. "],["the-itraxr-package.html", "0.1 The itraxR Package", " 0.1 The itraxR Package The itraxR package offers a range of convenience functions for working with Itrax data. The book mostly uses the functions in this package, but the source code is available and fully commented, so is easy to as a basis for other work. The package and code are available from github.com/tombishop1/itraxR. The package is available via CRAN, so can be installed using install.packages(\"itraxR\"), but the CRAN repository is updated less frequently. This guide always uses the latest version available on the Github repo. packageVersion(&quot;itraxR&quot;) ## [1] &#39;1.12.2&#39; "],["prerequisites.html", "0.2 Prerequisites", " 0.2 Prerequisites This guide assumes a basic knowledge of R and the tidyverse, including data types, assignments, and pipes. It also assumes a background knowledge of the core scanner and the nature of the data it produces; see Croudace et al. (2019) and references therein. Some of the sections on data analysis assume some knowledge of compositional data analysis. References Croudace, Ian W., Ludvig Löwemark, Rik Tjallingii, and Bernd Zolitschka. 2019. “High resolution XRF core scanners: A key tool for the environmental and palaeoclimate sciences.” Quaternary International 514 (May): 1–4. https://doi.org/10.1016/j.quaint.2019.05.038. "],["example-data.html", "0.3 Example Data", " 0.3 Example Data All of these examples given in this book are from Piston core CD166/19, which was recovered from the Agadir Basin during the RRS Charles Darwin expedition CD166 in 2004 (Wynn and Cronin (2005)). The core site is located at 31°31’ N, 17°11.77’ W at a water depth of 4502 m. The coring operation recovered 4.3 m of sediment composed of hemipelagite and turbidites. The hemipelagic sediments range from cream-brown carbonate-rich marl/ooze to red-brown clays. The turbidites include siliciclastic, volcaniclastic and calcareous sediments. The split core sections were analysed using the Cox Analytical Systems Itrax core scanner at the British Ocean Sediment Core Research Facility, National Oceanography Centre (Southampton, UK). The split core surfaces were cleaned and loaded to the core scanner along with a radiographic reference sample (Francus, Kanamaru, and Fortin (2015)) and a colour card. The cores were first scanned to measure the sample surface height and to obtain optical images. The core sections were then covered with Mylar film to reduce sample desiccation during XRF scanning and x-radiography. XRF data were acquired using a Molybdenum X-ray tube set at 30 kV and 30 mA with a dwell time of 30 seconds and a step size of 1 mm. X-radiographs were acquired with voltage and current set to 55 kV and 50 mA, respectively, and dwell time set to 500 ms. Step size for the x-radiography was set to 200 µm. Two replicate XRF scans were also performed for each of the three core sections that make up this core. The XRF settings for the replicates were kept consistent with those of the main XRF scan. The interval selected for the replicate scans ranged from 10 cm to 15 cm of the the total sample length. All raw XRF raw data were reprocessed using the QSpec spectral analysis software to optimise peak fitting. The data can be downloaded from the Github pages that form the source for this document, available at github.com/tombishop1/itraxBook. They are also available in the same form from the PANGAEA database (Bishop and Charidemou (2023)). References Bishop, T., and M. Charidemou. 2023. “Core Scanning Data from Core CD166/19.” PANGAEA. https://doi.org/10.1594/PANGAEA.955347. Francus, P., K. Kanamaru, and D. Fortin. 2015. “Standardization and Calibration of X-Radiographs Acquired with the ITRAX Core Scanner.” In Micro-XRF Studies of Sediment Cores, Developments in Paleoenvironmental Research 17, edited by I. W. Croudace and R. G. Rothwell, 491–505. Dordrecht: Springer. Wynn, R. B., and B. T. Cronin. 2005. “RRS \"Charles Darwin\" Cruise CD166, 29 Oct - 22 Nov 2004. Sedimentary processes and deposits in the Agadir Basin and Gulf of Cadiz.” 59. Vol. 44. "],["utility-data.html", "0.4 Utility Data", " 0.4 Utility Data It is often useful to have a list of symbols used to represent elements for various subsetting functions. This can be extracted from the example data included with the package periodicTable as follows. The vector called elementslist will be referred to elsewhere in this book. data(periodicTable) elementsList &lt;- periodicTable$symb rm(periodicTable) "],["citing-this-work.html", "0.5 Citing This Work", " 0.5 Citing This Work R and itraxR can be cited as follows: citation() ## To cite R in publications use: ## ## R Core Team (2024). _R: A Language and Environment for Statistical ## Computing_. R Foundation for Statistical Computing, Vienna, Austria. ## &lt;https://www.R-project.org/&gt;. ## ## A BibTeX entry for LaTeX users is ## ## @Manual{, ## title = {R: A Language and Environment for Statistical Computing}, ## author = {{R Core Team}}, ## organization = {R Foundation for Statistical Computing}, ## address = {Vienna, Austria}, ## year = {2024}, ## url = {https://www.R-project.org/}, ## } ## ## We have invested a lot of time and effort in creating R, please cite it ## when using it for data analysis. See also &#39;citation(&quot;pkgname&quot;)&#39; for ## citing R packages. citation(&quot;itraxR&quot;) ## To cite package &#39;itraxR&#39; in publications use: ## ## Bishop T (2024). _itraxR: Itrax Data Analysis Tools_. R package ## version 1.12.2, commit e958e0633ce921107ecf4581d561441246bb2cfd, ## &lt;https://github.com/tombishop1/itraxR&gt;. ## ## A BibTeX entry for LaTeX users is ## ## @Manual{, ## title = {itraxR: Itrax Data Analysis Tools}, ## author = {Thomas Bishop}, ## year = {2024}, ## note = {R package version 1.12.2, commit e958e0633ce921107ecf4581d561441246bb2cfd}, ## url = {https://github.com/tombishop1/itraxR}, ## } ## ## ATTENTION: This citation information has been auto-generated from the ## package DESCRIPTION file and may need manual editing, see ## &#39;help(&quot;citation&quot;)&#39;. You should also cite other packages you use, for example dplyr and tidypaleo. This book can be cited as per the citation information in the Github repository in which it is located: github.com/tombishop1/itraxBook. "],["intro.html", "Chapter 1 Data Structure", " Chapter 1 Data Structure The Itrax core scanner is a multi-sensor device, with separate data outputs for different measurements and uses. They are described in the following sections. Note that your data may not contain all of these objects, depending on the exact scanner, configuration, or data repository you use. The folder structure may vary between operators, but typically there will be a folder for each scan section, and each will contain the data described in the following sections. Note that where radiographs and XRF data have been acquired using different step-sizes (measurement intervals), the operator will create separate scan sections (folders) for the x-radiograph and XRF measurement. This is because a scan section can only have a single fixed step-size. For example, it is not uncommon for users to require an step-size of 200 μm for the x-radiograph, but only 1 mm for the XRF measurement. "],["metadata.html", "1.1 Metadata", " 1.1 Metadata Every scan folder has a document.txt file that contains information about the parameters of the scan. For example, it contains the current and voltage settings used for the x-ray tube, the step size, and the start and stop positions of the scan. It is a text file, although the formatting can be inconsistently rendered depending on the text editor used. Sometimes this information is required to process other parts of the data, and as such it is an important part of the overall data package. "],["xrf-data.html", "1.2 XRF Data", " 1.2 XRF Data The XRF data can be split into two groups — “raw” and “processed” data. The raw data is contained in a separate folder called XRF data, and consists of a single file, beginning with L000000.spe and incrementing sequentially. This file can be read using a text editor and is tab-delimited. The first part is a header, containing metadata information. The second part is a table of all the channels of the detector and the corresponding count for each channel. Increasing channel numbers represent increasing energy, but some thought needs to be given to calibrating channels into an energy — this step is usually performed using specialist software. In addition, a file called sumspectra.spe is often included in the root directory; this is simply the sum of all the *.spe files in the XRF data directory, and is sometimes useful in processing the data. Processed data comes from the Q-Spec software (Cox Analytical Systems, Sweden) provided with the machine. Its function is to process the spectral data files (*.spe) into peak-areas for each element of interest by fitting a model to the data. The model needs some user input and intervention to optimise it, and the quality of the model can be assessed using a number of diagnostic parameters, the most important being the root-mean-squared-error (RMSE). The Q-Spec software can also perform some quantitative calibration of the data, although this is a less typical use-case. Often the operator will include a file that contains all of the settings used by Q-Spec to translate the raw data files into the peak area output file — this file will have the extension *.dfl and will often simply be called settings.dfl. The processed elemental data comes in the form of a text file comprising of a tab-delimited table, with a single row for each measurement step, and a column for parameters including individual element peak areas in counts (n) or intensities (n/mA). The data files commonly have names like result.txt or Results.txt, but may have been subsequently renamed. These are the ED-XRF data most commonly worked with by analysts. "],["optical-images.html", "1.3 Optical Images", " 1.3 Optical Images The scanner collects good quality optical images that have consistent lighting and because they are line-scan images, they do not suffer from optical distortions. Medium resolution images are usually included in all scan section folders (typically optical.tif), and optional high-resolution images are often included by operators elsewhere. High resolution images are typically supplied as both 8-bit and 16-bit images, and are usually hundreds of megabytes in size. Although the brightness of the lighting in the scanner is adjustable, images sometimes need to be brightened and/or have the contrast adjusted. This can be performed in desktop publishing software (e.g. Adobe Photoshop, Corel Photo-Draw), but it is easiest to use the open-source scientific image analysis software imageJ (NIH, USA). By including an appropriate colour reference card in the scan the image can be calibrated, although it is often desirable to increase the contrast and gamma to elucidate features of the core. The photograph is always of the entire length of the bed scanned, rather than cropped to the limits of an individual scan section. If multiple scan sections are placed on the bed and are scanned together, it will include all of the scanned sections. The image needs to be cropped using the metadata for the relevant scan section; this process is covered in later chapters. In this case, the colour reference information for the card in the image is given in the file greywhitebalancecolourcard.csv. The plugin for imageJ, ijp-color, can be used to perform colour calibration on the images. In this case a linear - no intercept model has been used. to generate the image files image0.tif. The image below shows the uncorrected image (left hand side) and the colour corrected image (right hand side). "],["radiographic-images.html", "1.4 Radiographic Images", " 1.4 Radiographic Images The scanner has an x-radiographic line array capable of producing good-quality x-radiographs of the cores. The scan data can be split into two parts — “raw” and “processed” data. The raw data (usually radiograph.raw) is a tab-delimited text file containing a matrix of greyscale values. Each column represents a single step (measurement interval, often set to between 50 and 200 μm), and each row represents a single pixel on the line array. The pixel spacing is around 20 μm. Note that the raw radiographic data contains pixels at the extremes that are outside of the coverage of the x-ray beam — these are obviously useless and need to be cropped. The processed image (radiograph.tif) has a lower resolution than the raw data. This is because the pixels must be square, and so the pixels are down-sampled to fit with the step-size of the scan. Thus, if the step-size was 200 μm, each pixel will be 200 x 200 μm, whereas the raw data will have rectangular pixels with dimensions of 20 x 200 μm. The data at the extremes of the radiograph are always cropped, so the radiograph has a coverage of around 13 mm of the width of the core. Like the optical image, the radiograph often requires contrast and brightness adjustments, and these are easiest to perform in imageJ. With the inclusion of a suitable density standard, some relative or, where “u-channels” are used, absolute density calibration can be performed using these data; see Francus, Kanamaru, and Fortin (2015) for more. References Francus, P., K. Kanamaru, and D. Fortin. 2015. “Standardization and Calibration of X-Radiographs Acquired with the ITRAX Core Scanner.” In Micro-XRF Studies of Sediment Cores, Developments in Paleoenvironmental Research 17, edited by I. W. Croudace and R. G. Rothwell, 491–505. Dordrecht: Springer. "],["magnetic-susceptability.html", "1.5 Magnetic Susceptability", " 1.5 Magnetic Susceptability Some scanners include a Bartington MS2E surface sensor, but the example dataset supplied here does not have magnetic susceptability data collected from an Itrax core scanner. "],["importing.html", "Chapter 2 Importing Data", " Chapter 2 Importing Data All of the Itrax data is in either text-format or “tagged image format” (*.tif). Although this means it is easily read by the various import functions available in R, it still needs considerable cleaning and wrangling to get it to a point where it is usable for most analyses. There are three possible approaches to this task: Use existing functions published in the itraxR package available from github.com/tombishop1/itraxR. These are at an early stage and functionality might be broken, but are largely convenience functions for wrangling and analysing Itrax data. It is easy to install the package directly from Github using remotes::install_github(\"tombishop1/itraxR\"). Work in base R to wrangle the data. This is perfectly achievable, and much of the current itraxR functionality was originally written this way. Work in the tidyverse family of packages and style. For data wrangling tasks, this approach can result in simpler and more resilient code. In this guide examples will be given using the functions provided in itraxR, but the processes used in those functions will be explained, and the code used in those functions is fully commented so can be modified to suit particular needs. "],["metadata-1.html", "2.1 Metadata", " 2.1 Metadata The scan metadata file document.txt can be quickly parsed using itraxR::itrax_meta(). The output is a dataframe from which the individual components can be easily accessed through subsetting functions; for example as.numeric(itrax_meta()[6:7, 2]) would return a numeric vector of the start and end position of a scan. itrax_meta(&quot;CD166_19_S1/CD166_19_S1/document.txt&quot;) ## Parameter Value Unit ## 1 Sample name CD166_19_S1 str ## 2 Section name CD166_19_S1 str ## 3 Aquisition date 22/9/2020 dd/mm/yyyy ## 4 Operator name MC str ## 5 Tube Mo element ## 6 Start coordinate 31.5 mm ## 7 Stop coordinate 1314.1 mm ## 8 Step size 1000 microns ## 9 Optical Start 0.5 mm ## 10 Optical End 1401.3 mm ## 11 Optical step size 0.188 mm ## 12 Rad. voltage 55 kV ## 13 Rad. current 50 mA ## 14 Rad. exposure 0 ms ## 15 line camera signal level 323154 at 25 ms ## 16 XRF ON ON/OFF ## 17 XRF voltage 30 kV ## 18 XRF current 30 mA ## 19 XRF exp. time 15 seconds ## 20 Start temperature \\xb0C ## 21 Stop temperature \\xb0C ## 22 Start humidity % ## 23 Stop humidity % ## 24 Start vacuum -95.0 kPa ## 25 Stop vacuum -94.8 kPa "],["xrf-data-1.html", "2.2 XRF Data", " 2.2 XRF Data 2.2.1 Processed Data This is the data most commonly used in analysis and it can be quickly imported using itraxR::itrax_import(). Note that, like for the example data, it is possible to have more than one processed data file. Typically cores have at least two, one created at the time of the scan based on settings for a single point near the top of the sequence, and another from a holistic re-analysis of the sequence. itrax_import(&quot;CD166_19_S1/CD166_19_S1/Results.txt&quot;, depth = 0) %&gt;% glimpse() ## Rows: 1,283 ## Columns: 43 ## $ depth &lt;dbl&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,… ## $ MSE &lt;dbl&gt; 1.26, 1.41, 1.57, 1.55, 1.41, 1.41, 1.36, 1.52, 1.38, 1.58, 1… ## $ cps &lt;dbl&gt; 22415, 34525, 38370, 39796, 40022, 41973, 41268, 40977, 41104… ## $ validity &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, T… ## $ Al &lt;dbl&gt; 41, 76, 57, 74, 26, 53, 27, 72, 78, 69, 70, 61, 45, 41, 51, 9… ## $ Si &lt;dbl&gt; 177, 275, 306, 330, 206, 233, 347, 337, 346, 403, 381, 301, 4… ## $ P &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ S &lt;dbl&gt; 0, 10, 0, 32, 21, 37, 28, 59, 19, 44, 30, 0, 54, 48, 0, 28, 3… ## $ Cl &lt;dbl&gt; 726, 1318, 1513, 1470, 1312, 1740, 1576, 1605, 1559, 1503, 14… ## $ Ar &lt;dbl&gt; 697, 604, 595, 506, 555, 591, 504, 584, 489, 597, 512, 547, 5… ## $ K &lt;dbl&gt; 1412, 2565, 2628, 2378, 2265, 2947, 3179, 3376, 3193, 3220, 2… ## $ Ca &lt;dbl&gt; 59965, 112734, 144287, 162938, 153194, 135879, 132183, 134094… ## $ Sc &lt;dbl&gt; 19, 0, 14, 0, 0, 106, 14, 0, 46, 0, 0, 33, 0, 0, 0, 28, 22, 0… ## $ Ti &lt;dbl&gt; 804, 1661, 1806, 2121, 2031, 1826, 1923, 2059, 2443, 2701, 24… ## $ V &lt;dbl&gt; 26, 51, 88, 22, 75, 89, 107, 0, 71, 23, 91, 55, 66, 64, 62, 1… ## $ Cr &lt;dbl&gt; 220, 258, 326, 301, 306, 440, 401, 440, 407, 449, 336, 329, 4… ## $ Mn &lt;dbl&gt; 231, 508, 559, 483, 485, 738, 792, 799, 868, 794, 647, 680, 8… ## $ Fe &lt;dbl&gt; 19786, 35168, 36494, 35952, 35512, 44303, 45283, 45694, 46506… ## $ Ni &lt;dbl&gt; 54, 123, 104, 134, 144, 166, 151, 125, 157, 180, 147, 132, 16… ## $ Cu &lt;dbl&gt; 117, 277, 230, 150, 206, 276, 250, 217, 178, 191, 197, 189, 1… ## $ Zn &lt;dbl&gt; 184, 205, 268, 123, 239, 240, 294, 284, 238, 230, 211, 210, 2… ## $ Ga &lt;dbl&gt; 0, 4, 48, 0, 0, 0, 40, 8, 22, 26, 0, 47, 57, 0, 125, 0, 0, 0,… ## $ Ge &lt;dbl&gt; 0, 0, 40, 43, 0, 0, 48, 115, 100, 80, 164, 110, 132, 98, 90, … ## $ Br &lt;dbl&gt; 331, 491, 605, 550, 609, 659, 709, 583, 582, 635, 545, 678, 5… ## $ Rb &lt;dbl&gt; 175, 329, 76, 295, 313, 304, 318, 337, 323, 223, 306, 168, 25… ## $ Sr &lt;dbl&gt; 5817, 8306, 9181, 9644, 9940, 10287, 9917, 9931, 9897, 9209, … ## $ Y &lt;dbl&gt; 60, 15, 140, 119, 107, 180, 106, 52, 77, 112, 144, 237, 151, … ## $ Zr &lt;dbl&gt; 305, 255, 322, 280, 475, 160, 312, 319, 319, 552, 484, 373, 6… ## $ Pd &lt;dbl&gt; 0, 31, 78, 75, 66, 31, 72, 31, 54, 58, 70, 55, 81, 43, 6, 42,… ## $ Cd &lt;dbl&gt; 9, 13, 34, 55, 24, 34, 28, 56, 62, 96, 79, 41, 84, 64, 43, 75… ## $ I &lt;dbl&gt; 30, 38, 48, 80, 67, 23, 58, 30, 53, 93, 43, 36, 126, 58, 85, … ## $ Cs &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ Ba &lt;dbl&gt; 35, 63, 48, 25, 60, 132, 93, 172, 108, 108, 22, 74, 123, 82, … ## $ Nd &lt;dbl&gt; 12, 17, 67, 44, 75, 42, 97, 92, 68, 56, 59, 41, 50, 73, 56, 7… ## $ Sm &lt;dbl&gt; 0, 43, 46, 39, 64, 36, 12, 24, 69, 68, 17, 47, 59, 37, 45, 28… ## $ Yb &lt;dbl&gt; 166, 144, 282, 158, 218, 163, 184, 248, 166, 188, 169, 247, 2… ## $ Ta &lt;dbl&gt; 546, 789, 776, 703, 794, 919, 852, 926, 840, 847, 790, 736, 8… ## $ W &lt;dbl&gt; 1151, 1992, 2019, 2016, 2160, 2344, 2336, 2243, 2267, 2185, 2… ## $ Pb &lt;dbl&gt; 22, 77, 0, 13, 152, 54, 54, 75, 111, 78, 0, 92, 22, 0, 81, 19… ## $ Bi &lt;dbl&gt; 0, 72, 136, 199, 134, 157, 182, 151, 185, 116, 164, 141, 135,… ## $ `Mo inc` &lt;dbl&gt; 20427, 26323, 26778, 26550, 28310, 31356, 30631, 29040, 29104… ## $ `Mo coh` &lt;dbl&gt; 6812, 8799, 9117, 9303, 9886, 10140, 9968, 9973, 9701, 9751, … ## $ position &lt;dbl&gt; 31.54, 32.54, 33.54, 34.54, 35.54, 36.54, 37.54, 38.54, 39.54… 2.2.2 Joining XRF Data Often a core (sometimes referred to as a drive) is comprised of a sequence of individual sections, which may or may not be overlapping. Often we will want to integrate them into a continuous dataset for analytical purposes. When joining cores that do not overlap, this process is trivial — the data might simply appended in order of depth, and a new column is added with the identity of the original core section. Where overlapping cores are present, there can be multiple measurements at a single depth (on different cores). In these cases not only will the individual measurements need to be re-ordered by depth, but an additional variable should be created that can be used in combination or alone to uniquely identify each measurement. The code below does this by creating an additional variable called label, with the name of the original core given in the named list. mylist &lt;- list(core1 = core1, core2 = core2) df &lt;- lapply(names(mylist), function(i) within(mylist[[i]], {label &lt;- i})) %&gt;% bind_rows() %&gt;% arrange(depth) This process can be simplified using itraxR::itrax_join(), for example: # import the core sections CD166_19_S1 &lt;- itrax_import(&quot;CD166_19_S1/CD166_19_S1/Results.txt&quot;, depth_top = 0) CD166_19_S2 &lt;- itrax_import(&quot;CD166_19_S2/CD166_19_S2/Results.txt&quot;, depth_top = max(CD166_19_S1$depth)) CD166_19_S3 &lt;- itrax_import(&quot;CD166_19_S3/CD166_19_S3/Results.txt&quot;, depth_top = max(CD166_19_S2$depth)) #join them together CD166_19 &lt;- itrax_join(list(S1 = CD166_19_S1, S2 = CD166_19_S2, S3 = CD166_19_S3)) rm(CD166_19_S1, CD166_19_S2, CD166_19_S3) glimpse(CD166_19) ## Rows: 4,212 ## Columns: 44 ## $ depth &lt;dbl&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,… ## $ MSE &lt;dbl&gt; 1.26, 1.41, 1.57, 1.55, 1.41, 1.41, 1.36, 1.52, 1.38, 1.58, 1… ## $ cps &lt;dbl&gt; 22415, 34525, 38370, 39796, 40022, 41973, 41268, 40977, 41104… ## $ validity &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, T… ## $ Al &lt;dbl&gt; 41, 76, 57, 74, 26, 53, 27, 72, 78, 69, 70, 61, 45, 41, 51, 9… ## $ Si &lt;dbl&gt; 177, 275, 306, 330, 206, 233, 347, 337, 346, 403, 381, 301, 4… ## $ P &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ S &lt;dbl&gt; 0, 10, 0, 32, 21, 37, 28, 59, 19, 44, 30, 0, 54, 48, 0, 28, 3… ## $ Cl &lt;dbl&gt; 726, 1318, 1513, 1470, 1312, 1740, 1576, 1605, 1559, 1503, 14… ## $ Ar &lt;dbl&gt; 697, 604, 595, 506, 555, 591, 504, 584, 489, 597, 512, 547, 5… ## $ K &lt;dbl&gt; 1412, 2565, 2628, 2378, 2265, 2947, 3179, 3376, 3193, 3220, 2… ## $ Ca &lt;dbl&gt; 59965, 112734, 144287, 162938, 153194, 135879, 132183, 134094… ## $ Sc &lt;dbl&gt; 19, 0, 14, 0, 0, 106, 14, 0, 46, 0, 0, 33, 0, 0, 0, 28, 22, 0… ## $ Ti &lt;dbl&gt; 804, 1661, 1806, 2121, 2031, 1826, 1923, 2059, 2443, 2701, 24… ## $ V &lt;dbl&gt; 26, 51, 88, 22, 75, 89, 107, 0, 71, 23, 91, 55, 66, 64, 62, 1… ## $ Cr &lt;dbl&gt; 220, 258, 326, 301, 306, 440, 401, 440, 407, 449, 336, 329, 4… ## $ Mn &lt;dbl&gt; 231, 508, 559, 483, 485, 738, 792, 799, 868, 794, 647, 680, 8… ## $ Fe &lt;dbl&gt; 19786, 35168, 36494, 35952, 35512, 44303, 45283, 45694, 46506… ## $ Ni &lt;dbl&gt; 54, 123, 104, 134, 144, 166, 151, 125, 157, 180, 147, 132, 16… ## $ Cu &lt;dbl&gt; 117, 277, 230, 150, 206, 276, 250, 217, 178, 191, 197, 189, 1… ## $ Zn &lt;dbl&gt; 184, 205, 268, 123, 239, 240, 294, 284, 238, 230, 211, 210, 2… ## $ Ga &lt;dbl&gt; 0, 4, 48, 0, 0, 0, 40, 8, 22, 26, 0, 47, 57, 0, 125, 0, 0, 0,… ## $ Ge &lt;dbl&gt; 0, 0, 40, 43, 0, 0, 48, 115, 100, 80, 164, 110, 132, 98, 90, … ## $ Br &lt;dbl&gt; 331, 491, 605, 550, 609, 659, 709, 583, 582, 635, 545, 678, 5… ## $ Rb &lt;dbl&gt; 175, 329, 76, 295, 313, 304, 318, 337, 323, 223, 306, 168, 25… ## $ Sr &lt;dbl&gt; 5817, 8306, 9181, 9644, 9940, 10287, 9917, 9931, 9897, 9209, … ## $ Y &lt;dbl&gt; 60, 15, 140, 119, 107, 180, 106, 52, 77, 112, 144, 237, 151, … ## $ Zr &lt;dbl&gt; 305, 255, 322, 280, 475, 160, 312, 319, 319, 552, 484, 373, 6… ## $ Pd &lt;dbl&gt; 0, 31, 78, 75, 66, 31, 72, 31, 54, 58, 70, 55, 81, 43, 6, 42,… ## $ Cd &lt;dbl&gt; 9, 13, 34, 55, 24, 34, 28, 56, 62, 96, 79, 41, 84, 64, 43, 75… ## $ I &lt;dbl&gt; 30, 38, 48, 80, 67, 23, 58, 30, 53, 93, 43, 36, 126, 58, 85, … ## $ Cs &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ Ba &lt;dbl&gt; 35, 63, 48, 25, 60, 132, 93, 172, 108, 108, 22, 74, 123, 82, … ## $ Nd &lt;dbl&gt; 12, 17, 67, 44, 75, 42, 97, 92, 68, 56, 59, 41, 50, 73, 56, 7… ## $ Sm &lt;dbl&gt; 0, 43, 46, 39, 64, 36, 12, 24, 69, 68, 17, 47, 59, 37, 45, 28… ## $ Yb &lt;dbl&gt; 166, 144, 282, 158, 218, 163, 184, 248, 166, 188, 169, 247, 2… ## $ Ta &lt;dbl&gt; 546, 789, 776, 703, 794, 919, 852, 926, 840, 847, 790, 736, 8… ## $ W &lt;dbl&gt; 1151, 1992, 2019, 2016, 2160, 2344, 2336, 2243, 2267, 2185, 2… ## $ Pb &lt;dbl&gt; 22, 77, 0, 13, 152, 54, 54, 75, 111, 78, 0, 92, 22, 0, 81, 19… ## $ Bi &lt;dbl&gt; 0, 72, 136, 199, 134, 157, 182, 151, 185, 116, 164, 141, 135,… ## $ `Mo inc` &lt;dbl&gt; 20427, 26323, 26778, 26550, 28310, 31356, 30631, 29040, 29104… ## $ `Mo coh` &lt;dbl&gt; 6812, 8799, 9117, 9303, 9886, 10140, 9968, 9973, 9701, 9751, … ## $ position &lt;dbl&gt; 31.54, 32.54, 33.54, 34.54, 35.54, 36.54, 37.54, 38.54, 39.54… ## $ label &lt;chr&gt; &quot;S1&quot;, &quot;S1&quot;, &quot;S1&quot;, &quot;S1&quot;, &quot;S1&quot;, &quot;S1&quot;, &quot;S1&quot;, &quot;S1&quot;, &quot;S1&quot;, &quot;S1&quot;, &quot;… 2.2.3 Raw Data Sometimes it is useful to work with raw data rather than the calculated intensity data from the Q-Spec software. In this case, the raw data can be read directly from the individual files in the relevant directory. For individual measurements this is fairly trivial, although it must be considered that the data output is not calibrated to an energy and the data are in counts, not intensities. If the entire scan is read, some mechanism to iterate through the individual data files, adding them to a structured data object with relevant metadata (positions, for example) is required. "],["optical-images-1.html", "2.3 Optical Images", " 2.3 Optical Images In order to make the images usable for plotting alongside other data, they need to be cropped to the scanned area and referenced to the position or a depth variable. The initial crop can be supressed using trim = FALSE if the whole image is desired. Images in R are read in as a matrix with 3 dimensions (length, width, and the three colours). The data can be imported using itraxR::itrax_image(). This function can produce a basic diagram (shown in subsequent section), but here we focus on the structure of the imported object. $image is a three dimensional array, the first two dimensions are the length of the core and the width, respectively. Those two dimensions have \"dimnames\" (rownames() and colnames() respectively). For the rownames(), this is an interpolated position, in mm, and for the colnames() this is the width in mm, and always begins at 0. The last dimension always has a length of three, and comprises of the red, green and blue values for each of the pixels. $meta is a table containing selected data from the scan metadata relevant to the image. itrax_image(file = &quot;CD166_19_S1/CD166_19_S1/optical0.tif&quot;, meta = &quot;CD166_19_S1/CD166_19_S1/document.txt&quot;, trim = FALSE) %&gt;% str() ## List of 2 ## $ image: num [1:7452, 1:524, 1:3] 0.0745 0.0745 0.0745 0.0824 0.0824 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 3 ## .. ..$ : chr [1:7452] &quot;0.5&quot; &quot;0.688001610522078&quot; &quot;0.876003221044155&quot; &quot;1.06400483156623&quot; ... ## .. ..$ : chr [1:524] &quot;0&quot; &quot;0.187976382179281&quot; &quot;0.375952764358561&quot; &quot;0.563929146537842&quot; ... ## .. ..$ : NULL ## $ meta :&#39;data.frame&#39;: 6 obs. of 3 variables: ## ..$ Parameter: chr [1:6] &quot;Start coordinate&quot; &quot;Stop coordinate&quot; &quot;Step size&quot; &quot;Optical Start&quot; ... ## ..$ Value : chr [1:6] &quot;31.5&quot; &quot;1314.1&quot; &quot;1000&quot; &quot;0.5&quot; ... ## ..$ Unit : chr [1:6] &quot;mm&quot; &quot;mm&quot; &quot;microns&quot; &quot;mm&quot; ... "],["radiographic-images-1.html", "2.4 Radiographic Images", " 2.4 Radiographic Images The function itraxR::itrax_radio() imports the processed radiographic images (*.tif) in a very similar way to that for the optical images, the main difference being the matrix only has two dimensions (length and width) as the image is greyscale. itrax_radiograph(file = &quot;CD166_19_S1/CD166_19_S1_RAD/radiograph0.tif&quot;, meta = &quot;CD166_19_S1/CD166_19_S1_RAD/document.txt&quot;) %&gt;% str() ## List of 2 ## $ image: num [1:6570, 1:67] 1 1 0.755 0.686 0.755 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : chr [1:6570] &quot;0&quot; &quot;0.200045669051606&quot; &quot;0.400091338103212&quot; &quot;0.600137007154818&quot; ... ## .. ..$ : chr [1:67] &quot;0&quot; &quot;0.200015220700152&quot; &quot;0.400030441400304&quot; &quot;0.600045662100457&quot; ... ## $ meta :&#39;data.frame&#39;: 10 obs. of 3 variables: ## ..$ Parameter: chr [1:10] &quot;Start coordinate&quot; &quot;Stop coordinate&quot; &quot;Step size&quot; &quot;Optical Start&quot; ... ## ..$ Value : chr [1:10] &quot;0.0&quot; &quot;1314.1&quot; &quot;200&quot; &quot;0.5&quot; ... ## ..$ Unit : chr [1:10] &quot;mm&quot; &quot;mm&quot; &quot;microns&quot; &quot;mm&quot; ... However, if there is a desire to manipulate the raw data from the radiographic image, some further work is required because the “pixel” is not square, but rectangular; that is to say the length of the pixel differs from its width. On the core scanner a single pixel has a width across the core of 20 μm, but has a variable coverage along the core (usually between 50 and 200 μm). The processed image downscales the pixel width to match the pixel length in order to force square pixels, losing some resolution along the way. In addition, it should be noted that unlike the optical images that always begin from position == 0, the radiographic images have defined start and end points just like an XRF scan, the parameters of which can be accessed from the $meta object, or using itraxR::itrax_meta(). "],["importing-everything.html", "2.5 Importing Everything", " 2.5 Importing Everything Given the structured nature of individual core section scans, it might be helpful to import everything into a single data object (list()). This is particularly helpful when dealing with large numbers of core sections. However, some care should be taken here, as it is easy to end up holding large quantities of data in memory, particularly where images are included. The structure of this can be adapted to your needs, but could look something like the following example. These are the data that will be used elsewhere in this guide. CD166_19_S1 &lt;- list(metadata = itrax_meta(&quot;CD166_19_S1/CD166_19_S1/document.txt&quot;), xrf = itrax_import(&quot;CD166_19_S1/CD166_19_S1/Results.txt&quot;, depth = 0, parameters = &quot;all&quot;), image = itrax_image(file = &quot;CD166_19_S1/CD166_19_S1/optical0.tif&quot;, meta = &quot;CD166_19_S1/CD166_19_S1/document.txt&quot;), radiograph = itrax_radiograph(file = &quot;CD166_19_S1/CD166_19_S1_RAD/radiograph0.tif&quot;, meta = &quot;CD166_19_S1/CD166_19_S1_RAD/document.txt&quot;, trim = as.numeric(itrax_meta(&quot;CD166_19_S1/CD166_19_S1/document.txt&quot;)[6:7,2]))) CD166_19_S2 &lt;- list(metadata = itrax_meta(&quot;CD166_19_S2/CD166_19_S2/document.txt&quot;), xrf = itrax_import(&quot;CD166_19_S2/CD166_19_S2/Results.txt&quot;, depth = max(CD166_19_S1$xrf$depth), parameters = &quot;all&quot;), image = itrax_image(file = &quot;CD166_19_S2/CD166_19_S2/optical0.tif&quot;, meta = &quot;CD166_19_S2/CD166_19_S2/document.txt&quot;), radiograph = itrax_radiograph(file = &quot;CD166_19_S2/CD166_19_S2_RAD/radiograph0.tif&quot;, meta = &quot;CD166_19_S2/CD166_19_S2_RAD/document.txt&quot;, trim = as.numeric(itrax_meta(&quot;CD166_19_S2/CD166_19_S2/document.txt&quot;)[6:7,2]))) CD166_19_S3 &lt;- list(metadata = itrax_meta(&quot;CD166_19_S3/CD166_19_S3/document.txt&quot;), xrf = itrax_import(&quot;CD166_19_S3/CD166_19_S3/Results.txt&quot;, depth = max(CD166_19_S2$xrf$depth), parameters = &quot;all&quot;), image = itrax_image(file = &quot;CD166_19_S3/CD166_19_S3/optical0.tif&quot;, meta = &quot;CD166_19_S3/CD166_19_S3/document.txt&quot;), radiograph = itrax_radiograph(file = &quot;CD166_19_S3/CD166_19_S3_RAD/radiograph0.tif&quot;, meta = &quot;CD166_19_S3/CD166_19_S3_RAD/document.txt&quot;, trim = as.numeric(itrax_meta(&quot;CD166_19_S3/CD166_19_S3/document.txt&quot;)[6:7,2]))) CD166_19_xrf &lt;- itrax_join(list(S1 = CD166_19_S1$xrf, S2 = CD166_19_S2$xrf, S3 = CD166_19_S3$xrf)) "],["tidying-data.html", "Chapter 3 Tidying Data", " Chapter 3 Tidying Data In the functionality provided by itraxR, the need for data cleaning is much reduced. However, you may still encounter poor quality data that needs removing from subsequent analysis. This can be broadly defined as: Data at the start and end of the the core, where a volume of core material is “missing”. Measurements where the optical configuration is out of position (marked as validity == 0), often due to holes or stones in the core. Areas of the core with low total counts. Individual measurements that are statistical outliers. The easiest way to do this is using a tidyverse style sequence of pipes that set the observations of faulty data as NA. "],["low-count-rates.html", "3.1 Low Count Rates", " 3.1 Low Count Rates The count rate (“cps”, or counts-per-second) is the rate of energy event detection at the detector, and is a function of both the excitation beam condition (tube type, voltage and current) and the matrix. Because the tube type and voltage are often chosen based on other considerations, the operator usually only adjusts the tube current to optimise the count rate. The higher the total counts for each measurement, the better the measurement will be in terms of detection limits and uncertainties. Higher count rates allow for shorter dwell times, but this must be balanced against the need to minimise harmonics in the spectra. Harmonics can occur where the photon flux is so high that the detector cannot differentiate between two photons, and so registers the sum energy instead. For example, a particularly common phenomenon is the detection of two Fe Kα photons (energy 6.4 keV) as a single photon with the energy 12.8 keV. The graph below shows that for these cores there is the expected positive correlation between the count rate and the Fe Kα * 2 sum peak. To some extent the Q-Spec software will accommodate these harmonics, but operators commonly aim for a count rate of between 30-50 kcps. ggplot(data = CD166_19_xrf, mapping = aes(x = cps, y = `Fe a*2`)) + geom_point(alpha = 0.1) Deletion criteria can be on the basis of count rates (excluding very low and high values), selected harmonics (e.g. Fe a*2), or both. An example is shown below. CD166_19_xrf %&gt;% mutate(in_cps_tolerance = ifelse(cps &lt;=30000 | cps &gt;=60000 | is.na(cps) == TRUE, FALSE, TRUE)) %&gt;% ggplot(mapping = aes(x = depth, y = cps, col = in_cps_tolerance)) + geom_line(aes(group = 1)) + scale_x_reverse() + geom_hline(yintercept = c(30000, 60000)) + geom_rug(sides = &quot;b&quot;, data = . %&gt;% filter(in_cps_tolerance == FALSE)) It is possible, and sometimes preferable, to use some statistic to define the limits, rather than using arbitrary limits. For example, using the standard deviation (or even a confidence interval) to identify outlier data. For example: CD166_19_xrf %&gt;% mutate(in_cps_tolerance = ifelse(cps &lt;= mean(cps)-(3*sd(cps)) | cps &gt;=mean(cps)+(3*sd(cps)) | is.na(cps) == TRUE, FALSE, TRUE)) %&gt;% ggplot(mapping = aes(x = depth, y = cps, col = in_cps_tolerance)) + geom_line(aes(group = 1)) + scale_x_reverse() + geom_hline(yintercept = c(mean(CD166_19_xrf$cps)-(3*sd(CD166_19_xrf$cps)), mean(CD166_19_xrf$cps)+(3*sd(CD166_19_xrf$cps)))) + geom_rug(sides = &quot;b&quot;, data = . %&gt;% filter(in_cps_tolerance == FALSE)) "],["model-errors-mse.html", "3.2 Model Errors (MSE)", " 3.2 Model Errors (MSE) The mean-squared-error (MSE) of the model provides an indication of the goodness-of-fit for the peak fitting model implemented in the Q-Spec software. Operators aim to produce models with the lowest possible MSE, but compromises are sometimes required because normally the same model should be applied to observations that are being compared (for example, for the same core sequence). CD166_19_xrf %&gt;% mutate(in_mse_tolerance = ifelse(MSE &gt;=2, FALSE, TRUE)) %&gt;% ggplot(mapping = aes(x = depth, y = MSE, col = in_mse_tolerance)) + geom_line(aes(group = 1)) + scale_x_reverse() + geom_hline(yintercept = 2) + geom_rug(sides = &quot;b&quot;, data = . %&gt;% filter(in_mse_tolerance == FALSE)) "],["surface-slope.html", "3.3 Surface Slope", " 3.3 Surface Slope There is a relationship between the slope of the surface of the core material and the intensity for most elements. Hence areas with a large slope may produce an increase or decrease in a particular element intensity regardless of any actual change in the abundance of an element. This can be corrected for where the effect can be quantified experimentally, but Jarvis, Croudace, and Rothwell (2015) report results of experiments where the slope varies from -0.3 to +0.3 causing variation of around 120 to 80% of the true value. As such, we might initially seek to exclude areas of the core with a high slope. By default, itraxR::itrax_import() doesn’t import the sample surface variable, so the parameter parameters = \"all\" should be passed to access it. The computation is simple to perform using dplyr::lag(), and could be used as part of a conditional expression that would mark all measurements with a slope beyond a certain tolerance as having validity == FALSE. For example, the example below marks all values with a slope (in either direction) greater than 0.1 mm/200 μm (1:5) as being invalid. As shown for the core below, the core slope is well within the defined tolerances. CD166_19_xrf %&gt;% mutate(slope = `sample surface` - dplyr::lag(`sample surface`)) %&gt;% mutate(in_tolerance = ifelse(slope &lt;=-0.1 | slope &gt;=0.1 | is.na(slope) == TRUE, FALSE, TRUE)) %&gt;% ggplot(mapping = aes(x = depth, y = slope, col = in_tolerance)) + scale_y_continuous(limits = c(-0.15, 0.15), oob = scales::squish) + geom_line(aes(group = 1)) + geom_hline(yintercept = c(-0.1, 0.1)) + geom_rug(data = . %&gt;% filter(validity == FALSE)) + scale_x_reverse() References Jarvis, S., I. W. Croudace, and R. G. Rothwell. 2015. “Parameter Optimisation for the ITRAX Core Scanner.” In Micro-XRF Studies of Sediment Cores, Developments in Paleoenvironmental Research 17, edited by I. W. Croudace and R. G. Rothwell, 535–62. Dordrecht: Springer. "],["high-argon.html", "3.4 High Argon", " 3.4 High Argon Argon is present in the atmosphere at concentrations of around 1% volume, and therefore is present in our spectra because the instrument operates in air. The amount of Ar that is measured is dependent on the length of the optical path, which should remain relatively constant, and any air present in the sample area. This can vary depending on cracks and voids, and thus high Ar is often an indicator of a potentially problematic measurement. This method differs from the others because it uses the standard deviation to compute the limits - this is because every scan configuration and material will be different. You will need to tune these limits to suit your data. CD166_19_xrf %&gt;% mutate(in_Ar_tolerance = ifelse(Ar &gt;=mean(Ar, na.rm = TRUE) + 2*sd(Ar, na.rm = TRUE) | is.na(Ar), FALSE, TRUE)) %&gt;% ggplot(mapping = aes(x = depth, y = Ar, col = in_Ar_tolerance)) + geom_line(aes(group = 1)) + scale_x_reverse() + geom_hline(yintercept = mean(CD166_19_xrf$Ar, na.rm = TRUE) + 2*sd(CD166_19_xrf$Ar, na.rm = TRUE)) + geom_rug(sides = &quot;b&quot;, data = . %&gt;% filter(in_Ar_tolerance == FALSE)) ## Warning: Removed 3 rows containing missing values or values outside the scale range ## (`geom_line()`). "],["combining-validity-flags.html", "3.5 Combining “Validity” Flags", " 3.5 Combining “Validity” Flags It is often desirable to combine all deletion criteria into a single binary variable. This means combining multiple binary variables, returning FALSE if any of the values are FALSE, but only returning TRUE if all of the values are TRUE. CD166_19_xrf %&gt;% select(-any_of(&quot;qc&quot;)) %&gt;% mutate(slope = `sample surface` - dplyr::lag(`sample surface`)) %&gt;% mutate(in_slope_tolerance = ifelse(slope &lt;=-0.1 | slope &gt;=0.1 | is.na(slope) == TRUE, FALSE, TRUE)) %&gt;% select(-slope) %&gt;% mutate(in_cps_tolerance = ifelse(cps &lt;=30000 | cps &gt;=60000 | is.na(cps) == TRUE, FALSE, TRUE)) %&gt;% mutate(in_mse_tolerance = ifelse(MSE &lt;=2, TRUE, FALSE)) %&gt;% mutate(in_Ar_tolerance = ifelse(Ar &gt;=mean(Ar, na.rm = TRUE) + 2*sd(Ar, na.rm = TRUE) | is.na(Ar), FALSE, TRUE)) %&gt;% rowwise() %&gt;% mutate(qc = !any(c(validity, in_slope_tolerance, in_cps_tolerance, in_mse_tolerance, in_Ar_tolerance) == FALSE)) %&gt;% ungroup() %&gt;% select(c(depth, validity, in_slope_tolerance, in_cps_tolerance, in_mse_tolerance, in_Ar_tolerance, qc)) %&gt;% pivot_longer(!depth) %&gt;% mutate(name = factor(name, levels = c(&quot;validity&quot;, &quot;in_slope_tolerance&quot;, &quot;in_cps_tolerance&quot;, &quot;in_mse_tolerance&quot;, &quot;in_Ar_tolerance&quot;, &quot;qc&quot;))) %&gt;% ggplot() + geom_tile(aes(x = depth, y = 1, fill = value)) + scale_x_reverse() + facet_wrap(vars(name), ncol = 1) + theme(axis.title.y = element_blank(), axis.text.y = element_blank(), axis.ticks.y = element_blank()) Bear in mind that this doesn’t remove observations considered defective, only marks them has qc == FALSE. If they are to be excluded from subsequent analysis, they should be removed using filter(qc == TRUE). "],["tube-current.html", "3.6 Tube current", " 3.6 Tube current Although element peak areas are not exactly linearly related to tube current, they can be approximated to a linear relationship (see Jarvis, Croudace, and Rothwell (2015)). The Q-Spec software can report either peak areas (n) or intensities (n/mA), and it is easy to tell which you are using: peak areas are always integers, but intensities are always fractions. It is easy to adjust from one to the other: current &lt;- as.numeric(CD166_19_S1$metadata[18, 2]) CD166_19_S1$xrf %&gt;% mutate(intensity_Fe = round(Fe/current, 3)) %&gt;% # convert to intensity mutate(peakarea_Fe = round(intensity_Fe*current)) %&gt;% # convert to peak area select(Fe, intensity_Fe, peakarea_Fe) %&gt;% head() ## # A tibble: 6 × 3 ## Fe intensity_Fe peakarea_Fe ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 19786 660. 19786 ## 2 35168 1172. 35168 ## 3 36494 1216. 36494 ## 4 35952 1198. 35952 ## 5 35512 1184. 35512 ## 6 44303 1477. 44303 References Jarvis, S., I. W. Croudace, and R. G. Rothwell. 2015. “Parameter Optimisation for the ITRAX Core Scanner.” In Micro-XRF Studies of Sediment Cores, Developments in Paleoenvironmental Research 17, edited by I. W. Croudace and R. G. Rothwell, 535–62. Dordrecht: Springer. "],["dead-time.html", "3.7 Dead Time", " 3.7 Dead Time In silicone drift ED-XRF detectors the electronics are only so fast. This limits the flux of photons into the detector that can be measured. When the count rate is high, the detector will not be making observations whilst the electronics “catch-up”; this is the dead-time, and here it is expressed as a fraction of the overall dwell time. When the system is optimised variations in dead-time are usually small enough to be ignored, but where the matrix or configuration leads to large variations in dead-time they should be corrected for. ggplot(data = CD166_19_xrf, aes(x = depth, y = Dt)) + scale_x_reverse() + scale_y_continuous(sec.axis = sec_axis( trans=~(.+(1-mean(CD166_19_xrf$Dt, na.rm = TRUE))), name=&quot;Correction Factor&quot;)) + geom_line() + geom_hline(yintercept = mean(CD166_19_xrf$Dt, na.rm = TRUE), linetype = &quot;dotted&quot;) CD166_19_xrf %&gt;% mutate(newDt = Dt+(1-mean(CD166_19_xrf$Dt, na.rm = TRUE))) %&gt;% mutate(across(any_of(elementsList)) * newDt) %&gt;% select(-newDt) %&gt;% head() ## # A tibble: 6 × 57 ## depth MSE cps validity Al Si P S Cl Ar K Ca ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 1.26 22415 TRUE 41.1 178. 0 0 729. 699. 1417. 60173. ## 2 1 1.41 34525 TRUE 76.2 276. 0 10.0 1321. 605. 2571. 113013. ## 3 2 1.57 38370 TRUE 57.0 306. 0 0 1514. 595. 2629. 144356. ## 4 3 1.55 39796 TRUE 74.0 330. 0 32.0 1471. 506. 2379. 163016. ## 5 4 1.41 40022 TRUE 26.1 207. 0 21.1 1318. 557. 2275. 153880. ## 6 5 1.41 41973 TRUE 53.4 235. 0 37.3 1753. 595. 2969. 136895. ## # ℹ 45 more variables: Sc &lt;dbl&gt;, Ti &lt;dbl&gt;, V &lt;dbl&gt;, Cr &lt;dbl&gt;, Mn &lt;dbl&gt;, ## # Fe &lt;dbl&gt;, Ni &lt;dbl&gt;, Cu &lt;dbl&gt;, Zn &lt;dbl&gt;, Ga &lt;dbl&gt;, Ge &lt;dbl&gt;, Br &lt;dbl&gt;, ## # Rb &lt;dbl&gt;, Sr &lt;dbl&gt;, Y &lt;dbl&gt;, Zr &lt;dbl&gt;, Pd &lt;dbl&gt;, Cd &lt;dbl&gt;, I &lt;dbl&gt;, ## # Cs &lt;dbl&gt;, Ba &lt;dbl&gt;, Nd &lt;dbl&gt;, Sm &lt;dbl&gt;, Yb &lt;dbl&gt;, Ta &lt;dbl&gt;, W &lt;dbl&gt;, ## # Pb &lt;dbl&gt;, Bi &lt;dbl&gt;, `Mo inc` &lt;dbl&gt;, `Mo coh` &lt;dbl&gt;, filename &lt;chr&gt;, ## # position &lt;dbl&gt;, `sample surface` &lt;dbl&gt;, `E-gain` &lt;dbl&gt;, `E-offset` &lt;dbl&gt;, ## # `F-slope` &lt;dbl&gt;, `F-offset` &lt;dbl&gt;, `Fe a*2` &lt;dbl&gt;, `Fe a+b` &lt;dbl&gt;, … "],["correcting-for-water-content.html", "3.8 Correcting for Water Content", " 3.8 Correcting for Water Content Water content can be corrected for as a simple dilution effect, for example, if a sample contains 50% water by weight, the peak intensity can be doubled. The water content must be determined using some other lab method (e.g. loss-on-ignition), and it is likely that this can only be performed on samples larger than the original scan interval (e.g. 10 mm subsamples). A conservative approach is to use the itrax_averaging() function to re-sample the XRF data to the resolution of the water content data, or the reverse (upsampling the water content data) might be applied with caution. For some scans the Mo inc./Mo coh. varies with water content and can be used as a proxy to correct for water content. "],["correcting-grain-size.html", "3.9 Correcting Grain Size", " 3.9 Correcting Grain Size Grain size variations are known the affect the detection of some elements. Grain size determinations made using analytical equipment (e.g. a laser granulometer) can be used to correct for grain size. "],["uncertainties.html", "3.10 Uncertainties", " 3.10 Uncertainties The best way to quantify the uncertainties in the XRF data is in repeat measurements. The example dataset contains short sections of repeat measurements. An example might begin by reading in the repeat scan data, as below for CD166_19_S1. CD166_19_S1_REP1 &lt;- itrax_import(&quot;CD166_19_S1/CD166_19_S1_REP1/Results.txt&quot;) %&gt;% select(any_of(c(elementsList, &quot;position&quot;, &quot;Mo inc&quot;, &quot;Mo coh&quot;))) %&gt;% mutate(scan = &quot;scan1&quot;) CD166_19_S1_REP2 &lt;- itrax_import(&quot;CD166_19_S1/CD166_19_S1_REP2/Results.txt&quot;) %&gt;% select(any_of(c(elementsList, &quot;position&quot;, &quot;Mo inc&quot;, &quot;Mo coh&quot;))) %&gt;% mutate(scan = &quot;scan2&quot;) CD166_19_S1_REP3 &lt;- CD166_19_S1$xrf %&gt;% filter(position &gt;= min(c(CD166_19_S1_REP1$position, CD166_19_S1_REP2$position)) &amp; position &lt;= max(c(CD166_19_S1_REP1$position, CD166_19_S1_REP2$position))) %&gt;% select(any_of(c(elementsList, &quot;position&quot;, &quot;Mo inc&quot;, &quot;Mo coh&quot;))) %&gt;% mutate(scan = &quot;scan3&quot;) It should then be combined into a single dataset, and finally, a function to calculate the uncertainties is called (here we use sd()). The errors package is used to create an output that can combine both the value and the associated uncertainty. It is a really neat way of working with quantities and ensures the uncertainties are correctly propagated throughout the work. S1_reps &lt;- list(CD166_19_S1_REP1, CD166_19_S1_REP2, CD166_19_S1_REP3) %&gt;% reduce(full_join) %&gt;% select(scan, position, everything()) %&gt;% group_by(position) %&gt;% summarise(across(any_of(c(elementsList, &quot;Mo inc&quot;, &quot;Mo coh&quot;)), function(x){set_errors(x = mean(x, na.rm = TRUE), value = sd(x, na.rm = TRUE))})) These can be plotted using the mean and standard deviation instead of a single value using errors::errors_min() and errors::errors_max() really easily. Note that the errors are propagated through any arithmetic - in the example below, where the ratio of coherent Mo and incoherent Mo peak area is calculated. S1_reps %&gt;% mutate(`coh/inc` = `Mo coh`/`Mo inc`) %&gt;% select(Al, Si, Ti, Fe, Pb, Ca, `coh/inc`, position) %&gt;% pivot_longer(!c(&quot;position&quot;), names_to = &quot;elements&quot;, values_to = &quot;peakarea&quot;) %&gt;% drop_na() %&gt;% mutate(elements = factor(elements, levels = c(elementsList, &quot;coh/inc&quot;))) %&gt;% ggplot(aes(x = peakarea, y = position)) + scale_y_reverse() + geom_ribbonh(aes(xmin = errors_min(peakarea), xmax = errors_max(peakarea)), fill = &quot;grey80&quot;) + geom_lineh() + scale_x_continuous(n.breaks = 3) + facet_wrap(vars(elements), scales = &quot;free_x&quot;, nrow = 1) + theme_paleo() This information can be used to inform decisions about the inclusion and treatment of the remainder of the data - where there are unacceptably high uncertainties that element might be considered for exclusion from further analysis. "],["noisy-data.html", "3.11 Noisy Data", " 3.11 Noisy Data It is not always possible to do repeat scans for entire scanning projects, and as such it may be necessary to look at others ways of identifying problematic noisy data. Much of this can be done by good judgement on the part of the analyst, and will depend on the requirements of the projects. However, there are some analytical tools that can help. One method is to look at the autocorrelation of a time-series. We can use the auto-correlation factor (ACF) to explore this for our data. We might begin with examples for just two elements, Ca and Pb. It is clear from these plots that whereas Ca has the expected pattern of autocorrelation, Pb has generally much lower and a more disordered pattern of autocorrelation. library(forecast) egg::ggarrange( ggAcf(CD166_19_xrf$Ca) + ylim(c(NA,1)), ggAcf(CD166_19_xrf$Pb) + ylim(c(NA,1)), nrow = 1) Now we might move on to making calculations for the entirety of the data. In the example below we apply the Acf() function to each of the elements. We might then explore the data by sorting for an arbitrary lag, or by plotting the results together for inspection. In this case, the elements have been sorted by the autocorrelation value at a lag of 5. Although the visualisation is a bit messy with all the elements plotting over one another, it is clear that some elements exhibit very low autocorrelation (e.g. I, Pb, Sc, Ge, Cs, P) and could be possible candidates for exclusion. apply(CD166_19_xrf %&gt;% select(any_of(elementsList)), 2, FUN = function(x){round(Acf(x, plot = F)$acf, 3)}) %&gt;% as_tibble(rownames = &quot;lag&quot;) %&gt;% pivot_longer(!c(&quot;lag&quot;), names_to = &quot;elements&quot;, values_to = &quot;value&quot;) %&gt;% mutate(lag = as.numeric(lag), elements = factor(elements, levels = filter(., lag == 5) %&gt;% arrange(desc(value)) %&gt;% pull(elements))) %&gt;% group_by(elements) %&gt;% ggplot(aes(x = lag, y = value, col = elements)) + geom_line() It is worth noting that this analysis has been performed on the entirety of the data, but there is no reason why the data could be problematic for some facies and acceptable for others. It may be necessary to perform some facies analysis and perform these analyses per facies. Putting all the selection criteria together could look something like the example below. # identify acceptable variables apply(CD166_19_xrf %&gt;% select(any_of(elementsList)), 2, FUN = function(x){round(Acf(x, plot = F)$acf, 3)}) %&gt;% as_tibble(rownames = &quot;lag&quot;) %&gt;% pivot_longer(!c(&quot;lag&quot;), names_to = &quot;elements&quot;, values_to = &quot;value&quot;) %&gt;% mutate(lag = as.numeric(lag), elements = factor(elements, levels = filter(., lag == 5) %&gt;% arrange(desc(value)) %&gt;% pull(elements))) %&gt;% group_by(elements) %&gt;% filter(lag == 5) %&gt;% filter(value &gt;= 0.8) %&gt;% pull(elements) %&gt;% ordered() -&gt; myElements # get acceptable observations CD166_19_xrf %&gt;% filter(qc == TRUE) %&gt;% # pivot long select(any_of(myElements), depth, label) %&gt;% tidyr::pivot_longer(!c(&quot;depth&quot;, &quot;label&quot;), names_to = &quot;elements&quot;, values_to = &quot;peakarea&quot;) %&gt;% mutate(elements = factor(elements, levels = c(elementsList, &quot;coh/inc&quot;))) %&gt;% # plot ggplot(aes(x = peakarea, y = depth)) + tidypaleo::geom_lineh(aes(color = label)) + scale_y_reverse() + scale_x_continuous(n.breaks = 2) + facet_geochem_gridh(vars(elements)) + labs(x = &quot;peak area&quot;, y = &quot;Depth [mm]&quot;) + tidypaleo::theme_paleo() + theme(legend.position = &quot;none&quot;) "],["reversing-data.html", "3.12 Reversing Data", " 3.12 Reversing Data Sometimes a core section goes in the scanner backwards! If this has happened, the functions below simply re-map the position and depth data for the radiograph, the optical image and the xrf data. These functions can either be called at the same time as the data itself (for example in a plot), or used to over-write the original data itself. # for xrf data reverse_xrf &lt;- function(xrf = itrax_import()){ xrf %&gt;% mutate(depth = rev(depth), position = rev(position)) %&gt;% arrange(position) %&gt;% return() } # for images reverse_image &lt;- function(image = itrax_image()){ image &lt;- image[dim(image)[1]:1, , ] row.names(image) &lt;- rev(row.names(image)) return(image) } # for radiographs reverse_radio &lt;- function(image = itrax_radiograph()){ image &lt;- image[dim(image)[1]:1, ] row.names(image) &lt;- rev(row.names(image)) return(image) } "],["visualising-raw-data.html", "3.13 Visualising Raw Data", " 3.13 Visualising Raw Data A useful tool for investigating areas of high errors or other problems in the scan data is to visualise the raw spectral data. To inspect or read individual spectra, you can plot them from the object returned from itrax_restspectra(). itrax_spectra(filename = unz(description = &quot;CD166_19_S1/CD166_19_S1/XRF Data.zip&quot;, filename = &quot;XRF data/L000642.spe&quot;), parameters = &quot;CD166_19_S1/CD166_19_S1/Results_ settings.dfl&quot;) %&gt;% invisible() This rarely offers much insight, because we are interested in comparing multiple spectra. The itrax_restspectra() function takes its name from a similar function in the Q-Spec software from Cox Analytical Systems, Sweden. This shows a heat map of the spectra, and can be used to identify areas where particular spectral lines appear and disappear along the core. At its simplest, it can be used to produce a plot of a single scan section. itrax_restspectra(foldername = &quot;CD166_19_S1/CD166_19_S1/XRF Data.zip&quot;, parameters = &quot;CD166_19_S1/CD166_19_S1/Results_ settings.dfl&quot; ) %&gt;% invisible() By combining all of the sections, and joining the raw spectral data with the processed data from the Results.txt (Q-Spec) output file, it can be used as a powerful diagnostic tool. read_spectra &lt;- function(foldername, labeltext){itrax_restspectra(foldername = foldername, plot = FALSE) %&gt;% mutate(label = labeltext) %&gt;% select(label, everything()) %&gt;% mutate(filename = str_split(filename, pattern = &quot;/&quot;) %&gt;% sapply(., `[`, 2))} # import the channel energy information settings &lt;- itrax_qspecsettings(&quot;CD166_19_S1/CD166_19_S1/Results_ settings.dfl&quot;) # import and join them left_join(CD166_19_xrf %&gt;% select(depth, validity, filename, label) %&gt;% mutate(filename = filename %&gt;% str_split(pattern = &quot;\\\\\\\\&quot;) %&gt;% sapply(., `[`, 7)), bind_rows(read_spectra(foldername = &quot;CD166_19_S1/CD166_19_S1/XRF Data.zip&quot;, labeltext = &quot;S1&quot;), read_spectra(foldername = &quot;CD166_19_S2/CD166_19_S2/XRF Data.zip&quot;, labeltext = &quot;S2&quot;), read_spectra(foldername = &quot;CD166_19_S3/CD166_19_S3/XRF Data.zip&quot;, labeltext = &quot;S3&quot;)) %&gt;% mutate(label = as.factor(label)), by = c(&quot;filename&quot;, &quot;label&quot;)) %&gt;% pivot_longer(cols = -c(depth, validity, filename, label, position), names_to = &quot;channel&quot;, values_to = &quot;counts&quot;) %&gt;% mutate(channel = as.numeric(channel)) %&gt;% select(-c(&quot;filename&quot;, &quot;validity&quot;, &quot;position&quot;)) %&gt;% ggplot(aes(x = channel, y = depth, fill = counts)) + geom_tile() + scale_fill_gradient(name = &quot;value&quot;, trans = &quot;pseudo_log&quot;, low = &quot;#132B43&quot;, high = &quot;#56B1F7&quot;, labels = round) + scale_y_reverse(breaks = seq(from = 0, to = max(CD166_19_xrf$depth), by = 500), name = &quot;depth [mm]&quot;) + scale_x_continuous(name = &quot;channel [n]&quot;, sec.axis = sec_axis(trans = ~ ((. * as.numeric(settings[1,2])) + as.numeric(settings[2,2])), name = &quot;energy [k eV]&quot;)) + guides(fill = &quot;none&quot;) + facet_grid(rows = vars(label), scales = &quot;free_y&quot;, space = &quot;free_y&quot;) rm(settings, read_spectra) "],["plotting.html", "Chapter 4 Plotting", " Chapter 4 Plotting There are a number of plotting options included in base R and some palaeoenvironmental packages like analouge, but here we will use ggplot2 and compatible packages. We will also be using tidypaleo, a package made available by Dewey Dunnington on Github. "],["plotting-xrf-data.html", "4.1 Plotting XRF Data", " 4.1 Plotting XRF Data For simple biplots, ggplot2 provides the following solution. This includes different colour coding to differentiate the core sections. ggplot(data = na.omit(CD166_19_xrf), mapping = aes(x = depth, y = `Mo coh`/`Mo inc`)) + geom_line(aes(color = label)) + coord_flip() + scale_x_reverse() + labs(x = &quot;Depth [mm]&quot;, color = &quot;Core&quot;) + theme_classic() + theme(legend.position = &quot;none&quot;) Where a traditional stratigraphic plot is desired, the tidypaleo package provides useful functionality for producing these. However, the data does need to converted to long-form from the existing table. In this way tidypaleo::facet_geochem_gridh works in a way similar to ggplot::facet_wrap but has been creates a stratigraphic plot. The elementsList is used with select(any_of()) to include only columns that are chemical elements. Depth and label are also added because they are used in the plot. Although this is useful for producing large-format summary diagrams of the data, these do not render well in smaller plot areas, so a small subset of variables has been selected manually; see the code comments below. Note that in the line mutate(), the vector passed to levels = controls the order in which the plots appear. In this example, they will appear in order of atomic weight, and the coh/inc ratio will always be last. xrfStrat &lt;- CD166_19_xrf %&gt;% mutate(`coh/inc` = `Mo coh`/`Mo inc`) %&gt;% select(any_of(elementsList), `coh/inc`, depth, label) %&gt;% # this is useful but produces a very large plot that doesn&#39;t render well select(Fe, Ti, `coh/inc`, Mn, depth, label) %&gt;% # a smaller set of elements is defined here manually. tidyr::pivot_longer(!c(&quot;depth&quot;, &quot;label&quot;), names_to = &quot;elements&quot;, values_to = &quot;peakarea&quot;) %&gt;% tidyr::drop_na() %&gt;% mutate(elements = factor(elements, levels = c(elementsList, &quot;coh/inc&quot;))) %&gt;% # note that the levels controls the order ggplot(aes(x = peakarea, y = depth)) + tidypaleo::geom_lineh(aes(color = label)) + scale_y_reverse() + scale_x_continuous(n.breaks = 2) + facet_geochem_gridh(vars(elements)) + labs(x = &quot;peak area&quot;, y = &quot;Depth [mm]&quot;) + tidypaleo::theme_paleo() + theme(legend.position = &quot;none&quot;) print(xrfStrat) Notice that in the previous example the data must be converted to “long-form” from “short-form” for tidypaleo, using tidyr::pivot_longer. “Short-form” data has observations as rows, and variables as columns, whereas “long-form” data has a column for names, which in this case are the elements, and a column for values, in this case the peak intensities. This results in many rows, but few columns. CD166_19_xrf %&gt;% select(any_of(elementsList), depth, label) %&gt;% tidyr::pivot_longer(!c(&quot;depth&quot;, &quot;label&quot;), names_to = &quot;elements&quot;, values_to = &quot;peakarea&quot;) %&gt;% glimpse() ## Rows: 151,632 ## Columns: 4 ## $ depth &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ label &lt;chr&gt; &quot;S1&quot;, &quot;S1&quot;, &quot;S1&quot;, &quot;S1&quot;, &quot;S1&quot;, &quot;S1&quot;, &quot;S1&quot;, &quot;S1&quot;, &quot;S1&quot;, &quot;S1&quot;, &quot;… ## $ elements &lt;chr&gt; &quot;Al&quot;, &quot;Si&quot;, &quot;P&quot;, &quot;S&quot;, &quot;Cl&quot;, &quot;Ar&quot;, &quot;K&quot;, &quot;Ca&quot;, &quot;Sc&quot;, &quot;Ti&quot;, &quot;V&quot;,… ## $ peakarea &lt;dbl&gt; 41, 177, 0, 0, 726, 697, 1412, 59965, 19, 804, 26, 220, 231, … "],["plotting-images.html", "4.2 Plotting Images", " 4.2 Plotting Images The package itraxR provides functions for importing and creating basic plots of both the photographic and radiographic images. For the optical images, it is almost always necessary to trim the image to the extent of the actual scan, or extraneous image is included (e.g. the calibration card in the example shown below). The function itraxR::itrax_image() allows the extent of the scan to be defined, or extracted from the metadata. In the example below, trim = FALSE forces the whole image to be shown. When dealing with high aspect ratio images like these, it is often useful to use svgPanZoom::svgPanZoom() instead of print() to view the output. myImage &lt;- itrax_image(file = &quot;CD166_19_S1/CD166_19_S1/optical0.tif&quot;, meta = &quot;CD166_19_S1/CD166_19_S1/document.txt&quot;, trim = FALSE, plot = TRUE ) For radiographic images the function itraxR::itrax_radiograph() can be used using an identical syntax as itraxR::itrax_image(). Often the contrast needs to be adjusted, so a contrast-adjusted radiograph is supplied in the example data (radiograph0.tif). myRadio &lt;- itrax_radiograph(file = &quot;CD166_19_S1/CD166_19_S1_RAD/radiograph0.tif&quot;, meta = &quot;CD166_19_S1/CD166_19_S1_RAD/document.txt&quot;, plot = TRUE ) The simple plots generated as a side effect of these functions are sometimes sufficient for simple plots, but for more complex diagrams you will need to generate them from scratch. For example, it is often desirable to overplot the radiograph on the optical image. The code below does this. Note the calculations made for the xmin = and ymin = parameters for annotation_custom() when adding the radiograph rasterGrob(); these correctly position the radiograph along the centre of the image. ggplot() + scale_y_reverse(limits = rev(range(as.numeric(row.names(myImage$image))))) + scale_x_continuous(breaks = round(range(as.numeric(colnames(myImage$image))), 0), limits = range(as.numeric(colnames(myImage$image)))) + labs(y = &quot;Position [mm]&quot;, x = &quot;[mm]&quot;) + annotation_custom(rasterGrob(myImage$image, width = unit(1, &quot;npc&quot;), height = unit(1, &quot;npc&quot;)), ymax = max(as.numeric(row.names(myImage$image)))/-1, ymin = min(as.numeric(row.names(myImage$image)))/-1, xmin = min(as.numeric(colnames(myImage$image))), xmax = max(as.numeric(colnames(myImage$image))) ) + annotation_custom(rasterGrob(myRadio$image, width = unit(1, &quot;npc&quot;), height = unit(1, &quot;npc&quot;)), ymax = max(as.numeric(row.names(myRadio$image)))/-1, ymin = min(as.numeric(row.names(myRadio$image)))/-1, xmin = mean(as.numeric(colnames(myImage$image))) - mean(as.numeric(colnames(myRadio$image))), xmax = mean(as.numeric(colnames(myImage$image))) + mean(as.numeric(colnames(myRadio$image))) ) + coord_fixed(ratio = 1) It is often desirable to combine a sequence of cores. This is trivial for cores like CD166_19 where there are no overlapping sections. However, note that the coring depth information is different from the position that these images are referenced to. The coring depth is not contained in any of the Itrax metadata, and should be recorded elsewhere, whereas position refers to the coordinates on the Itrax core holder. In the code below the depth information is extracted from the combined XRF data created in a previous chapter (for example using max(CD166_19[which(CD166_19$label == \"S2\"),]$depth) to get the maximum depth of the middle section. The radiographs can be added in the same fashion as the previous example. ggplot() + scale_y_reverse(limits = rev(range(CD166_19_xrf$depth))) + scale_x_continuous(breaks = round(range(as.numeric(colnames(CD166_19_S1$image$image))), 0), limits = range(as.numeric(colnames(CD166_19_S1$image$image)))) + coord_fixed(ratio = 1) + labs(y = &quot;Depth [mm]&quot;, x = &quot;[mm]&quot;) + annotation_custom(rasterGrob(CD166_19_S1$image$image, width = unit(1, &quot;npc&quot;), height = unit(1, &quot;npc&quot;)), ymax = max(CD166_19_S1$xrf$depth)/-1, ymin = min(CD166_19_S1$xrf$depth)/-1, xmin = min(as.numeric(colnames(CD166_19_S1$image$image))), xmax = max(as.numeric(colnames(CD166_19_S1$image$image))) ) + annotation_custom(rasterGrob(CD166_19_S2$image$image, width = unit(1, &quot;npc&quot;), height = unit(1, &quot;npc&quot;)), ymax = max(CD166_19_S2$xrf$depth)/-1, ymin = min(CD166_19_S2$xrf$depth)/-1, xmin = min(as.numeric(colnames(CD166_19_S2$image$image))), xmax = max(as.numeric(colnames(CD166_19_S2$image$image))) ) + annotation_custom(rasterGrob(CD166_19_S3$image$image, width = unit(1, &quot;npc&quot;), height = unit(1, &quot;npc&quot;)), ymax = max(CD166_19_S3$xrf$depth)/-1, ymin = min(CD166_19_S3$xrf$depth)/-1, xmin = min(as.numeric(colnames(CD166_19_S3$image$image))), xmax = max(as.numeric(colnames(CD166_19_S3$image$image))) ) Where there are overlaps it is often desirable to plot the cores adjacent to one another. It is simply a case of adjusting the xmin = and xmax = parameters in the annotation_custom() function for every other section. In this context the x-axis labels become largely meaningless and can be omitted. ggplot() + scale_y_reverse(limits = rev(range(CD166_19_xrf$depth))) + scale_x_continuous(breaks = round(c(0, max(as.numeric(colnames(CD166_19_S2$image$image)))*2)), limits = c(0, max(as.numeric(colnames(CD166_19_S2$image$image)))*2)) + coord_fixed(ratio = 1) + labs(y = &quot;Depth [mm]&quot;, x = &quot;[mm]&quot;) + annotation_custom(rasterGrob(CD166_19_S1$image$image, width = unit(1, &quot;npc&quot;), height = unit(1, &quot;npc&quot;)), ymax = max(CD166_19_S1$xrf$depth)/-1, ymin = min(CD166_19_S1$xrf$depth)/-1, xmin = min(as.numeric(colnames(CD166_19_S1$image$image))), xmax = max(as.numeric(colnames(CD166_19_S1$image$image))) ) + annotation_custom(rasterGrob(CD166_19_S2$image$image, width = unit(1, &quot;npc&quot;), height = unit(1, &quot;npc&quot;)), ymax = max(CD166_19_S2$xrf$depth)/-1, ymin = min(CD166_19_S2$xrf$depth)/-1, xmin = max(as.numeric(colnames(CD166_19_S2$image$image))), xmax = max(as.numeric(colnames(CD166_19_S2$image$image)))*2 ) + annotation_custom(rasterGrob(CD166_19_S3$image$image, width = unit(1, &quot;npc&quot;), height = unit(1, &quot;npc&quot;)), ymax = max(CD166_19_S3$xrf$depth)/-1, ymin = min(CD166_19_S3$xrf$depth)/-1, xmin = min(as.numeric(colnames(CD166_19_S3$image$image))), xmax = max(as.numeric(colnames(CD166_19_S3$image$image))) ) Note that omitting the line coord_fixed(ratio = 1) allows the aspect ratio of the plot to be reshaped. Although usually it is desirable to keep the plot in the correct shape, sometimes for very long sequences the images end up so narrow as to be quite useless, so the x-axis can be stretched to accentuate the features in it. The example below simply omits the coord_fixed() line from the previous example. For lengthy sequences, the repetition of the annotation_custom() block of code is inefficient, but it can be easily placed inside a function. The functions ggItraxImage() and ggItraxRadio() shown below allow this, but they might need small modifications depending on your data structure and the desired output. ggItraxImage &lt;- function(section, xposition){ if(xposition == &quot;l&quot;){ annotation_custom(rasterGrob(section$image$image, width = unit(1, &quot;npc&quot;), height = unit(1, &quot;npc&quot;)), ymax = max(section$xrf$depth)/-1, ymin = min(section$xrf$depth)/-1, xmin = min(as.numeric(colnames(section$image$image))), xmax = max(as.numeric(colnames(section$image$image))) )} else if(xposition == &quot;r&quot;){ annotation_custom(rasterGrob(section$image$image, width = unit(1, &quot;npc&quot;), height = unit(1, &quot;npc&quot;)), ymax = max(section$xrf$depth)/-1, ymin = min(section$xrf$depth)/-1, xmin = max(as.numeric(colnames(section$image$image))), xmax = max(as.numeric(colnames(section$image$image)))*2) } else{stop(&quot;Specify `l`eft or `r`ight for the image.&quot;)} } ggItraxRadio &lt;- function(section, xposition){ if(xposition == &quot;l&quot;){ annotation_custom(rasterGrob(section$radiograph$image, width = unit(1, &quot;npc&quot;), height = unit(1, &quot;npc&quot;)), ymin = min(section$xrf$depth)/-1, ymax = max(section$xrf$depth)/-1, xmin = mean(as.numeric(colnames(section$image$image))) - mean(as.numeric(colnames(section$radiograph$image))), xmax = mean(as.numeric(colnames(section$image$image))) + mean(as.numeric(colnames(section$radiograph$image))) ) } else if(xposition == &quot;r&quot;){ annotation_custom(rasterGrob(section$radiograph$image, width = unit(1, &quot;npc&quot;), height = unit(1, &quot;npc&quot;)), ymin = min(section$xrf$depth)/-1, ymax = max(section$xrf$depth)/-1, xmin = max(as.numeric(colnames(section$image$image))) + mean(as.numeric(colnames(section$image$image))) - mean(as.numeric(colnames(section$radiograph$image))), xmax = max(as.numeric(colnames(section$image$image))) + mean(as.numeric(colnames(section$image$image))) + mean(as.numeric(colnames(section$radiograph$image))) ) } else(stop(&quot;Specify `l`eft or `r`ight for the radiograph.&quot;)) } Recall that ggplot objects can be saved and recalled rather than immediately printed. Let us take the plot generated in a previous example, but assign it using imagePlot &lt;- ggplot() + ..., whilst also using our newly created functions to shorten the code. imagePlot &lt;- ggplot() + scale_y_reverse(limits = rev(range(CD166_19_xrf$depth))) + scale_x_continuous(breaks = round(c(0, max(as.numeric(colnames(CD166_19_S1$image$image)))*2)), limits = c(0, max(as.numeric(colnames(CD166_19_S1$image$image)))*2)) + labs(y = &quot;Depth [mm]&quot;, x = &quot;[mm]&quot;) + ggItraxImage(CD166_19_S1, &quot;l&quot;) + ggItraxImage(CD166_19_S2, &quot;r&quot;) + ggItraxImage(CD166_19_S3, &quot;l&quot;) The radiographs can be added afterwards without starting from scratch. We can also modify plot parameters, for example, the x-axis labels can be removed. imagePlot &lt;- imagePlot + ggItraxRadio(CD166_19_S1, &quot;l&quot;) + ggItraxRadio(CD166_19_S2, &quot;r&quot;) + ggItraxRadio(CD166_19_S3, &quot;l&quot;) + theme(axis.title.x=element_blank(), axis.text.x=element_blank(), axis.ticks.x=element_blank()) print(imagePlot + coord_fixed(ratio = 1)) Finally, it may be necessary to plot the images horizontally rather than the more traditional vertical plots often used in down-core data presentation. aperm() and t() may be used for the optical and radiographic images respectively to transpose the x- and y-coordinates, but the images will need to be mirrored (using []) so that the tops of the sections are on the right-hand side and the graphs run from oldest to youngest when read left-to-right, in the generally accepted order of these types of graph. For the example below, this is achieved in the first and second lines for the optical and radiographic images respectively. mySidewaysImage &lt;- aperm(CD166_19_S1$image$image, c(2,1,3))[, dim(aperm(CD166_19_S1$image$image, c(2,1,3)))[2]:1, ] mySidewaysRadio &lt;- t(CD166_19_S1$radiograph$image)[, dim(t(CD166_19_S1$radiograph$image))[2]:1] ggplot() + scale_x_reverse(limits = rev(range(as.numeric(colnames(mySidewaysImage))))) + scale_y_continuous(breaks = round(range(as.numeric(row.names(mySidewaysImage))), 0), limits = range(as.numeric(row.names(mySidewaysImage)))) + labs(y = &quot;[mm]&quot;, x = &quot;Position [mm]&quot;) + annotation_custom(rasterGrob(mySidewaysImage, width = unit(1, &quot;npc&quot;), height = unit(1, &quot;npc&quot;)), xmax = max(as.numeric(colnames(mySidewaysImage)))/-1, xmin = min(as.numeric(colnames(mySidewaysImage)))/-1, ymin = min(as.numeric(row.names(mySidewaysImage))), ymax = max(as.numeric(row.names(mySidewaysImage))) ) + annotation_custom(rasterGrob(mySidewaysRadio, width = unit(1, &quot;npc&quot;), height = unit(1, &quot;npc&quot;)), xmax = max(as.numeric(colnames(mySidewaysRadio)))/-1, xmin = min(as.numeric(colnames(mySidewaysRadio)))/-1, ymin = mean(as.numeric(row.names(mySidewaysImage))) - mean(as.numeric(row.names(mySidewaysRadio))), ymax = mean(as.numeric(row.names(mySidewaysImage))) + mean(as.numeric(row.names(mySidewaysRadio))) ) + coord_fixed(ratio = 1) "],["combining-images-with-xrf-data.html", "4.3 Combining Images with XRF Data", " 4.3 Combining Images with XRF Data It is possible to combine different plots. For example, it is often desirable to plot the XRF data alongside the core imagery. By not passing coord_fixed(ratio = 1) to the image plot, its relative width can be controlled via widths =, which makes for a visually pleasing plot. Note also that the y-axis labels have been removed from all but one of the plots to avoid duplication. egg::ggarrange(imagePlot + theme_paleo(), xrfStrat + theme(axis.title.y = element_blank(), axis.text.y = element_blank(), axis.ticks.y = element_blank()), ncol = 2, widths = c(1, 4) # these are relative. For c(1, 5), the first plot will be 1/5th the width of the second. ) "],["transforming-data.html", "Chapter 5 Transforming Data", " Chapter 5 Transforming Data The XRF data typically reported from the Itrax core scanner come from the spectral processing software Q-Spec. The output is usually in the form of an intensity, which is a dimensionless metric derived from the size of the spectral peak for a particular element, above the background Bremsstrahlung radiation, sometimes normalised for the tube current and/or counting time. "],["ratios-and-normalisation.html", "5.1 Ratios and Normalisation", " 5.1 Ratios and Normalisation These data are compositional, and represent the changes in the relative proportions of all components of the matrix, measured and un-measured. As such it is likely the data will need transforming for certain types of multivariate analysis. As previously mentioned, these data are dimensionless, and as such do not represent a quantity, but are directly related to the absolute amount of a particular element in the matrix. It is often the case that ratios of elements are used to represent changes in composition — this is sometimes referred to as normalisation, or normalising one element against another. For example, particular element ratios can be used to make some environmental inference. Log-ratios are usually preferable here as they are resilient to matrix effects. In this context, the Ca/Ti log-ratio is used to infer a relative measure of productivity (that it, it can identify the hemipelagic sediments in the sequence). Note the use of abs() has the effect of ensuring that switching the denominator and numerator has no effect. CD166_19_xrf %&gt;% mutate(`log(Fe/Ca)` = abs(log(Fe/Ca))) %&gt;% filter(qc == TRUE) %&gt;% ggplot(aes(x = depth, y = `log(Fe/Ca)`)) + geom_line() + scale_x_reverse() Sometimes z-scores are useful to use when units don’t have meaningful units. They scale the data in standard deviations from their mean, so give a useful quantity for the magnitude of change. For example: CD166_19_xrf %&gt;% mutate(`log(Fe/Ca)` = abs(log(Fe/Ca))) %&gt;% mutate(zscore = (`log(Fe/Ca)` - mean(`log(Fe/Ca)`, na.rm = TRUE))/sd(`log(Fe/Ca)`, na.rm = TRUE)) %&gt;% filter(qc == TRUE) %&gt;% ggplot(aes(x = depth, y = zscore)) + geom_line() + scale_x_reverse() + ylab(&quot;log(Fe/Ca) [z-score]&quot;) It is trivial to calculate element ratios, to the extent that these can often simply be calculated where they are required rather than saving them to memory. For example, if a plot of the Compton divided by the Rayleigh scatter was desired, there is no need to save the computed value to a new variable (e.g. coh_inc &lt;- df$Mo.coh/df$Mo.inc) — simply define the calculation during plotting. To calculate ratios for all elements at once, use mutate(across(any_ofelementsList)), where elementsList is a list of chemical elements extracted from data(PeriodicTable). CD166_19_xrf %&gt;% mutate(across(any_of(elementsList)) /`Mo inc`) This can be integrated into the work flow from the data tidying chapter, for example: CD166_19_xrf %&gt;% # transform mutate(across(any_of(elementsList)) /`Mo inc`) %&gt;% # identify acceptable observations filter(validity == TRUE) %&gt;% # identify acceptable variables select(any_of(myElements), depth, label) %&gt;% # pivot tidyr::pivot_longer(!c(&quot;depth&quot;, &quot;label&quot;), names_to = &quot;elements&quot;, values_to = &quot;peakarea&quot;) %&gt;% mutate(elements = factor(elements, levels = c(elementsList, &quot;coh/inc&quot;))) %&gt;% # plot ggplot(aes(x = peakarea, y = depth)) + tidypaleo::geom_lineh(aes(color = label)) + scale_y_reverse() + scale_x_continuous(n.breaks = 2) + facet_geochem_gridh(vars(elements)) + labs(x = &quot;peak area / Mo. inc.&quot;, y = &quot;Depth [mm]&quot;) + tidypaleo::theme_paleo() + theme(legend.position = &quot;none&quot;) Note that where zero values are encountered in divisions, dividing by zero will lead to Inf values. This can cause issues with plotting data, and these values should be cleaned into NA before plotting. For example: ggarrange( CD166_19_xrf %&gt;% mutate(`Fe/Sc` = Fe/Sc) %&gt;% ggplot(aes(x = depth, y = `Fe/Sc`)) + geom_line() + scale_x_reverse() + ggtitle(&quot;w/ Inf.&quot;), CD166_19_xrf %&gt;% mutate(`Fe/Sc` = Fe/Sc) %&gt;% mutate_if(is.numeric, list(~na_if(., Inf))) %&gt;% # convert all Inf to NA ggplot(aes(x = depth, y = `Fe/Sc`)) + geom_line() + scale_x_reverse() + ggtitle(&quot;na_if(., Inf)&quot;) ) "],["running-mean-and-other-window-functions.html", "5.2 Running Mean and Other Window Functions", " 5.2 Running Mean and Other Window Functions Where a signal is noisy but appears to exhibit some signal it may be appropriate to use a running mean to “smooth” the signal. However, considerable caution should be exercised in the use of this tool. It is rare for an analysis to be genuinely improved by the use of running means, although it can artificially improve statistics for some tests. When visualising data using a running mean the original, unmodified data should always be shown alongside to avoid any misunderstanding. This method can be used for any suitable window function (e.g. min(), max(), range() and sd.) CD166_19_xrf %&gt;% # uses a 50 point running mean (50 mm for this data); 25 before, 25 after mutate(across(any_of(elementsList), function(x){unlist(slider::slide(x, mean, .before = 25, .after = 25))} ) ) %&gt;% ggplot(mapping = aes(x = depth, y = Ca)) + geom_line(data = CD166_19_xrf, col = &quot;grey80&quot;) + geom_line() + scale_x_reverse() + theme_paleo() To plot the running means in a stratigraphic diagram, the smoothed data has to be labelled and combined with the original data so it can be faceted. # make the xrf plot with running means full_join(y = CD166_19_xrf %&gt;% as_tibble() %&gt;% # uses a 50 point running mean (50 mm for this data); 25 before, 25 after mutate(across(any_of(c(elementsList)), function(x){unlist(slider::slide(x, mean, .before = 25, .after = 25))} ) ) %&gt;% mutate(type = &quot;mean&quot;), x = CD166_19_xrf %&gt;% as_tibble() %&gt;% mutate(type = &quot;raw&quot;) ) %&gt;% filter(validity == TRUE) %&gt;% select(Fe, Ti, Cu, Pb, Si, MSE, Mn, depth, label, type) %&gt;% tidyr::pivot_longer(!c(&quot;depth&quot;, &quot;label&quot;, &quot;type&quot;), names_to = &quot;elements&quot;, values_to = &quot;peakarea&quot;) %&gt;% tidyr::drop_na() %&gt;% mutate(elements = factor(elements, levels = c(&quot;MSE&quot;, elementsList))) %&gt;% mutate(label = as_factor(label), type = as_factor(type) ) %&gt;% glimpse() %&gt;% ggplot(aes(x = peakarea, y = depth)) + tidypaleo::geom_lineh(aes(group = type, colour = label, alpha = type)) + scale_alpha_manual(values = c(0.1, 1)) + scale_y_reverse() + scale_x_continuous(n.breaks = 2) + facet_geochem_gridh(vars(elements)) + labs(x = &quot;peak area&quot;, y = &quot;Depth [mm]&quot;) + tidypaleo::theme_paleo() + theme(legend.position = &quot;none&quot;, axis.text.x = element_blank(), axis.ticks.x = element_blank()) ## Rows: 57,134 ## Columns: 5 ## $ depth &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2… ## $ label &lt;fct&gt; S1, S1, S1, S1, S1, S1, S1, S1, S1, S1, S1, S1, S1, S1, S1, S… ## $ type &lt;fct&gt; raw, raw, raw, raw, raw, raw, raw, raw, raw, raw, raw, raw, r… ## $ elements &lt;fct&gt; Fe, Ti, Cu, Pb, Si, MSE, Mn, Fe, Ti, Cu, Pb, Si, MSE, Mn, Fe,… ## $ peakarea &lt;dbl&gt; 19786.00, 804.00, 117.00, 22.00, 177.00, 1.26, 231.00, 35168.… "],["multivariate-methods.html", "Chapter 6 Multivariate Methods ", " Chapter 6 Multivariate Methods "],["preparing-data-for-multivariate-data-analysis.html", "6.1 Preparing Data for Multivariate Data Analysis", " 6.1 Preparing Data for Multivariate Data Analysis Where multivariate methods (cluster analysis, principal components analysis, correlation matrices) are required, it is usually necessary to transform data and treat them in a special way to avoid breaking the assumptions of a method or drawing erroneous conclusions. This is because the constant sum constraint that defines compositional data - put simply, if one element increases in proportion, an equal decrease must occur in the remaining elements that make up the composition. This is further complicated by unobserved elements, dimensionless (i.e. calibrated) data, and observations below the limit of detection. The package compositions deals with much of these issues, including any necessary transformation, and ensures that the data are treated differently by functions like princomp() and dist(), e.g. compositions::princomp.acomp() by setting a class attribute and providing modified functions. It’s worth spending some time getting familiar with the documentation for compositions, and possibly the wider literature around compositional data analysis. Several different methods are provided depending on the nature of the compositional data - in these examples, we use the acomp() but other methods are available. This package also deals with the variety of different types of zero and missing values. For these count data we deal with values that are below the limit of detection by letting the compositions package know the detection limit. For these count data, the detection limit is 1, and thus is coded as -1. This allows zeroreplace() to correctly deal with this problem. In order to correctly identify the observations to their original data source, it is necessary to use row names that uniquely identify observations. For single scans this is not usually an issue — the depth or position variable can be used. However, where a dataset is a composition of multiple cores, you may find that there are multiple observations for a particular depth where cores overlap. We don’t usually use row names when working in the tidyverse style, but because were going to be using base functions like princomp() and dist(), they may become necessary. Here we create a unique identifier in CD166_19_xrf called uid that we will use to uniquely identify observations throughout the rest of our analysis. CD166_19_xrf &lt;- CD166_19_xrf %&gt;% mutate(uid = paste0(label, &quot;_&quot;, depth)) TRUE %in% CD166_19_xrf$uid %&gt;% duplicated() ## [1] FALSE CD166_19_xrf_acomp &lt;- CD166_19_xrf %&gt;% filter(qc == TRUE) %&gt;% select(any_of(c(elementsList, &quot;uid&quot;))) %&gt;% column_to_rownames(&quot;uid&quot;) %&gt;% mutate(across(everything(), function(x){ifelse(x == 0, -1, x)})) %&gt;% acomp() head(CD166_19_xrf_acomp) ## Al Si P S Cl ## S1_1 &quot;0.0004510065&quot; &quot;0.001631931&quot; &quot;&lt;5.934295e-06&quot; &quot; 5.934295e-05&quot; &quot;0.007821401&quot; ## S1_2 &quot;0.0002804828&quot; &quot;0.001505750&quot; &quot;&lt;4.920751e-06&quot; &quot;&lt;4.920751e-06&quot; &quot;0.007445097&quot; ## S1_3 &quot;0.0003343092&quot; &quot;0.001490838&quot; &quot;&lt;4.517691e-06&quot; &quot; 1.445661e-04&quot; &quot;0.006641006&quot; ## S1_4 &quot;0.0001227370&quot; &quot;0.000972455&quot; &quot;&lt;4.720655e-06&quot; &quot; 9.913376e-05&quot; &quot;0.006193500&quot; ## S1_5 &quot;0.0002582984&quot; &quot;0.001135538&quot; &quot;&lt;4.873556e-06&quot; &quot; 1.803216e-04&quot; &quot;0.008479987&quot; ## S1_6 &quot;0.0001333485&quot; &quot;0.001713775&quot; &quot;&lt;4.938833e-06&quot; &quot; 1.382873e-04&quot; &quot;0.007783600&quot; ## Ar K Ca Sc Ti ## S1_1 &quot;0.003584314&quot; &quot;0.01522147&quot; &quot;0.6689969&quot; &quot;&lt;5.934295e-06&quot; &quot;0.009856865&quot; ## S1_2 &quot;0.002927847&quot; &quot;0.01293173&quot; &quot;0.7100004&quot; &quot; 6.889052e-05&quot; &quot;0.008886877&quot; ## S1_3 &quot;0.002285952&quot; &quot;0.01074307&quot; &quot;0.7361036&quot; &quot;&lt;4.517691e-06&quot; &quot;0.009582023&quot; ## S1_4 &quot;0.002619964&quot; &quot;0.01069228&quot; &quot;0.7231761&quot; &quot;&lt;4.720655e-06&quot; &quot;0.009587651&quot; ## S1_5 &quot;0.002880271&quot; &quot;0.01436237&quot; &quot;0.6622139&quot; &quot; 5.165969e-04&quot; &quot;0.008899113&quot; ## S1_6 &quot;0.002489172&quot; &quot;0.01570055&quot; &quot;0.6528297&quot; &quot; 6.914366e-05&quot; &quot;0.009497375&quot; ## V Cr Mn Fe Ni ## S1_1 &quot;3.026491e-04&quot; &quot;0.001531048&quot; &quot;0.003014622&quot; &quot;0.2086973&quot; &quot;0.0007299183&quot; ## S1_2 &quot;4.330261e-04&quot; &quot;0.001604165&quot; &quot;0.002750700&quot; &quot;0.1795779&quot; &quot;0.0005117581&quot; ## S1_3 &quot;9.938921e-05&quot; &quot;0.001359825&quot; &quot;0.002182045&quot; &quot;0.1624200&quot; &quot;0.0006053706&quot; ## S1_4 &quot;3.540491e-04&quot; &quot;0.001444520&quot; &quot;0.002289518&quot; &quot;0.1676399&quot; &quot;0.0006797744&quot; ## S1_5 &quot;4.337464e-04&quot; &quot;0.002144364&quot; &quot;0.003596684&quot; &quot;0.2159131&quot; &quot;0.0008090102&quot; ## S1_6 &quot;5.284551e-04&quot; &quot;0.001980472&quot; &quot;0.003911555&quot; &quot;0.2236452&quot; &quot;0.0007457637&quot; ## Cu Zn Ga Ge Br ## S1_1 &quot;0.0016437998&quot; &quot;0.001216531&quot; &quot; 2.373718e-05&quot; &quot;&lt;5.934295e-06&quot; &quot;0.002913739&quot; ## S1_2 &quot;0.0011317728&quot; &quot;0.001318761&quot; &quot; 2.361961e-04&quot; &quot; 1.968301e-04&quot; &quot;0.002977055&quot; ## S1_3 &quot;0.0006776537&quot; &quot;0.000555676&quot; &quot;&lt;4.517691e-06&quot; &quot; 1.942607e-04&quot; &quot;0.002484730&quot; ## S1_4 &quot;0.0009724550&quot; &quot;0.001128237&quot; &quot;&lt;4.720655e-06&quot; &quot;&lt;4.720655e-06&quot; &quot;0.002874879&quot; ## S1_5 &quot;0.0013451013&quot; &quot;0.001169653&quot; &quot;&lt;4.873556e-06&quot; &quot;&lt;4.873556e-06&quot; &quot;0.003211673&quot; ## S1_6 &quot;0.0012347081&quot; &quot;0.001452017&quot; &quot; 1.975533e-04&quot; &quot; 2.370640e-04&quot; &quot;0.003501632&quot; ## Rb Sr Y Zr Pd ## S1_1 &quot;0.0019523832&quot; &quot;0.04929026&quot; &quot;8.901443e-05&quot; &quot;0.0015132453&quot; &quot;0.0001839632&quot; ## S1_2 &quot;0.0003739771&quot; &quot;0.04517742&quot; &quot;6.889052e-04&quot; &quot;0.0015844819&quot; &quot;0.0003838186&quot; ## S1_3 &quot;0.0013327189&quot; &quot;0.04356861&quot; &quot;5.376053e-04&quot; &quot;0.0012649536&quot; &quot;0.0003388268&quot; ## S1_4 &quot;0.0014775651&quot; &quot;0.04692331&quot; &quot;5.051101e-04&quot; &quot;0.0022423112&quot; &quot;0.0003115632&quot; ## S1_5 &quot;0.0014815609&quot; &quot;0.05013427&quot; &quot;8.772400e-04&quot; &quot;0.0007797689&quot; &quot;0.0001510802&quot; ## S1_6 &quot;0.0015705488&quot; &quot;0.04897840&quot; &quot;5.235163e-04&quot; &quot;0.0015409158&quot; &quot;0.0003555959&quot; ## Cd I Cs Ba ## S1_1 &quot;7.714584e-05&quot; &quot;0.0002255032&quot; &quot;&lt;5.934295e-06&quot; &quot;0.0003738606&quot; ## S1_2 &quot;1.673055e-04&quot; &quot;0.0002361961&quot; &quot;&lt;4.920751e-06&quot; &quot;0.0002361961&quot; ## S1_3 &quot;2.484730e-04&quot; &quot;0.0003614153&quot; &quot;&lt;4.517691e-06&quot; &quot;0.0001129423&quot; ## S1_4 &quot;1.132957e-04&quot; &quot;0.0003162839&quot; &quot;&lt;4.720655e-06&quot; &quot;0.0002832393&quot; ## S1_5 &quot;1.657009e-04&quot; &quot;0.0001120918&quot; &quot;&lt;4.873556e-06&quot; &quot;0.0006433093&quot; ## S1_6 &quot;1.382873e-04&quot; &quot;0.0002864523&quot; &quot;&lt;4.938833e-06&quot; &quot;0.0004593114&quot; ## Nd Sm Yb Ta W ## S1_1 &quot;0.0001008830&quot; &quot;2.551747e-04&quot; &quot;0.0008545385&quot; &quot;0.004682159&quot; &quot;0.011821117&quot; ## S1_2 &quot;0.0003296903&quot; &quot;2.263546e-04&quot; &quot;0.0013876519&quot; &quot;0.003818503&quot; &quot;0.009934997&quot; ## S1_3 &quot;0.0001987784&quot; &quot;1.761900e-04&quot; &quot;0.0007137952&quot; &quot;0.003175937&quot; &quot;0.009107666&quot; ## S1_4 &quot;0.0003540491&quot; &quot;3.021219e-04&quot; &quot;0.0010291028&quot; &quot;0.003748200&quot; &quot;0.010196615&quot; ## S1_5 &quot;0.0002046893&quot; &quot;1.754480e-04&quot; &quot;0.0007943896&quot; &quot;0.004478798&quot; &quot;0.011423614&quot; ## S1_6 &quot;0.0004790668&quot; &quot;5.926599e-05&quot; &quot;0.0009087452&quot; &quot;0.004207885&quot; &quot;0.011537113&quot; ## Pb Bi ## S1_1 &quot; 4.569408e-04&quot; &quot;0.0004272693&quot; ## S1_2 &quot;&lt;4.920751e-06&quot; &quot;0.0006692222&quot; ## S1_3 &quot; 5.872999e-05&quot; &quot;0.0008990206&quot; ## S1_4 &quot; 7.175396e-04&quot; &quot;0.0006325678&quot; ## S1_5 &quot; 2.631720e-04&quot; &quot;0.0007651482&quot; ## S1_6 &quot; 2.666970e-04&quot; &quot;0.0008988675&quot; ## attr(,&quot;class&quot;) ## [1] &quot;acomp&quot; Where it is necessary to keep some (or all) of the other information, this can be re-joined to the acomp() object, but only if its acomp class is removed. For example: CD166_19_xrf_acomp_meta &lt;- full_join(CD166_19_xrf_acomp %&gt;% as.data.frame() %&gt;% rownames_to_column(&quot;uid&quot;), CD166_19_xrf %&gt;% select(-any_of(elementsList)), by = &quot;uid&quot; ) %&gt;% arrange(depth, label) "],["principal-component-analysis.html", "6.2 Principal Component Analysis", " 6.2 Principal Component Analysis Principal component analysis (PCA) is a common method for exploring multivariate data. Note the use of zeroreplace() - this is because the princomp() method defined for th acomp class uses a centred-log-ratio (clr()) transformation that is intolerant to zero-values. CD166_19_xrf_acomp %&gt;% zeroreplace() %&gt;% princomp() %&gt;% biplot(xlabs = rep(&quot;.&quot;,times = nrow(CD166_19_xrf_acomp))) It is useful to plot components over depth. They can be extracted and plotted as follows: bind_rows( tibble(depth = CD166_19_xrf %&gt;% filter(qc == FALSE) %&gt;% pull(&quot;depth&quot;), Comp.1 = NA ), tibble( depth = CD166_19_xrf %&gt;% filter(qc == TRUE) %&gt;% pull(&quot;depth&quot;), Comp.1 = CD166_19_xrf_acomp %&gt;% zeroreplace() %&gt;% princomp() %&gt;% magrittr::extract2(&quot;scores&quot;) %&gt;% as_tibble() %&gt;% pull(&quot;Comp.1&quot;) )) %&gt;% arrange(depth) %&gt;% ggplot(aes(x = depth, y = Comp.1)) + geom_line() + scale_x_reverse(name = &quot;depth [mm]&quot;) ## Warning: Removed 4 rows containing missing values or values outside the scale range ## (`geom_line()`). "],["using-ordr-for-pca.html", "6.3 Using ordr for PCA", " 6.3 Using ordr for PCA The ordr package can be used to retain metadata (depths, labels, groups etc.) along with the PCA object. This is particularly useful for complex biplots using ggplot2. The following example uses the CD166_19_xrf_acomp_meta from the previous section; note that myElements is a vector of elements to include in the ordination, in this case generated in the section Noisy Data. CD166_19_xrf_acomp_meta %&gt;% ordr::ordinate(., cols = any_of(myElements), model = ~ princomp(clr(.)), augment = any_of(c(&quot;depth&quot;, &quot;label&quot;)) ) %&gt;% ordr::ggbiplot(., sec.axes = &quot;cols&quot;, scale.factor = 8) + ordr::geom_rows_point(aes(colour = depth, shape = label), alpha = 0.5) + ordr::geom_cols_vector() + ordr::geom_cols_text_radiate(aes(label = name)) + scale_color_continuous(type = &quot;viridis&quot;, trans = &quot;reverse&quot;) By embedding the depth and label data into the ordination object itself, it makes it easy to do down-core plots without having to rejoin the elemental data with the core depth and label information: CD166_19_xrf_acomp_meta %&gt;% ordr::ordinate(., cols = any_of(myElements), model = ~ princomp(clr(.)), augment = any_of(c(&quot;depth&quot;, &quot;label&quot;)) ) %&gt;% fortify() %&gt;% select(depth, label, `Comp.1`) %&gt;% tidyr::drop_na() %&gt;% ggplot(aes(x = `Comp.1`, y = depth, colour = label)) + geom_lineh() + scale_y_reverse(name = &quot;Depth [mm]&quot;) "],["unconstrained-cluster-analysis.html", "6.4 Unconstrained Cluster Analysis", " 6.4 Unconstrained Cluster Analysis Unconstrained cluster analysis will group each measurement by similarity, returning an arbitrary number of groups as defined by the function. This is useful in identifying recurring compositional units, or identifying candidate samples for quantitative analysis. left_join(CD166_19_xrf, tibble(uid = CD166_19_xrf_acomp %&gt;% rownames(), group = dist(CD166_19_xrf_acomp) %&gt;% hclust(method = &quot;ward.D2&quot;) %&gt;% cutree(k = 5) %&gt;% as.factor() ), by = &quot;uid&quot; ) %&gt;% ggplot() + geom_tile(aes(x = depth, y = 1, fill = group)) + scale_x_reverse() + theme(axis.title.y = element_blank(), axis.text.y = element_blank(), axis.ticks.y = element_blank()) "],["constrained-cluster-analysis.html", "6.5 Constrained Cluster Analysis", " 6.5 Constrained Cluster Analysis Constrained cluster analysis is helpful in identifying units of deposition in the sedimentary sequence. It forces each sample to remain in sequence, thus there are no recurring units along the stratigraphy. bind_rows(tibble(depth = filter(CD166_19_xrf, qc == TRUE) %&gt;% pull(depth), group = rioja::chclust(dist(CD166_19_xrf_acomp)) %&gt;% cutree(k = 5) %&gt;% as.factor()), CD166_19_xrf %&gt;% filter(qc == FALSE) %&gt;% mutate(group = NA) %&gt;% select(depth, group)) %&gt;% arrange(depth) %&gt;% ggplot() + geom_tile(aes(x = depth, y = 1, fill = group)) + scale_x_reverse() + theme(axis.title.y = element_blank(), axis.text.y = element_blank(), axis.ticks.y = element_blank()) "],["calibrating-data.html", "Chapter 7 Calibrating Data", " Chapter 7 Calibrating Data Although much analysis can be done using the peak-area data from the Itrax, it may be desirable to attempt to fully quantify the data. Because of the variability typically encountered in palaeoenvironmental work a “fundamental parameters” type approach is not often possible, although there is some functionality for this in the Q-Spec software on the instrument. Rather, an empirical approach is usually required, whereby some subset of the material is sampled and a relationship (calibration curve) is calculated for analytes of interest. An empirical approach also has the benefit of being able to estimate the uncertainties of the calibration. "],["suitable-methods.html", "7.1 Suitable Methods", " 7.1 Suitable Methods There are a number of published methods for quantifying elemental concentrations from ED-XRF data, and the area is of continued research interest. Most methods fall into the category of bivariate linear models of the relationship between ED-XRF peak area, and independently derived element concentrations (e.g. sub-samples analysed by ICP-MS). However, there are also more complex approaches, like Lee Drake’s modern version of the Lucas-Tooth and Price (1961) algorithm, “CloudCal”. "],["selecting-samples.html", "7.2 Selecting Samples", " 7.2 Selecting Samples Unless you plan on sub-sampling the entire core, perhaps contiguously, you will need to decide where best to take your samples for best model coverage. If you are only interested in a single element, this is trivial - simply ensure your sampling strategy encompasses a wide range of samples. However, where you are attempting to calibrate for many elements, developing an optimal sampling strategy manually can be difficult. The function below illustrates the use of cluster analysis in optimising the sampling regime. Because it is unrealistic to plan on accurately sub-sampling at the high-resolution of the data (in this case, 1 mm), we use itrax_reduce() to re sample the data to a lower resolution, in this case, 10 mm. The following itrax_section() performs an unconstrained cluster analysis, and also reports the central sample in each cluster. This could be considered the most representative of each cluster, and thus the suitable location for sampling. In the graph below, the black ticks created by geom_rug() are the suggested sampling locations. CD166_19_xrf %&gt;% filter(qc == TRUE) %&gt;% itrax_reduce(by = 10) %&gt;% mutate(uid = 1:length(uid)) %&gt;% # we have to spoof uid here drop_na(&quot;depth&quot;) %&gt;% itrax_section(divisions = 60, elementsonly = c(&quot;Al&quot;, &quot;Si&quot;, &quot;K&quot; , &quot;Ca&quot;, &quot;Ti&quot;, &quot;V&quot; , &quot;Cr&quot;, &quot;Mn&quot;, &quot;Fe&quot;, &quot;Ni&quot;, &quot;Cu&quot;, &quot;Zn&quot;,&quot;Sr&quot; , &quot;Y&quot; , &quot;Zr&quot;, &quot;Ba&quot;) ) %&gt;% ggplot(mapping = aes(x = depth, y = 1, fill = group)) + geom_tile(width = 10) + scale_x_reverse() + geom_rug(sides = &quot;b&quot;, data = . %&gt;% filter(calib_sample == TRUE)) + theme(axis.title.y = element_blank(), axis.text.y = element_blank(), axis.ticks.y = element_blank(), legend.position = &quot;none&quot;) To return a core sampling plan, some conversions must be performed: CD166_19_xrf %&gt;% filter(qc == TRUE) %&gt;% itrax_reduce(breaks_lower = seq(from = 0, to = 4209-10, by = 10), breaks_upper = seq(from = 0+10, to = 4209, by = 10)) %&gt;% # this needs a method for simply adding all the character vectors from each chunk together into a vector... mutate(uid = 1:length(uid)) %&gt;% mutate(breaks_lower = seq(from = 0, to = 4209-10, by = 10), breaks_upper = seq(from = 0+10, to = 4209, by = 10)) %&gt;% # we have to spoof uid here drop_na(&quot;depth&quot;) %&gt;% itrax_section(divisions = 60, elementsonly = c(&quot;Al&quot;, &quot;Si&quot;, &quot;K&quot; , &quot;Ca&quot;, &quot;Ti&quot;, &quot;V&quot; , &quot;Cr&quot;, &quot;Mn&quot;, &quot;Fe&quot;, &quot;Ni&quot;, &quot;Cu&quot;, &quot;Zn&quot;,&quot;Sr&quot; , &quot;Y&quot; , &quot;Zr&quot;, &quot;Ba&quot;) ) %&gt;% filter(calib_sample == TRUE) %&gt;% select(group, depth, breaks_lower, breaks_upper, everything()) %&gt;% arrange(depth) %&gt;% select(breaks_lower, breaks_upper) %&gt;% mutate(core = if_else(breaks_upper &gt;= min(CD166_19_S1$xrf$depth) &amp; breaks_upper &lt;= max(CD166_19_S1$xrf$depth), &quot;c1&quot;, &quot;&quot;)) %&gt;% mutate(core = if_else(breaks_upper &gt;= min(CD166_19_S2$xrf$depth) &amp; breaks_upper &lt;= max(CD166_19_S2$xrf$depth), &quot;c2&quot;, core)) %&gt;% mutate(core = if_else(breaks_upper &gt;= min(CD166_19_S3$xrf$depth) &amp; breaks_upper &lt;= max(CD166_19_S3$xrf$depth), &quot;c3&quot;, core)) %&gt;% mutate(core_position_lower = ifelse(core == &quot;c1&quot;, breaks_lower-min(CD166_19_S1$xrf$depth), NA)) %&gt;% mutate(core_position_lower = ifelse(core == &quot;c2&quot;, breaks_lower-min(CD166_19_S2$xrf$depth), core_position_lower)) %&gt;% mutate(core_position_lower = ifelse(core == &quot;c3&quot;, breaks_lower-min(CD166_19_S3$xrf$depth), core_position_lower)) %&gt;% mutate(core_position_upper = ifelse(core == &quot;c1&quot;, breaks_upper-min(CD166_19_S1$xrf$depth), NA)) %&gt;% mutate(core_position_upper = ifelse(core == &quot;c2&quot;, breaks_upper-min(CD166_19_S2$xrf$depth), core_position_upper)) %&gt;% mutate(core_position_upper = ifelse(core == &quot;c3&quot;, breaks_upper-min(CD166_19_S3$xrf$depth), core_position_upper)) %&gt;% select(core, core_position_lower, core_position_upper) ## # A tibble: 60 × 3 ## core core_position_lower core_position_upper ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 c1 80 90 ## 2 c1 330 340 ## 3 c1 430 440 ## 4 c1 440 450 ## 5 c1 460 470 ## 6 c1 670 680 ## 7 c1 770 780 ## 8 c1 840 850 ## 9 c1 930 940 ## 10 c1 1020 1030 ## # ℹ 50 more rows "],["preparing-data.html", "7.3 Preparing Data", " 7.3 Preparing Data In order to perform empirical calibration it is necessary to have some quantitative data that relates directly to the Itrax XRF data. The quantitative data is unlikely to be at the same resolution as the Itrax XRF data. For example, it is typical to scan cores at between 0.2 and 1 mm, but typical to sub-sample for conventional XRF or ICP at between 5 - 10 mm. In addition, where Itrax scans are contiguous, sub-samples may not be, for example a 10 mm sub-sample might be taken every 80 mm. In this example we have sub-sampled and analysed the 60 samples defined in the previous section. Here they have been freeze-dried, lightly milled using agate stoneware, pressed into loose powder pellets and analysed with a Niton XL3t GOLDD+ ED-XRF using the “TestAllGeo” mode. load(&quot;calibration_samples/CD166_hhxrf.RData&quot;) glimpse(hhxrf) ## Rows: 60 ## Columns: 38 ## $ top &lt;dbl&gt; 330, 430, 440, 460, 670, 770, 800, 840, 930, 1020, 1090, 1110… ## $ bot &lt;dbl&gt; 340, 440, 450, 470, 680, 780, 810, 850, 940, 1030, 1100, 1120… ## $ SampleID &lt;chr&gt; &quot;cd166-33&quot;, &quot;cd166-43&quot;, &quot;cd166-44&quot;, &quot;cd166-46&quot;, &quot;cd166-67&quot;, &quot;… ## $ Mg (err) NA(NA), NA(NA), NA(NA), NA(NA), 30000(10000), NA(NA), NA(NA),… ## $ Al (err) 14000(2000), 22000(2000), 25000(2000), 28000(3000), 19000(200… ## $ Si (err) 45000(1000), 67000(1000), 75000(1000), 93000(1000), 56000(100… ## $ P (err) NA(NA), NA(NA), NA(NA), NA(NA), NA(NA), NA(NA), NA(NA), 300(2… ## $ S (err) 890(80), 1200(80), 1110(80), 1600(90), 1020(80), 1160(80), 11… ## $ Cl (err) 11100(100), 5210(90), 13100(100), 10700(100), 5870(90), 3180(… ## $ K (err) 4700(200), 8200(200), 8900(200), 9000(200), 5700(200), 6100(2… ## $ Ca (err) 2.48(2)e5, 2.14(1)e5, 1.97(1)e5, 1.90(1)e5, 2.32(1)e5, 2.24(1… ## $ Sc (err) NA(NA), NA(NA), 260(90), NA(NA), NA(NA), NA(NA), 300(100), NA… ## $ Ti (err) 900(100), 1360(90), 990(40), 1540(90), 1400(100), 2000(100), … ## $ V (err) 60(20), 70(20), 60(20), 50(20), 70(20), 70(30), 70(20), 80(30… ## $ Cr (err) 30(20), NA(NA), NA(NA), 30(20), 30(20), NA(NA), NA(NA), 50(20… ## $ Mn (err) 250(40), 320(30), 280(30), 270(40), 240(30), 420(40), 380(30)… ## $ Fe (err) 9200(200), 11900(100), 12400(100), 12800(200), 10400(100), 13… ## $ Co (err) NA(NA), NA(NA), NA(NA), NA(NA), NA(NA), NA(NA), NA(NA), 50(30… ## $ Ni (err) NA(NA), NA(NA), NA(NA), 50(20), NA(NA), 30(20), 40(10), NA(NA… ## $ Cu (err) 30(10), 28(8), 28(6), 21(9), 25(7), 37(8), 38(7), 33(8), 50(1… ## $ Zn (err) 51(8), 67(6), 60(5), 33(6), 21(4), 25(5), 23(4), 21(5), 45(8)… ## $ As (err) NA(NA), NA(NA), 16(5), NA(NA), NA(NA), 4(2), 6(3), NA(NA), 6(… ## $ Se (err) NA(NA), NA(NA), NA(NA), NA(NA), NA(NA), NA(NA), 2(1), NA(NA),… ## $ Rb (err) 20(2), 28(2), 30(1), 29(2), 18(1), 19(1), 17(1), 17(1), 14(2)… ## $ Sr (err) 950(10), 824(7), 720(5), 646(7), 862(6), 854(7), 794(5), 845(… ## $ Zr (err) 42(4), 48(3), 43(2), 53(3), 48(2), 54(3), 52(2), 58(3), 129(4… ## $ Nb (err) 6(1), 5(1), 5.2(8), 4(1), 5.4(9), 8(1), 6.9(9), 9(1), 20(10),… ## $ Mo (err) 6(2), 3(2), 3(1), NA(NA), 4(1), 3(2), 3(1), 3(1), 5(2), 3(2),… ## $ Ag (err) 5(3), 5(3), NA(NA), NA(NA), NA(NA), 6(3), NA(NA), NA(NA), NA(… ## $ Cd (err) 10(6), NA(NA), NA(NA), NA(NA), NA(NA), NA(NA), NA(NA), NA(NA)… ## $ Sn (err) 13(7), NA(NA), NA(NA), NA(NA), NA(NA), NA(NA), NA(NA), NA(NA)… ## $ Ba (err) 170(30), 130(30), 150(30), 220(30), 150(30), 140(30), 180(30)… ## $ W (err) NA(NA), NA(NA), NA(NA), NA(NA), 20(10), 30(20), NA(NA), NA(NA… ## $ Au (err) NA(NA), NA(NA), NA(NA), NA(NA), NA(NA), NA(NA), NA(NA), 4(3),… ## $ Hg (err) NA(NA), NA(NA), NA(NA), NA(NA), NA(NA), NA(NA), NA(NA), NA(NA… ## $ Pb (err) 6(3), 13(3), 227(6), 10(3), 6(2), 10(2), 47(3), 10(2), 8(3), … ## $ Th (err) 5(2), 4(2), 10(4), 4(2), 4(1), 13(5), 12(4), 13(5), NA(NA), 4… ## $ U (err) NA(NA), NA(NA), NA(NA), NA(NA), NA(NA), NA(NA), NA(NA), NA(NA… The function itraxR::itrax_reduce() can be used to reduce the Itrax data to match the resolution of some other data. The example below uses the same positions of the conventional XRF analyses of the data to summarise it. And here is the use of itrax_reduce() to reduce our Itrax XRF data (CD166_19_xrf), using the shape of the conventional XRF data (in xrf). Note the requirement to remove text based columns (in this case file and label) from the data before this step is performed - if the reducing function cannot handle a data type (e.g. passing characters to mean()), errors will occur. If we wanted to add the standard deviation alongside the mean for each chunk, this can be done my modifying the default reducing function (mean()) to sd(), for example: xrf &lt;- CD166_19_xrf %&gt;% filter(qc == TRUE) %&gt;% select(-c(label, filename, uid)) %&gt;% itrax_reduce(names = hhxrf$SampleID, breaks_lower = hhxrf$top, breaks_upper = hhxrf$bot) %&gt;% rename(SampleID = resample_names) %&gt;% mutate(top = hhxrf$top, bot = hhxrf$bot) glimpse(xrf) ## Rows: 60 ## Columns: 58 ## $ SampleID &lt;chr&gt; &quot;cd166-33&quot;, &quot;cd166-43&quot;, &quot;cd166-44&quot;, &quot;cd166-46&quot;, &quot;cd16… ## $ depth &lt;dbl&gt; 334.5, 434.5, 444.5, 464.5, 674.5, 774.5, 804.5, 844.… ## $ MSE &lt;dbl&gt; 1.707, 1.585, 1.562, 1.499, 1.644, 1.581, 1.669, 1.65… ## $ cps &lt;dbl&gt; 46735.9, 38494.0, 36829.5, 34929.5, 44762.0, 45024.2,… ## $ validity &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ Al &lt;dbl&gt; 79.5, 71.5, 78.1, 57.0, 77.0, 87.5, 71.1, 75.0, 71.0,… ## $ Si &lt;dbl&gt; 347.4, 929.3, 1338.6, 1043.9, 394.1, 424.5, 375.0, 41… ## $ P &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ S &lt;dbl&gt; 54.5, 7.6, 6.0, 10.6, 89.2, 28.3, 17.8, 20.1, 12.1, 2… ## $ Cl &lt;dbl&gt; 1839.4, 755.2, 492.3, 677.4, 1781.5, 1499.4, 1565.9, … ## $ Ar &lt;dbl&gt; 651.4, 543.7, 446.4, 398.8, 530.5, 503.7, 525.3, 552.… ## $ K &lt;dbl&gt; 2539.8, 3059.0, 3143.8, 3182.9, 2910.6, 2885.7, 2597.… ## $ Ca &lt;dbl&gt; 234113.7, 157804.5, 97585.4, 115428.4, 202283.5, 2034… ## $ Sc &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 10.1, 0.0, 0.0, 0.0, 0.0, 4.9, 9.… ## $ Ti &lt;dbl&gt; 1723.8, 1936.0, 4692.8, 1964.6, 2398.6, 2549.2, 2194.… ## $ V &lt;dbl&gt; 15.5, 59.0, 116.6, 77.9, 26.2, 56.6, 39.0, 52.1, 82.2… ## $ Cr &lt;dbl&gt; 309.6, 260.2, 329.8, 278.6, 343.1, 314.9, 281.7, 347.… ## $ Mn &lt;dbl&gt; 743.8, 580.3, 794.1, 796.1, 597.4, 910.3, 738.9, 723.… ## $ Fe &lt;dbl&gt; 32624.9, 43792.7, 74938.8, 44888.9, 40525.2, 45688.0,… ## $ Ni &lt;dbl&gt; 98.3, 65.7, 77.2, 92.3, 86.3, 108.4, 123.6, 114.2, 17… ## $ Cu &lt;dbl&gt; 191.6, 90.3, 94.3, 80.6, 171.4, 202.8, 172.6, 185.2, … ## $ Zn &lt;dbl&gt; 145.4, 163.5, 171.5, 177.8, 161.8, 179.2, 142.6, 176.… ## $ Ga &lt;dbl&gt; 9.5, 80.8, 110.2, 72.1, 19.6, 23.1, 10.0, 27.2, 13.9,… ## $ Ge &lt;dbl&gt; 73.6, 103.1, 107.7, 104.2, 95.5, 105.4, 68.3, 115.3, … ## $ Br &lt;dbl&gt; 494.0, 287.1, 204.8, 260.3, 483.4, 431.7, 422.9, 409.… ## $ Rb &lt;dbl&gt; 196.6, 295.8, 404.3, 375.9, 201.0, 221.6, 182.8, 188.… ## $ Sr &lt;dbl&gt; 11406.2, 6618.7, 4505.5, 5548.2, 10746.5, 10305.0, 10… ## $ Y &lt;dbl&gt; 109.9, 97.8, 108.0, 52.5, 156.0, 126.7, 159.1, 114.1,… ## $ Zr &lt;dbl&gt; 208.0, 413.4, 2082.7, 456.9, 257.6, 320.7, 294.2, 301… ## $ Pd &lt;dbl&gt; 79.9, 43.0, 25.8, 31.4, 72.0, 63.6, 64.2, 61.6, 41.7,… ## $ Cd &lt;dbl&gt; 111.5, 88.6, 44.8, 56.3, 83.5, 88.5, 108.0, 109.8, 72… ## $ I &lt;dbl&gt; 57.1, 54.8, 27.5, 28.9, 70.6, 61.9, 69.2, 74.6, 57.5,… ## $ Cs &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ Ba &lt;dbl&gt; 29.7, 44.3, 111.6, 72.4, 31.7, 53.1, 10.6, 28.2, 18.9… ## $ Nd &lt;dbl&gt; 34.3, 20.9, 46.1, 32.5, 27.5, 31.2, 28.2, 29.6, 37.0,… ## $ Sm &lt;dbl&gt; 6.9, 47.8, 82.8, 50.0, 15.6, 18.5, 15.8, 16.7, 28.7, … ## $ Yb &lt;dbl&gt; 345.3, 276.6, 205.2, 178.5, 285.1, 264.9, 262.9, 277.… ## $ Ta &lt;dbl&gt; 727.9, 659.0, 603.8, 630.3, 746.6, 688.6, 706.4, 674.… ## $ W &lt;dbl&gt; 2288.2, 2044.0, 2001.0, 2056.4, 2153.3, 2090.1, 2129.… ## $ Pb &lt;dbl&gt; 63.7, 64.7, 109.2, 66.6, 55.0, 87.7, 40.4, 217.0, 49.… ## $ Bi &lt;dbl&gt; 162.0, 123.9, 136.1, 124.1, 148.2, 139.9, 126.7, 147.… ## $ `Mo inc` &lt;dbl&gt; 26754.9, 23616.2, 21588.2, 23656.8, 26309.5, 25681.1,… ## $ `Mo coh` &lt;dbl&gt; 10207.9, 9466.9, 8911.0, 9174.5, 9985.4, 9881.3, 1007… ## $ position &lt;dbl&gt; 366.04, 466.04, 476.04, 496.04, 706.04, 806.04, 836.0… ## $ `sample surface` &lt;dbl&gt; 6.498, 6.000, 6.005, 6.080, 6.466, 6.484, 6.476, 6.51… ## $ `E-gain` &lt;dbl&gt; 0.010267, 0.010267, 0.010267, 0.010267, 0.010267, 0.0… ## $ `E-offset` &lt;dbl&gt; -0.009931, -0.009931, -0.009931, -0.009931, -0.009931… ## $ `F-slope` &lt;dbl&gt; 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.009… ## $ `F-offset` &lt;dbl&gt; 0.077115, 0.077115, 0.077115, 0.077115, 0.077115, 0.0… ## $ `Fe a*2` &lt;dbl&gt; 158.6, 74.1, 132.1, 167.6, 158.4, 118.7, 125.7, 105.0… ## $ `Fe a+b` &lt;dbl&gt; 82.2, 139.4, 111.5, 146.1, 136.0, 146.1, 133.9, 106.2… ## $ S1 &lt;dbl&gt; 306.6, 311.3, 475.1, 303.5, 318.9, 370.9, 274.8, 380.… ## $ S2 &lt;dbl&gt; 219.8, 199.8, 299.0, 209.5, 244.8, 263.5, 231.1, 249.… ## $ S3 &lt;dbl&gt; 252.7, 298.4, 430.0, 337.4, 261.1, 340.6, 312.0, 380.… ## $ Dt &lt;dbl&gt; 0.1021, 0.0990, 0.0979, 0.1017, 0.1022, 0.1008, 0.101… ## $ qc &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ top &lt;dbl&gt; 330, 430, 440, 460, 670, 770, 800, 840, 930, 1020, 10… ## $ bot &lt;dbl&gt; 340, 440, 450, 470, 680, 780, 810, 850, 940, 1030, 11… It is worth reading ?itraxR::itrax_reduce() as the behavior can and should be modified depending on your exact use case. For example, in th situation above where none of the samples are contiguous it might be wise to modify the parameters of itraxR::itrax_reduce() to include edges = c(\"&gt;=\", \"&lt;=\") so that the “edges” of the sub-samples are captured. This might not be the case for contiguous samples in order to avoid “double-counting”. "],["linear-methods.html", "7.4 Linear Methods", " 7.4 Linear Methods We’ll use our xrf and hhxrf data sets to create linear models of all the variables of interest. They must be combined using pivot_longer(). The plot indicates that some, but not all variables are suitable for calibration. full_join( hhxrf %&gt;% select(any_of(c(elementsList, &quot;SampleID&quot;))) %&gt;% pivot_longer(any_of(elementsList), values_to = &quot;hhxrf&quot;, names_to = &quot;element&quot;), xrf %&gt;% select(any_of(c(elementsList, &quot;SampleID&quot;))) %&gt;% pivot_longer(any_of(elementsList), values_to = &quot;xrf&quot;, names_to = &quot;element&quot;), by = c(&quot;SampleID&quot;, &quot;element&quot;) ) %&gt;% filter(element %in% myElements) %&gt;% drop_na() %&gt;% ggplot(aes(x = hhxrf, y = xrf)) + geom_point() + ggpmisc::stat_poly_line() + ggpmisc::stat_poly_eq() + facet_wrap(vars(element), scales = &quot;free&quot;) + xlab(&quot;Conventional XRF [ppm]&quot;) + ylab(&quot;Itrax ED-XRF [peak area]&quot;) To apply these calibration models to our existing data, we first need to save the models created using lm(). We don’t force the intercept through zero (e.g. hhxrf ~ 0 + xrf) because of background (high baseline) conditions that may be present. calibration &lt;- full_join( hhxrf %&gt;% select(any_of(c(elementsList, &quot;SampleID&quot;))) %&gt;% pivot_longer(any_of(elementsList), values_to = &quot;hhxrf&quot;, names_to = &quot;element&quot;), xrf %&gt;% select(any_of(c(elementsList, &quot;SampleID&quot;))) %&gt;% pivot_longer(any_of(elementsList), values_to = &quot;xrf&quot;, names_to = &quot;element&quot;), by = c(&quot;SampleID&quot;, &quot;element&quot;) ) %&gt;% mutate(element = as_factor(element)) %&gt;% drop_na() calibration &lt;- calibration %&gt;% group_by(element) %&gt;% group_split() %&gt;% lapply(function(x){lm(data = x, hhxrf~xrf)}) %&gt;% `names&lt;-`(calibration %&gt;% group_by(element) %&gt;% group_keys() %&gt;% pull(element)) We can see the performance of our model using summary(), for example: summary(calibration$Ca) ## Warning: In &#39;Ops&#39; : non-&#39;errors&#39; operand automatically coerced to an &#39;errors&#39; ## object with no uncertainty ## ## Call: ## lm(formula = hhxrf ~ xrf, data = x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -90606 -20066 1609 20785 76404 ## ## Coefficients: ## Warning in printCoefmat(coefs, digits = digits, signif.stars = signif.stars, : ## NAs introduced by coercion ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 51588.943250000(4312) 12535.14(6691) 4.11600(2242) 0.000124 *** ## xrf 0.729260000(3663) 0.0598700(3241) 12.18200(6531) &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 33200(200) on 58 degrees of freedom ## Multiple R-squared: 0.719, Adjusted R-squared: 0.7141 ## F-statistic: 148.4 on 1 and 58 DF, p-value: &lt; 2.2e-16 Now we can use predict() to apply those models to the data. CD166_19_xrf %&gt;% mutate(Ca_ppm = predict(calibration$Ca, newdata = CD166_19_xrf %&gt;% select(Ca) %&gt;% rename(xrf = Ca)) ) %&gt;% ggplot(aes(x = Ca_ppm, y = depth)) + geom_lineh() + scale_y_reverse() + scale_x_continuous(labels = function(x){x/10000}) + ylab(&quot;Depth [mm]&quot;) + xlab(&quot;Ca [%]&quot;) ## Warning: Removed 3 rows containing missing values or values outside the scale range ## (`geom_lineh()`). We can also extract the confidence intervals, for example: predict(calibration$Ca, newdata = CD166_19_xrf %&gt;% select(Ca) %&gt;% rename(xrf = Ca), interval = &quot;confidence&quot;, level = 0.95, type = &quot;response&quot;) %&gt;% as_tibble() %&gt;% mutate(depth = CD166_19_xrf$depth) %&gt;% ggplot(aes(y = depth, x = fit)) + geom_errorbar(aes(xmin = lwr, xmax = upr), col = &quot;grey&quot;) + geom_lineh() + scale_x_continuous(labels = function(x){x/10000}, name = &quot;Ca [%]&quot;) + scale_y_reverse(name = &quot;Depth [mm]&quot;) + theme_paleo() "],["log-ratio-methods.html", "7.5 Log-Ratio Methods", " 7.5 Log-Ratio Methods "],["exporting-data.html", "Chapter 8 Exporting Data", " Chapter 8 Exporting Data Data is easy to export from R. Tables can be exported by piping to readr::write_csv(). For diagrams created in ggplot2, pipe to ggsave(). Sometimes it is necessary to reduce the resolution of Itrax XRF data, usually to facilitate direct comparison with some other lower resolution data. For example, if calibrating using ICP-MS sub-samples taken at 10 mm intervals, but the Itrax XRF scan is at 0.2 mm, it will be necessary to average 50 Itrax measurements for each ICP-MS measurement. This is facilitated using the itrax_reduce() function. The example below demonstrates calculating the averages for contiguous 20 mm chunks of the XRF data. # get the data as 20 mm intervals itrax_reduce(CD166_19_xrf %&gt;% drop_na(), by = 20) %&gt;% select(-label) %&gt;% head() ## # A tibble: 6 × 58 ## resample_names depth MSE cps validity Al Si P S Cl Ar ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 9.5 1.50 39598. 1 63.4 337. 0 23.6 1382. 575 ## 2 2 29.5 1.65 44454. 1 64.2 272. 0 22.7 1316 627. ## 3 3 49.5 1.67 46676. 1 71.6 288. 0 12.9 1298 669. ## 4 4 69.5 1.70 47447. 1 70.2 317. 0 23.6 1486. 677. ## 5 5 89.5 1.70 47708. 1 73.2 388. 0 23.2 1512. 691. ## 6 6 110. 1.67 47713. 1 83.6 489. 0 23.2 1510. 655 ## # ℹ 47 more variables: K &lt;dbl&gt;, Ca &lt;dbl&gt;, Sc &lt;dbl&gt;, Ti &lt;dbl&gt;, V &lt;dbl&gt;, ## # Cr &lt;dbl&gt;, Mn &lt;dbl&gt;, Fe &lt;dbl&gt;, Ni &lt;dbl&gt;, Cu &lt;dbl&gt;, Zn &lt;dbl&gt;, Ga &lt;dbl&gt;, ## # Ge &lt;dbl&gt;, Br &lt;dbl&gt;, Rb &lt;dbl&gt;, Sr &lt;dbl&gt;, Y &lt;dbl&gt;, Zr &lt;dbl&gt;, Pd &lt;dbl&gt;, ## # Cd &lt;dbl&gt;, I &lt;dbl&gt;, Cs &lt;dbl&gt;, Ba &lt;dbl&gt;, Nd &lt;dbl&gt;, Sm &lt;dbl&gt;, Yb &lt;dbl&gt;, ## # Ta &lt;dbl&gt;, W &lt;dbl&gt;, Pb &lt;dbl&gt;, Bi &lt;dbl&gt;, `Mo inc` &lt;dbl&gt;, `Mo coh` &lt;dbl&gt;, ## # filename &lt;dbl&gt;, position &lt;dbl&gt;, `sample surface` &lt;dbl&gt;, `E-gain` &lt;dbl&gt;, ## # `E-offset` &lt;dbl&gt;, `F-slope` &lt;dbl&gt;, `F-offset` &lt;dbl&gt;, `Fe a*2` &lt;dbl&gt;, … "],["references.html", "References", " References Bishop, T., and M. Charidemou. 2023. “Core Scanning Data from Core CD166/19.” PANGAEA. https://doi.org/10.1594/PANGAEA.955347. Croudace, Ian W., Ludvig Löwemark, Rik Tjallingii, and Bernd Zolitschka. 2019. “High resolution XRF core scanners: A key tool for the environmental and palaeoclimate sciences.” Quaternary International 514 (May): 1–4. https://doi.org/10.1016/j.quaint.2019.05.038. Francus, P., K. Kanamaru, and D. Fortin. 2015. “Standardization and Calibration of X-Radiographs Acquired with the ITRAX Core Scanner.” In Micro-XRF Studies of Sediment Cores, Developments in Paleoenvironmental Research 17, edited by I. W. Croudace and R. G. Rothwell, 491–505. Dordrecht: Springer. Jarvis, S., I. W. Croudace, and R. G. Rothwell. 2015. “Parameter Optimisation for the ITRAX Core Scanner.” In Micro-XRF Studies of Sediment Cores, Developments in Paleoenvironmental Research 17, edited by I. W. Croudace and R. G. Rothwell, 535–62. Dordrecht: Springer. Wynn, R. B., and B. T. Cronin. 2005. “RRS \"Charles Darwin\" Cruise CD166, 29 Oct - 22 Nov 2004. Sedimentary processes and deposits in the Agadir Basin and Gulf of Cadiz.” 59. Vol. 44. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
