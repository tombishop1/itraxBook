[["index.html", "Using Itrax Data in R Preface 0.1 Why did I write this book? 0.2 Prerequisites 0.3 Example Data", " Using Itrax Data in R Thomas Bishop 2020-11-06 Preface 0.1 Why did I write this book? During the COVID-19 pandemic in 2020 it became necessary to formalise (or at least, organise) the support offered to students using the Itrax core scanner at The University of Manchester Geography Laboratories so they could be offered online. This led to an unexpectedly popular series of seminars. When these came to an end, we decided it would be useful to summarise our seminars into a reference guide that could be updated over time. This is that guide. 0.2 Prerequisites This guide assumes a basic knowledge of R and the tidyverse, including data types, assignments, and pipes. It also assumes a background knowledge of the core scanner and the nature of the data it produces; see Croudace et al. (2019) and references therein. Some of the sections on data analysis assume some knowledge of compositional data analysis. 0.3 Example Data All of the examples in this book use scans from a sequence of marine sediment cores obtained during a cruise of the RRS Charles Darwin CD166 on the 6th of November 2004. The 4.3 meter sequence was obtained at station CD16619-01 (31º30.794’N, 17º11.777’W) from a depth of 4,467 meters. It is notable for containing a series of volcanic “bypass” turbidites. The scans were performed at the British Ocean Sediment Core Research Facility (BOSCORF) at the National Oceanographic Centre, Southampton in 2020. The drive is a sequence of three consecutive cores, and have been scanned for photographic and radiographic line-scan imagery, as well as three repeats of x-ray flourescence measurements. References "],["intro.html", "Chapter 1 Data Structure 1.1 Metadata 1.2 XRF Data 1.3 Optical Images 1.4 Radiographic Images 1.5 Magnetic Susceptability", " Chapter 1 Data Structure The Itrax core scanner is a multi-sensor device, with seperate data outputs for different measurements and uses. They are described in the following sections. Note that your data may not contain all of these objects, depending on the exact scanner, configuration, or data repository you use. The folder structure may vary between operators, but typically there will be a folder for each scan section, and each will contain the data described in the following sections. Note that where radiographs and XRF data have been aquired using different step-sizes (measurement intervals), the operator will create seperate scan sections (folders) for the x-radiograph and XRF measurement. This is because a scan section can only have a single fixed step-size. For example, it is not uncommon for users to require an step-size of 200 micrometers for the x-radiograph, but only 1 millimeter for the XRF measurement. 1.1 Metadata Every scan section has a document.txt file that contains information about the parameters of the scan. For example, it contains the current and voltage settings used for the x-ray tube, the step size, and the start and stop positions of the scan. The file can be opened using any text editor, or can be easily parsed in R using itraxR::itrax_meta(). Sometimes this information is required to process other parts of the data, and as such it is an important part of the overall data package. 1.2 XRF Data The XRF data can be split into two groups — “raw” and “processed” data. The raw data is contained in a seperate folder called XRF data, and consists of a single file, beginning with L000000.spe and incrementing sequentially. This file can be read using a text editor and is tab-delimited. The first part is a header, containing metadata information. The second part is a table of all the channels of the detector and the corresponding count for each channel. Increasing channel numbers represent increasing energy, but some thought needs to be given to calibrating channels into an energy — this step is usually performed using specialist software. In addition, a file called sumspectra.spe is often included in the root directory; this is simply the sum of all the *.spe files in the XRF data directory, and is sometimes useful in processing the data. Processed data comes from the Q-Spec software (Cox Analytical Systems, Sweden) provided with the machine. Its function is to process the spectral data files (*.spe) into peak-areas for each element of interest by fitting a model to the data. The model needs some user input and intervention to optimise it, and the quality of the model can be assessed using a number of diagnostic parameters, the most important being the RMSE. The Q-Spec software can also perform some calibration of the data, although this is a less typical use-case. Often the operator will include a file that contains all of the settings used by Q-Spec to translate the raw data files into the peak area outfile file — this file will have the extension *.dfl and will often simply be called settings.dfl. It comes in the form of a table, with a single row for each measurement step, and a column for parameters including individual element peak areas in counts (n) or intensities (n/mA). The data files commonly have names like result.txt or Results.txt, but may have been subsequently renamed. 1.3 Optical Images The scanner collects good quality optical images that have consistent lighting and because they are line-scan images, they do not suffer from optical distortions. Medium resolution images are usually included in all scan section folders (typically optical.tif), and optional high-resolution images are often included by operators elsewhere. High resolution images are typically supplied as both 8-bit and 16-bit images, and are usually hundreds of megabytes in size. Although the brightness of the lighting in the scanner is adjustable, images sometimes need to be brightened and/or have the contrast adjusted. This can be performed in desktop publishing software (e.g. Adobe Photoshop, Corel Photo-Draw), but it is easiest to use the open-source scientific image analysis software imageJ (NIH, USA). By including an appropriate colour reference card in the scan the image can be calibrated, although it is often desirable to increase the contrast to elucidate features of the core. The photograph will of the entire length of the bed scanned, rather than cropped to the limits of an individual scan section. If multiple scan sections are placed on the bed and are scanned together, it will include all of the scanned sections. The image needs to be cropped using the metadata for the relevant scan section; this process is covered in later chapters. 1.4 Radiographic Images The scanner has an x-radiographic line array capable of producing good-quality x-radiographs of the cores. The scan data can be split into two parts — “raw” and “processed” data. The raw data (usually radiograph.raw) is a tab-delimited text file containing a matrix of greyscale values. Each column represents a single step (measurement interval), and each row represents a single pixel on the line array. The pixel spacing is around 20 microns. The processed image (radiograph.tif) has a lower resolution than the raw data. This is because the pixels must be square, and so the pixels are down-sampled to fit with the step-size of the scan. Thus, if the step-size was 200 micrometers, each pixel will be 200 x 200 micrometers, whereas the raw data will have rectangular pixels with dimensions of 20 x 200 micrometers. Like the optical image, the radiograph often requires contrast and brightness adjustments, and these are easiest to perform in imageJ. With the inclusion of a suitable density standard, some relative or, where “u-channels” are used, absolute density calibration can be performed using these data. 1.5 Magnetic Susceptability Some scanners include a Bartington MS2E surface sensor. "],["importing.html", "Chapter 2 Importing Data 2.1 Metadata 2.2 XRF Data 2.3 Optical Images 2.4 Radiographic Images 2.5 Magnetic Susceptability", " Chapter 2 Importing Data All of the Itrax data is in either text-format or “tagged image format” (*.tif). Although this means it is easily read by the various import functions available in R, it still needs considerable cleaning and wrangling to get it to a point where it is useable for most analyses. There are three possible approaches to this task: Use existing functions published in the itraxR package available from github.com/tombishop1. These are at an early stage and functionality might be broken, but are largely convenience functions for wrangling and analysing Itrax data. Work in base R to wrangle the data. This is perfectly achievable, and much of the current itraxR functionality was written this way. Work in the tidyverse family of packages and style. For data wrangling tasks, this approach can result in simpler and more resilient code. In this guide examples will be given mostly using the tidyverse, and reference will be made to the convenience functions in itraxR. All of the Itrax data is in either text-format or “tagged image format” (*.tiff). Although this means it is easily read by the various import functions available in R, it still needs considerable cleaning and wrangling to get it to a point where it is useable. A number of convenience functions are provided in the package itraxR. 2.1 Metadata The scan data file document.txt can be quickly parsed using itraxR::itrax_meta(). The output is a dataframe from which the individual components can be easily accessed through subsetting functions, for example as.numeric(itrax_meta()[6:7, 2]) would return a numeric vector of the start and end position of a scan. library(itraxR) itrax_meta(&quot;CD166_19_S1/CD166_19_S1/document.txt&quot;) ## Parameter Value Unit ## 1 Sample name CD166_19_S1 str ## 2 Section name CD166_19_S1 str ## 3 Aquisition date 22/9/2020 dd/mm/yyyy ## 4 Operator name MC str ## 5 Tube Mo element ## 6 Start coordinate 31.5 mm ## 7 Stop coordinate 1314.1 mm ## 8 Step size 1000 microns ## 9 Optical Start 0.5 mm ## 10 Optical End 1401.3 mm ## 11 Optical step size 0.188 mm ## 12 Rad. voltage 55 kV ## 13 Rad. current 50 mA ## 14 Rad. exposure 0 ms ## 15 line camera signal level 323154 at 25 ms ## 16 XRF ON ON/OFF ## 17 XRF voltage 30 kV ## 18 XRF current 30 mA ## 19 XRF exp. time 15 seconds ## 20 Start temperature \\xb0C ## 21 Stop temperature \\xb0C ## 22 Start humidity % ## 23 Stop humidity % ## 24 Start vacuum -95.0 kPa ## 25 Stop vacuum -94.8 kPa 2.2 XRF Data 2.2.1 Processed Data This is the data most commonly used in analysis and it can be quickly imported using itraxR::itrax_import(). Note that, like for the example data, it is possible to have more than one processed data file. Typically cores have at least two, one created at the time of the scan based on settings for a single point near the top of the sequence, and another from a holistic re-analysis of the sequence. CD166_19_S1 &lt;- itrax_import(&quot;CD166_19_S1/CD166_19_S1/Results.txt&quot;, depth = 0) head(CD166_19_S1) ## position sample.surface validity cps MSE Al Si P S Cl K Ca Sc ## 0 31.54 6.38 TRUE 22415 1.26 41 177 0 0 726 1412 59965 19 ## 1 32.54 6.39 TRUE 34525 1.41 76 275 0 10 1318 2565 112734 0 ## 2 33.54 6.42 TRUE 38370 1.57 57 306 0 0 1513 2628 144287 14 ## 3 34.54 6.42 TRUE 39796 1.55 74 330 0 32 1470 2378 162938 0 ## 4 35.54 6.43 TRUE 40022 1.41 26 206 0 21 1312 2265 153194 0 ## 5 36.54 6.44 TRUE 41973 1.41 53 233 0 37 1740 2947 135879 106 ## Ti V Cr Mn Fe Ni Cu Zn Ga Ge Br Rb Sr Y Zr Pd Cd I Cs Ba ## 0 804 26 220 231 19786 54 117 184 0 0 331 175 5817 60 305 0 9 30 0 35 ## 1 1661 51 258 508 35168 123 277 205 4 0 491 329 8306 15 255 31 13 38 0 63 ## 2 1806 88 326 559 36494 104 230 268 48 40 605 76 9181 140 322 78 34 48 0 48 ## 3 2121 22 301 483 35952 134 150 123 0 43 550 295 9644 119 280 75 55 80 0 25 ## 4 2031 75 306 485 35512 144 206 239 0 0 609 313 9940 107 475 66 24 67 0 60 ## 5 1826 89 440 738 44303 166 276 240 0 0 659 304 10287 180 160 31 34 23 0 132 ## Nd Sm Yb Ta W Pb Bi Mo.inc Mo.coh depth ## 0 12 0 166 546 1151 22 0 20427 6812 0 ## 1 17 43 144 789 1992 77 72 26323 8799 1 ## 2 67 46 282 776 2019 0 136 26778 9117 2 ## 3 44 39 158 703 2016 13 199 26550 9303 3 ## 4 75 64 218 794 2160 152 134 28310 9886 4 ## 5 42 36 163 919 2344 54 157 31356 10140 5 If you’d like more control over the import process, a typical Tidyverse workflow would consist of reading the file (they are tab-delimited), removing variables that are not of interest, and correctly labelling data that should be NA. You may also wish to assign a coring depth information at this stage. Firstly, we need to load the packages we will use, and define a list of elements and other names of variables we might expect to find in the files. library(tidyverse) library(janitor) elements &lt;- c( &quot;H&quot;, &quot;He&quot;, &quot;Li&quot;, &quot;Be&quot;, &quot;B&quot; , &quot;C&quot; , &quot;N&quot;, &quot;O&quot;, &quot;F&quot; , &quot;Ne&quot;, &quot;Na&quot;, &quot;Mg&quot;, &quot;Al&quot;, &quot;Si&quot;, &quot;P&quot;, &quot;S&quot;, &quot;Cl&quot;, # &quot;Ar&quot;, &quot;K&quot;, &quot;Ca&quot;, &quot;Sc&quot;, &quot;Ti&quot;, &quot;V&quot;, &quot;Cr&quot;, &quot;Mn&quot;, &quot;Fe&quot;, &quot;Co&quot;, &quot;Ni&quot;, &quot;Cu&quot;, &quot;Zn&quot;, &quot;Ga&quot;, &quot;Ge&quot;, &quot;As&quot;, &quot;Se&quot;, &quot;Br&quot;, &quot;Kr&quot;, &quot;Rb&quot;, &quot;Sr&quot;, &quot;Y&quot;, &quot;Zr&quot;, &quot;Nb&quot;, &quot;Mo&quot;, &quot;Tc&quot;, &quot;Ru&quot;, &quot;Rh&quot;, &quot;Pd&quot;, &quot;Ag&quot;, &quot;Cd&quot;, &quot;In&quot;, &quot;Sn&quot;, &quot;Sb&quot;, &quot;Te&quot;, &quot;I&quot;, &quot;Xe&quot;, &quot;Cs&quot;, &quot;Ba&quot;, &quot;La&quot;, &quot;Ce&quot;, &quot;Pr&quot;, &quot;Nd&quot;, &quot;Pm&quot;, &quot;Sm&quot;, &quot;Eu&quot;, &quot;Gd&quot;, &quot;Tb&quot;, &quot;Dy&quot;, &quot;Ho&quot;, &quot;Er&quot;, &quot;Tm&quot;, &quot;Yb&quot;, &quot;Lu&quot;, &quot;Hf&quot;, &quot;Ta&quot;, &quot;W&quot;, &quot;Re&quot;, &quot;Os&quot;, &quot;Ir&quot;, &quot;Pt&quot;, &quot;Au&quot;, &quot;Hg&quot;, &quot;Tl&quot;, &quot;Pb&quot;, &quot;Bi&quot;, &quot;Po&quot;, &quot;At&quot;, &quot;Rn&quot;, &quot;Fr&quot;, &quot;Ra&quot;, &quot;Ac&quot;, &quot;Th&quot;, &quot;Pa&quot;, &quot;U&quot;, &quot;Np&quot;, &quot;Pu&quot;, &quot;Am&quot;, &quot;Cm&quot;, &quot;Bk&quot;, &quot;Cf&quot;, &quot;Es&quot;, &quot;Fm&quot;, &quot;Md&quot;, &quot;No&quot;, &quot;Lr&quot;, &quot;Unq&quot;, &quot;Unp&quot;, &quot;Unh&quot;, &quot;Uns&quot;, &quot;Uno&quot;, &quot;Une&quot;, &quot;Unn&quot; ) others &lt;- c( &quot;position (mm)&quot;, &quot;sample.surface&quot;, &quot;MSE&quot;, &quot;cps&quot;, &quot;validity&quot;, &quot;Mo inc&quot;, &quot;Mo coh&quot;, &quot;Cr inc&quot;, &quot;Cr coh&quot; ) Secondly, we import the file, remove empty rows and columns, and change the data type of the validity flag. df &lt;- read_tsv(&quot;results.txt&quot;, skip = 2) %&gt;% remove_empty() %&gt;% mutate(validity = as.logical(validity)) Next we can select only the variables we need. The code below leaves anything that is a chemical element or in our list of other useful parameters, but you can change these depending on your needs. df &lt;- df %&gt;% select(any_of(c(elements, others))) If you have coring depth information, you could add it now. A simple calculation can be made using the position variable (demonstrated below, here assuming the top of the core is at a depth of 400 mm), or a more advanced calculation as you require. It is also possible to add ages derived from an age/depth model now. The final line re-orders the variables so the depth variable is the first. df &lt;- df %&gt;% rename(position = `position (mm)`) %&gt;% mutate(depth = position - min(position) + 400) %&gt;% select(depth, everything()) Finally, a quirk of the formatting of data where validity == FALSE needs to be addressed. These cells have a value of 0, but they could be confused with truely 0 value cells that have validity == TRUE. The solution is to convert them to NA as follows. Note that for metadata variables (depth, sample.surface etc. the measurements are still valid, so should not be set to NA). df &lt;- full_join(df %&gt;% filter(validity == TRUE), df %&gt;% filter(validity == FALSE) %&gt;% select(any_of(c(elements, &quot;position&quot;))) %&gt;% na_if(0)) %&gt;% arrange(position) %&gt;% mutate(depth = df$depth, validity = df$validity, cps = df$cps, MSE = df$MSE) Finally, you may wish to do some quality control. The deletion criteria will be more fully discussed in the next chapter, but it is often the case that the first and last few measurements are of poor quality. The code below removes the first and last 10 mm of measurements, however depending on your workflow you may wish to simply flag it in some way. df &lt;- df %&gt;% filter(depth &gt; min(depth) + 10 &amp; depth &lt; max(depth) - 10) 2.2.2 Joining XRF Data Ofen a core (sometimes referred to as a drive) is comprised of a sequence of individual sections, which may or may not be overlapping. Often we will want to integrate them into a continuous dataset for analytical purposes. When joining cores that do not overlap, this process is trivial — the data might simply appended in order of depth, and a new column is added with the identity of the original core section. Where overlapping cores are present, there can be multiple measurements at a single depth (on different cores). In these cases not only will the individual measurements need to be re-ordered by depth, but an additional variable should be created that can be used in combination or alone to uniquely identify each measurement. The code below does this by creating an additional variable called label, with the name of the original core given in the named list. mylist &lt;- list(core1 = core1, core2 = core2) df &lt;- lapply(names(mylist), function(i) within(mylist[[i]], {label &lt;- i})) %&gt;% bind_rows() %&gt;% arrange(depth) This process can be simplified using itraxR::itrax_join(), for example: # import the core sections CD166_19_S1 &lt;- itrax_import(&quot;CD166_19_S1/CD166_19_S1/Results.txt&quot;, depth_top = 0) CD166_19_S2 &lt;- itrax_import(&quot;CD166_19_S2/CD166_19_S2/Results.txt&quot;, depth_top = max(CD166_19_S1$depth)) CD166_19_S3 &lt;- itrax_import(&quot;CD166_19_S3/CD166_19_S3/Results.txt&quot;, depth_top = max(CD166_19_S2$depth)) #join them together CD166_19 &lt;- itrax_join(list(S1 = CD166_19_S1, S2 = CD166_19_S2, S3 = CD166_19_S3)) ## Loading required package: dplyr ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union str(CD166_19) ## &#39;data.frame&#39;: 4212 obs. of 44 variables: ## $ position : num 31.5 32.5 33.5 34.5 35.5 ... ## $ sample.surface: num 6.38 6.39 6.42 6.42 6.43 6.44 6.46 6.47 6.48 6.49 ... ## $ validity : logi TRUE TRUE TRUE TRUE TRUE TRUE ... ## $ cps : int 22415 34525 38370 39796 40022 41973 41268 40977 41104 41408 ... ## $ MSE : num 1.26 1.41 1.57 1.55 1.41 1.41 1.36 1.52 1.38 1.58 ... ## $ Al : int 41 76 57 74 26 53 27 72 78 69 ... ## $ Si : int 177 275 306 330 206 233 347 337 346 403 ... ## $ P : int 0 0 0 0 0 0 0 0 0 0 ... ## $ S : int 0 10 0 32 21 37 28 59 19 44 ... ## $ Cl : int 726 1318 1513 1470 1312 1740 1576 1605 1559 1503 ... ## $ K : int 1412 2565 2628 2378 2265 2947 3179 3376 3193 3220 ... ## $ Ca : int 59965 112734 144287 162938 153194 135879 132183 134094 138570 145762 ... ## $ Sc : int 19 0 14 0 0 106 14 0 46 0 ... ## $ Ti : int 804 1661 1806 2121 2031 1826 1923 2059 2443 2701 ... ## $ V : int 26 51 88 22 75 89 107 0 71 23 ... ## $ Cr : int 220 258 326 301 306 440 401 440 407 449 ... ## $ Mn : int 231 508 559 483 485 738 792 799 868 794 ... ## $ Fe : int 19786 35168 36494 35952 35512 44303 45283 45694 46506 47859 ... ## $ Ni : int 54 123 104 134 144 166 151 125 157 180 ... ## $ Cu : int 117 277 230 150 206 276 250 217 178 191 ... ## $ Zn : int 184 205 268 123 239 240 294 284 238 230 ... ## $ Ga : int 0 4 48 0 0 0 40 8 22 26 ... ## $ Ge : int 0 0 40 43 0 0 48 115 100 80 ... ## $ Br : int 331 491 605 550 609 659 709 583 582 635 ... ## $ Rb : int 175 329 76 295 313 304 318 337 323 223 ... ## $ Sr : int 5817 8306 9181 9644 9940 10287 9917 9931 9897 9209 ... ## $ Y : int 60 15 140 119 107 180 106 52 77 112 ... ## $ Zr : int 305 255 322 280 475 160 312 319 319 552 ... ## $ Pd : int 0 31 78 75 66 31 72 31 54 58 ... ## $ Cd : int 9 13 34 55 24 34 28 56 62 96 ... ## $ I : int 30 38 48 80 67 23 58 30 53 93 ... ## $ Cs : int 0 0 0 0 0 0 0 0 0 0 ... ## $ Ba : int 35 63 48 25 60 132 93 172 108 108 ... ## $ Nd : int 12 17 67 44 75 42 97 92 68 56 ... ## $ Sm : int 0 43 46 39 64 36 12 24 69 68 ... ## $ Yb : int 166 144 282 158 218 163 184 248 166 188 ... ## $ Ta : int 546 789 776 703 794 919 852 926 840 847 ... ## $ W : int 1151 1992 2019 2016 2160 2344 2336 2243 2267 2185 ... ## $ Pb : int 22 77 0 13 152 54 54 75 111 78 ... ## $ Bi : int 0 72 136 199 134 157 182 151 185 116 ... ## $ Mo.inc : int 20427 26323 26778 26550 28310 31356 30631 29040 29104 27932 ... ## $ Mo.coh : int 6812 8799 9117 9303 9886 10140 9968 9973 9701 9751 ... ## $ depth : num 0 1 2 3 4 5 6 7 8 9 ... ## $ label : chr &quot;S1&quot; &quot;S1&quot; &quot;S1&quot; &quot;S1&quot; ... 2.2.3 Raw Data Sometimes it is useful to work with raw data rather than the calculated intensity data from the Q-Spec software. In this case, the raw data can be read directly from the individual files in the relevant directory. For individual measurements this is fairly trivial, although it must be considered that the data output is not calibrated to an energy and the data are in counts, not intensities. If the entire scan is read, some mechanism to iterate through the individual data files, adding them to a structured data object with relevant metadata (positions, for example) is required. 2.3 Optical Images In order to make the images usable for plotting beside other data, they need to be cropped to the scanned area and referenced to the position or a depth variable. Images in R are read in as a matrix with 3 dimensions (length, width, and the three colours). To begin with the start and end positions of the scan need to be established; this usually involves parsing the metadata file (e.g. itraxR::itrax_meta()[]), although in some circumstances they can be obtained from another data source (e.g. range(myData[,\"Position\"])). These then need to be mapped onto individual pixel values using the known size of a single pixel. Once this has been calculated, it is trvial to then subset the part of the image required. library(itraxR) library(tiff) image &lt;- readTIFF(&quot;optical.tif&quot;) meta &lt;- itrax_meta(&quot;document.txt&quot;) image_depth &lt;- rev(seq(from = as.numeric(meta[9, 2]), to = as.numeric(meta[10, 2]), length.out = dim(image)[2])) image &lt;- image[ , which(image_depth &gt; as.numeric(meta[6, 2]) &amp; image_depth &lt; as.numeric(meta[7, 2])) , ] 2.4 Radiographic Images The workflow for manipulating the processed radiographic images (*.tif) is very similar to that for the optical images, the main difference being the matrix only has two dimensions (length and width) as the image is greyscale. However, if there is a desire to manipulate the raw data from the radiographic image, some further work is required because the “pixel” is not square, but rectangular. That is to say the length of the pixel differs from its width. On the core scanner a single pixel has a width across the core of 20 micrometers, but has a variable coverage along the core (usually between 50 and 200 micrometers). The processed image downscales the pixel width to match the pixel length in order to force square pixels, losing some resolution along the way. In addition, it should be noted that unlike the optical images that always begin from position == 0, the radiographic images have defined start and end points just like an XRF scan, the parameters of which can be accessed from the document.txt metadata file (or by using itrax_meta()). 2.5 Magnetic Susceptability "],["tidying-data.html", "Chapter 3 Tidying Data", " Chapter 3 Tidying Data In the functionality provided by itraxR, the need for data cleaning is much reduced. However, you may still encounter poor quality data that needs removing from subsequent analysis. This can be broadly defined as: Data at the start and end of the the core, where a volume of core material is “missing”. Measurements where the optical configuration is out of position (marked as validity == 0), often due to holes or stones in the core. Areas of the core with low total counts. Individual measurements that are statistical outliers. The easiest way to do this is using a tidyverse style sequence of pipes that set the observations of faulty data as NA. "],["plotting.html", "Chapter 4 Plotting 4.1 Plotting XRF Data 4.2 Plotting Images", " Chapter 4 Plotting There are a number of plotting options included in base R and some paleoenvironmental packages like analouge, but here we will use ggplot2 and compatible packages. 4.1 Plotting XRF Data For simple biplots, ggplot2 provides the following solution. ggplot(data = na.omit(df), mapping = aes(x = depth, y = Mo.coh/Mo.inc)) + geom_line(aes(color = label)) + coord_flip() + scale_x_reverse() + labs(x = &quot;Depth [mm]&quot;,color = &quot;Core&quot;) + theme_classic() + theme(legend.position = &quot;none&quot;) Where a traditional stratigraphic plot is desired, the tidypaleo package provides useful functionality for producting these. However, the data does need to converted to long-form from the existing table. In this way tidypaleo::facet_geochem_gridh works in a way similar to ggplot::facet_wrap but has been creates a stratigraphic plot. library(tidypaleo) df %&gt;% mutate(`coh/inc` = Mo.coh/Mo.inc) %&gt;% select(Fe, Ti, `coh/inc`, Mn, depth, label) %&gt;% pivot_longer(!c(&quot;depth&quot;, &quot;label&quot;), names_to = &quot;elements&quot;, values_to = &quot;peakarea&quot;) %&gt;% drop_na() %&gt;% mutate(elements = factor(elements, levels = c(&quot;Fe&quot;, &quot;Mn&quot;, &quot;Ti&quot;, &quot;coh/inc&quot;))) %&gt;% ggplot(aes(x = peakarea, y = depth)) + geom_lineh(aes(color = label)) + scale_y_reverse() + facet_geochem_gridh(vars(elements)) + labs(x = &quot;peak area&quot;, y = &quot;Depth [mm]&quot;) + theme_paleo() + theme(legend.position = &quot;none&quot;) 4.2 Plotting Images Once an image file has been imported, the image can be integrated into ggplot code using the workflow below. library(tidyverse) library(grid) ggplot() + annotation_custom(rasterGrob(image2, width = unit(1, &quot;npc&quot;), height = unit(1, &quot;npc&quot;))) + xlab(&quot;&quot;) + ylab(&quot;Position [mm]&quot;) + ylim(as.numeric(meta[6:7, 2])) If the image needs to be rotated, the aperm() function needs to be used, followed by reversing the image. image2 &lt;- aperm(image, c(2,1,3)) image2 &lt;- image2[c(dim(image2)[1]: 1), , ] "],["transforming-data.html", "Chapter 5 Transforming Data 5.1 Ratios and Normalisation 5.2 Preparing Data for Multivariate Data Analysis", " Chapter 5 Transforming Data The XRF data typically reported from the Itrax core scanner come from the spectral processing software Q-Spec. The output is usually in the form of an intensity, which is a dimensionless metric derived from the size of the spectral peak for a particular element, above the background Bremsstralung radiation, sometimes normalised for the tube current and/or counting time. 5.1 Ratios and Normalisation These data are compositional, and represent the changes in the relative proportions of all components of the matrix, measured and un-measured. As such it is likely the data will need transforming for certain types of multivariate analysis. As previously mentioned, these data are dimensionless, and as such do not represent a quantity, but are directly related to the absolute amount of a particular element in the matrix. It is often the case that ratios of elements are used to represent changes in composition — this is sometimes referred to as normalisation, or normalising one element against another. It is trivial to calculate element ratios, to the extent that these can often simply be calculated where they are required rather than saving them to memory. For example, if a plot of the Compton divided by the Rayleigh scatter was desired, there is no need to save the computed value to a new variable (e.g. coh_inc &lt;- df$Mo.coh/df$Mo.inc ) — simply define the calculation during plotting. 5.2 Preparing Data for Multivariate Data Analysis Where multivariate methods (cluster analysis, principle components analysis, correlation matricies) are required, it is usually necessary to transform data. This is because the statistical assumptions that underlie these methods are often not met when dealing with compositional data like that from an XRF core-scanner. There is no “right” or ideal way to deal with these issues, but a common method is to use a form of log transformation. Here we use a centered log transformation, which cannot be performed on zero values. df %&gt;% chemometrics::clr(df) There are a number of possible solutions to this problem of zero values, none ideal, but the most common are to add an arbitary number to the entirity of the data, or to replace zero values with a very small number, perhaps the limit of precision or the limit of detection. In the example shown, the limit of precision for the peak area intensity is used (0.001). input[input == 0] &lt;- 0.001 A final solution may be to exclude zero values from any subsequent analysis, although not all methods tolerate NA in the data. Where there are many zero values for a particular variable, the variable may not provide good data and could be excluded. df %&gt;% na_if(0) In most multivariate analyses there are a number of variables which have high signal-to-noise ratios or otheriwse only add noise to multivariate methods. In these cases, they might be excluded from multivariate methods. The selection of variables for inclusion is a matter for the analyst. It goes without saying that variables that are not part of the composition (that is, anything that is not an element) must be removed from the data used for multivariate analysis. df %&gt;% select(-any_of(c(&quot;Mg&quot;, &quot;Co&quot;, &quot;Mo&quot;)) Finally, in order to correctly identify the measurements to their original data source, it is necessary to use row names that uniquely identify observations. For single scans this is not usually an issue — the depth or position variable can be used. However, where a dataset is a composition of multiple cores, you may find that there are multiple observations for a particular depth where cores overlap. In the example shown we create a unique name for each observation by concatating the name of the core as recorded in the label column of a core sequence joined using itraxR::itrax_join() with the corresponding depth variable, but the code could be modified to suit a different workflow. rowlabels &lt;- str_c(df$label, df$depth, sep = &quot;_&quot;) input &lt;- df %&gt;% select(any_of(elements)) input &lt;- input %&gt;% select(-any_of(c(&quot;Mg&quot;, &quot;Co&quot;, &quot;Mo&quot;))) input[input == 0] &lt;- 0.001 library(chemometrics) input &lt;- clr(input) row.names(input) &lt;- rowlabels "],["multivariate-methods.html", "Chapter 6 Multivariate Methods 6.1 Correlation Coefficients 6.2 Unconstrained Cluster Analysis 6.3 Constrained Cluster Analaysis 6.4 Principle Components Analysis", " Chapter 6 Multivariate Methods Input data often needs extensive preparation if useful results are to be obtained — transformation, dealing with zero values, and NA values. It is worth mentioning some general issues around the robustness of multivariate analysis. 6.1 Correlation Coefficients The Pearson’s R correlation coefficient is occasionally used to study the relationships between individual elements. 6.2 Unconstrained Cluster Analysis 6.3 Constrained Cluster Analaysis 6.4 Principle Components Analysis "],["calibrating-data.html", "Chapter 7 Calibrating Data 7.1 Suitable Methods 7.2 Linear Methods 7.3 Log-Ratio Methods", " Chapter 7 Calibrating Data 7.1 Suitable Methods 7.2 Linear Methods 7.3 Log-Ratio Methods "],["exporting-data.html", "Chapter 8 Exporting Data", " Chapter 8 Exporting Data "],["references.html", "References", " References "]]
